\begin{savequote}[45mm]
  \ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
  \qauthor{\ascii{- Martin Flower}}
\end{savequote}


\chapter{Distributed TensorFlow}
\label{ch:distributed}
\begin{content}
\tf{} can be run in a distributed environment to complete the execution of the calculation graph. This chapter will focus on the basic architecture and operation mechanism of distributed runtime; focus on the interaction between various service processes; and deeply analyze the key operations in graph operations in distributed environments and their session lifecycle control;
\end{content}


\section{Distributed mode}
\begin{content}
In distributed mode, \ascii{Client} is responsible for computing the construction of the graph, and then starts the execution of the graph by calling \code{Session.run}.

After the \ascii{Master} process receives the message of the calculation graph execution, it starts the pruning, splitting, optimization, etc. of the calculation graph; finally, the subgraph distribution is registered to each \ascii{Worker} process, and then each \ascii{ The Worker} process executes the subgraph concurrently.

After the \ascii{Worker} process receives the message of the sub-picture registration, according to the local computing device resources, the calculation sub-picture is subjected to secondary splitting, the sub-picture is allocated to each computing device, and finally each computing device is started concurrently. Figure; if there is data exchange between \ascii{Worker}, the interaction can be done through interprocess communication.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/distributed.png}
  \caption{distributed mode}
  \label{fig:distributed}
\end{figure}


\subsection{Graph operation}
As shown in \refig{dist-runtime}, in the execution of \code{run\_step}, it involves the pruning, splitting, and execution of three important graph operations. Among them, in the distributed operation, the graph split undergoes a two-stage splitting process.

\begin{enum}
  \eitem{level split: done by \code{MasterSession}, complete the graph split process according to \code{SplitByWorker} or \code{SplitByTask};}
  \eitem{Secondary split: completed by \code{WorkerSession}, complete the graph splitting process according to \code{SplitByDevice}. }
\end{enum}

In the distributed mode, the graph pruning also reflects the design concept of the \tf{} part of the implementation; and the splitting and execution of the graph also reflects the design concept of \tf{} concurrent execution. Among them, the graph pruning only occurs on \ascii{Master}, does not occur on \ascii{Worker}; and the graph split occurs on \ascii{Master} and \ascii{Worker}; graph execution only occurs in \ascii On {Worker}, it does not happen on \ascii{Master}.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/dist-runtime.png}
  \caption{Distributed: Figure Operation}
  \label{fig:dist-runtime}
\end{figure}


\subsubsection{Graph split}
To better understand how the distributed runtime works, a simple example illustrates the specific process of the graph operation. As shown in \refig{dist-exp-1}, if there is a simple calculation graph, and \code{f, c, a} is deployed on \code{/job:ps/task:0}, and Arranged on \code{CPU0, CPU1, CPU2}; \code{g, h} is deployed on \code{/job:worker/task:0} and is also arranged on \code{GPU0}; \code {b, d, e} is deployed on \code{/job:worker/task:1}, and \code{d, e} is arranged on \code{GPU0}, and \code{b} is arranged to On \code{GPU1}.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/dist-exp-1.png}
  \caption{distributed: split graph}
  \label{fig:dist-exp-1}
\end{figure}


\subsubsection{Data Exchange}
As shown in \refig{dist-exp-2}, for the edge across devices, the runtime automatically splits the edges, inserting two end nodes, \code{Send} and \code{Recv}, on the sender and receiver respectively. .

The \code{Send} and \code{Recv} nodes between processes exchange data through \code{GrpcRemoteRendezvous}. For example, \code{/job:ps/task:0} and \code{/job:worker/task:0},\code{/job:ps/task:0} and \code{/job:worker/task :1}, or \code{/job:worker/task:0} and \code{/job:worker/task:1} are exchanged via \code{GrpcRemoteRendezvous}.

The \code{Send} and \code{Recv} nodes in the process exchange data through \code{IntraProcessRendezvous}. For example, there are two \code{GPU}s in \code{/job:worker/task:1}, which use \code{IntraProcessRendezvous} for data exchange. The specific implementation process of \code{Rendezvous} will be highlighted below.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/dist-exp-2.png}
  \caption{Distributed: Data Exchange}
  \label{fig:dist-exp-2}
\end{figure}


\subsection{Formalization}
In real system implementations, the distributed runtime is implemented using \ascii{C++}. The key path to the \tf{} runtime is \code{run\_step}. Because the real system implementation involves too much detail, it is not easy to find the backbone and logic of the algorithm. To simplify the description of the problem, the implementation of \code{run\_step} will be described formally.


\subsubsection{Master::RunStep}
On \ascii{Master}, the pruning operation of \code{FullGraph} is mainly completed, and \code{ClientGraph} is generated; then, \code{ClientGraph} is split into multiple \code{PartitionGraph} according to \ascii{Worker} Finally, register the \code{PartitionGraph} list with each \ascii{Worker} and launch each \ascii{Worker} concurrently to execute the \code{PartitionGraph} list.

\begin{leftbar}
\begin{python}
def run_step(workers, full_graph, inputs, outputs):
  client_graph = prune(full_graph, inputs, outputs)
  partition_graphs = split(client_graph, workers)
  register_graphs(partition_graphs, inputs, outputs)
  run_graphs(partition_graphs, inputs, outputs)
\end{python}
\end{leftbar}


\subsubsection{Worker::RunStep}
On a particular \ascii{Worker} node, when the \code{RegisterGraphRequest} message is received, the computed graph is split into multiple \code{PartitionGraph}s according to the local device set. Then, start a \code{Executor} on each computing device to execute the \code{PartitionGraph} assigned to it.

After a certain computing device executes the allocated \code{PartitionGraph}, the counter of \code{ExecutorBarrier} is added to \ascii{1} until all devices complete the execution of the \code{PartitionGraph} list, \code{barrier.wait ()} Blocking operation exits.

There may be data dependencies between \code{PartitionGraph} across devices, and they interact by inserting \code{Send/Recv} nodes. In fact, in a distributed runtime, \code{Send/Recv} does the data exchange via \code{RpcRemoteRendezvous}.

The %\code{Send} node will call \code{RpcRemoteRendezvous::Send}, which delegates the \code{LocalRendezvous} data locally. The \code{Recv} node gets the data according to the identifier call \code{RpcRemoteRendezvous::Recv}. At this point, there may be two situations.

% \begin{enum}
% \eitem{The original device is in the same \ascii{Worker} as the target device: \code{RpcRemoteRendezvous::Recv} will directly delegate \code{LocalRendezvous::Recv} locally;
% \eitem{The original device is not in the same \ascii{worker} as the target device: \code{RpcRemoteRendezvous::Recv} sends a \code{RecvTensorRequest} request to the \ascii{Worker} where the target device is located; target\ascii{Worker } will get the data locally from \code{LocalRendezvous::Recv} and return a \code{RecvTensorResponse} message to the peer. }
% \end{enum}

\begin{leftbar}
\begin{python}
def send_inputs(remote_rendezvous, inputs):
  for (key, tensor) in inputs:
    remote_rendezvous.send(key, tensor)

def do_run_partitions(executors_and_partitions):
  barrier = ExecutorBarrier(executors_and_partitions.size())
  for (executor, partition) in executors_and_partitions:
    executor.run(partition, barrier.on_done())  
  barrier.wait()

def recv_outputs(remote_rendezvous, outputs):
  for (key, tensor) in outputs:
    remote_rendezvous.recv(key, tensor)

def run_partitions(executors_and_partitions, inputs, outputs):
  remote_rendezvous = RpcRemoteRendezvous()
  send_inputs(remote_rendezvous, inputs)
  do_run_partitions(executors_and_partitions)
  recv_outputs(remote_rendezvous, outputs)

def run_step(devices, full_graph, inputs, outputs):
  executors_and_partitions = split(full_graph, devices)
  run_partitions(executors_and_partitions, inputs, outputs)
\end{python}
\end{leftbar}


\subsection{Domain model}
As shown in \refig{cc-dist-model}, there is a sophisticated domain model in the \tf{} distributed runtime.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/cc-dist-model.png}
  \caption{Distributed: Domain Model}
  \label{fig:cc-dist-model}
\end{figure}


\subsubsection{Cluster}
\ascii{Cluster} is described using \ascii{ClusterSpec}, which can be divided into one or more \ascii{Job}, one \ascii{Job} contains one or more \ascii{Task}. That is, the \ascii{TensorFlow} cluster is composed of the task set \ascii{(Task Set)} that executes the calculation graph.

Each \ascii{Task} can be run on a separate machine, or multiple \ascii{Task} can be run on a single machine (for example, stand-alone multi-ascii{CPU}, or stand-alone multi-ascii{GPU} ).


\subsubsection{Job}
Assign the same \ascii{Task} to the same \ascii{Job}. Each \ascii{Job} uses a unique identifier of \code{job\_id}.

In general, there are two basic \ascii{Job} types in the model training process of distributed deep learning:

\begin{enum}
  \eitem{\ascii{ps}: responsible for storing and updating model parameters;}
  \eitem{\ascii{worker}: Responsible for computationally intensive model training and reasoning. }
\end{enum}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/py-dist-ps-worker.png}
  \caption{Distributed Model Training: Interaction between PS and Worker}
  \label{fig:py-dist-ps-worker}
\end{figure}


\subsubsection{Task}
In general, in a distributed runtime, \ascii{Task} runs in a separate process and runs a \code{tf.train.Server} instance on it. Where \ascii{Task} uses the binary unique identifier of \code{job\_id:task\_index}.


\subsubsection{Server}
\ascii{Server} represents the \ascii{Task} service process, which provides \code{MasterService} and \code{WorkerService} services. In other words, \ascii{Server} can play both \ascii{Master} and \ascii{Worker} roles.


\subsection{Building a cluster}
In the distributed \tf{} runtime, each \ascii{Task} launches a \ascii{Server} and provides the \code{MasterService} service and the \code{WorkerService} service. Among them, the formation of \ascii{TensorFlow} cluster consists of two basic steps:

\begin{enum}
  \eitem{Create\code{tf.train.ClusterSpec}, which describes the deployment information of \ascii{Task} in the cluster and is organized as \ascii{Job};}
  \eitem{For each \ascii{Task}, start a \code{tf.train.Server} instance. }
\end{enum}


\subsubsection{Cluster configuration}
\code{ClusterSpec} describes the deployment information for \ascii{Task} in the cluster and is organized as \ascii{Job}. In general, in a distributed execution mode, a process is started for each \ascii{Task}. Therefore, \code{ClusterSpec} also describes the process distribution of the \ascii{TensorFlow} distributed runtime.

For example, there is a \ascii{TensorFlow} cluster consisting of \code{ps} and \code{worker}\ascii{Job}. Where \code{ps} is deployed on \code{ps0:2222, ps1:2222}; \code{worker} is deployed on \code{worker0:2222, worker1:2222, worker2:2222}.

\begin{leftbar}
\begin{python}
tf.train.ClusterSpec({
  "worker": [
    "worker0:2222",   # /job:worker/task:0
    "worker1:2222",   # /job:worker/task:1
    "worker2:2222"    # /job:worker/task:2
  ],  
  "ps": [
    "ps0:2222",       # /job:ps/task:0
    "ps1:2222"        # /job:ps/task:0
  ]})
\end{python}
\end{leftbar}

In this case, the index of \ascii{Task} is not explicitly specified. By default, in the \ascii{Task} collection of \ascii{Job}, the \ascii{Task} index is incremented sequentially from \ascii{0}.


\subsubsection{Protobuf description}

\begin{leftbar}
\begin{python}
message JobDef {
  string name = 1;
  map<int32, string> tasks = 2;
}

message ClusterDef {
  repeated JobDef job = 1;
}
\end{python}
\end{leftbar}

The \code{tasks} keyword represents \code{task\_index} and the value represents \code{host:port}.

\end{content}


\section{Master service}
\begin{content}

\code{MasterService} is a \ascii{RPC} service. When \ascii{Client} accesses the \ascii{Server} instance according to \code{target}, \ascii{Server} plays the role of \ascii{Master} and provides the \code{MasterService} service.

The interaction between \ascii{Client} and \ascii{Master} follows the interface specification defined by \code{MasterService}. In other words, \code{MasterService} defines the public contract for \ascii{Client} access\ascii{Master}, which is responsible for coordinating and controlling the execution of multiple \code{WorkerService}.


\subsection{Interface definition}
In the \code{master\_service.proto} file, all interfaces of \code{MasterService} are defined; in the \code{master.proto} file, the body of each interface is defined.

\begin{leftbar}
\begin{c++}
service MasterService {
  rpc CreateSession(CreateSessionRequest) 
      returns (CreateSessionResponse);
  
  rpc ExtendSession(ExtendSessionRequest) 
      returns (ExtendSessionResponse);

  rpc PartialRunSetup(PartialRunSetupRequest) 
      returns (PartialRunSetupResponse);

  rpc RunStep (RunStepRequest) 
      returns (RunStepResponse);
  
  rpc CloseSession(CloseSessionRequest) 
      returns (CloseSessionResponse);
  
  rpc ListDevices(ListDevicesRequest) 
      returns (ListDevicesResponse);

  rpc Reset(ResetRequest) 
      returns (ResetResponse);
}
\end{c++}
\end{leftbar}


\subsection{Access Service}
In general, \ascii{Client} uses the interface \code{MasterInterface} to get the remote \code{MasterService} service. In particular, all interfaces of \code{MasterInterface} are synchronous interfaces, making \ascii{Client} access to the remote \code{MasterService} service as if it were a local function.

Note that the \code{RunStepRequest/RunStepResponse} message may contain a large \code{Tensor} instance. In order to avoid unnecessary copying of objects, specialization implements a message wrapper.

\begin{leftbar}
\begin{c++}
// Abstract interface for communicating with the TensorFlow Master service.
//
// This interface supports both RPC-based master implementations, and
// in-process master implementations that do not require an RPC roundtrip.
struct MasterInterface {
  virtual ~MasterInterface() {}
  
  virtual Status CreateSession(
      CallOptions * call_options,
      const CreateSessionRequest* request,
      CreateSessionResponse* response) = 0;

  virtual Status ExtendSession(
      CallOptions * call_options,
      const ExtendSessionRequest* request,
      ExtendSessionResponse* response) = 0;

  virtual Status PartialRunSetup(
      CallOptions * call_options,
      const PartialRunSetupRequest* request,
      PartialRunSetupResponse* response) {
    return errors::Unimplemented(
      "Partial run not implemented for master");
  }

  Virtual Status RunStep (
      CallOptions * call_options,
      RunStepRequestWrapper * request,
      MutableRunStepResponseWrapper* response) = 0;

  // Wrapper classes for the `MasterService.RunStep` message.
  //
  // The `RunStepRequest/RunStepResponse` message can contain 
  // potentially large tensor data as part of its `feed/fetch` 
  // submessages.
  Virtual Status RunStep (
    CallOptions * call_options,
    const RunStepRequest* request,
    RunStepResponse * response) {
    std::unique_ptr<RunStepRequestWrapper> wrapped_request(
        new ProtoRunStepRequest(request));
    std::unique_ptr<MutableRunStepResponseWrapper> wrapped_response(
        new NonOwnedProtoRunStepResponse(response));
    return RunStep(call_options, 
        wrapped_request.get(), 
        wrapped_response.get());
  }

  // Returns a request object for use in calls to
  // `RunStep()`. Ownership is transferred to the caller.
  virtual MutableRunStepRequestWrapper* CreateRunStepRequest() {
    return new MutableProtoRunStepRequest;
  }

  // Returns a response object for use in calls to
  // `RunStep()`. Ownership is transferred to the caller.
  virtual MutableRunStepResponseWrapper* CreateRunStepResponse() {
    return new OwnedProtoRunStepResponse;
  }

  virtual Status CloseSession(
    CallOptions * call_options,
    const CloseSessionRequest* request,
    CloseSessionResponse* response) = 0;

  virtual Status ListDevices(
    CallOptions * call_options,
    const ListDevicesRequest* request,
    ListDevicesResponse* response) = 0;

  virtual Status Reset(
    CallOptions* call_options, const ResetRequest* request,
    ResetResponse* response) = 0;
};
\end{c++}
\end{leftbar}

As shown by \refig{dist-master-interface}, there are two basic implementations of \code{MasterInterface}.

\begin{enum}
  \eitem{distributed: \code{GrpcRemoteMaster} based on \ascii{gRPC}, \ascii{Client} and \ascii{Master} are deployed in two different processes;}
  \eitem{Local mode: Based on the \code{LocalMaster} implementation of the function call, \ascii{Client} is in the same process as \ascii{Master}. }
\end{enum}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-master-interface.png}
  \caption{\code{MasterInterface}}
  \label{fig:dist-master-interface}
\end{figure}

In distributed mode, \code{GrpcRemoteMaster} uses pseudo-code similar to the following and gets the remote \code{MasterService} service via \ascii{gRPC}.

\begin{leftbar}
\begin{c++}
stub = NewStub("/job:worker/replica:0/task:0")
handle = stub->CreateSession({graph_def})
do {
  stub->RunStep(handle, feeds, fetches);
} while (!should_stop());
stub->CloseSession({handle})
\end{c++}
\end{leftbar}


\subsection{RPC procedure}
As shown by \refig{dist-client-master-interaction}, \ascii{Client} gets the service of remote\ascii{MasterService} via \code{MasterInterface}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/dist-client-master-interaction.png}
  \caption{Client to get the principle of MasterService}
  \label{fig:dist-client-master-interaction}
\end{figure}

Among them, \code{GrpcRemoteMaster} is an implementation of the \ascii{gRPC} client, which finally obtains the \code{GrpcMasterService} service on the remote \ascii{Master} via \code{Stub}, making its behavior behave as if Local function calls are generic. Among them, \code{GrpcMasterService} implements all the service interfaces defined by \code{MasterService}, which is the real service entity of \code{MasterService}.

\begin{remark}
Strictly speaking, \script{GrpcSession, ClientMaster, GrpcRemoteMaster} are all part of the \ascii{Client} implementation. Rather than commonly understood, the \ascii{Python} front-end system is a full \ascii{Client} implementation, and the backend \ascii{C++} backend system does not include any implementation of \ascii{Client}.
\end{remark}


\subsection{Message definition}
Next, we'll take a closer look at the message definitions for each interface. Among them, the most important thing is to identify the identity of each service. For example, \ascii{Master} can be accessed by multiple \ascii{Client} and generate a corresponding \code{MasterSession} instance for each \ascii{Client}. So \code{GrpcSession} holds the \code{MasterSession} handle and implements \ascii{Client} to get the \ascii{Master} service.


\subsubsection{CreateSession}
As shown in \refig{dist-ms-create-sess-req}, the \code{CreateSessionRequest} message carries the initial calculation graph and establishes a connection with \ascii{Master} specified by \code{target}. When \ascii{Master} receives the request message, it creates a corresponding \code{MasterSession} instance and uniquely identifies the \code{MasterSession} instance using \code{session\_handle}.

After the \ascii{Master} logic is processed, return the message \code{CreateSessionResponse} to \ascii{Client}. The \code{CreateSessionResponse} message carries \code{session\_handle}, and the \code{GrpcSession} on the \ascii{Client} side is associated with \code{MasterSession} on the \ascii{Master} side. Subsequently, in all interactions between \ascii{Client} and \ascii{Master}, the \ascii{MasterSession} corresponding to it is indexed in the request message by carrying \code{session\_handle}, \ascii{Master} Example.

In addition, \code{CreateSessionResponse} also carries the initial \code{graph\_version} for subsequent initiating \code{ExtendSession} operations, adding new nodes to the original calculation graph.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-ms-create-sess-req.png}
  \caption{\code{CreateSession}}
  \label{fig:dist-ms-create-sess-req}
\end{figure}

\begin{leftbar}
\begin{c++}
message CreateSessionRequest {
  GraphDef graph_def = 1;
  ConfigProto config = 2;
  string target = 3;
}

message CreateSessionResponse {
  string session_handle = 1;
  int64 graph_version = 2;
}
\end{c++}
\end{leftbar}


\subsubsection{ExtendSession}
After \ascii{CreateSession} succeeds, the subsequent \ascii{Client} can pass the subgraph to be expanded to \ascii{Master} via \code{ExtendSession}, increasing the size of the original calculation graph (only submaps can be added) Cannot modify or delete nodes).

As shown in \refig{dist-ms-extend-sess-req}, you need to carry \code{current\_graph\_version}, \ascii{Master} for version matching verification in the request message; wait for \code{ExtendSession} After the logical processing is completed, \code{new\_graph\_version} is carried in the response message for the next \code{ExtendSession} operation. The initial \code{graph\_version} is carried by \code{CreateSessionResponse} to \ascii{Client}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/dist-ms-extend-sess-req.png}
  \caption{\code{ExtendSession}}
  \label{fig:dist-ms-extend-sess-req}
\end{figure}

\begin{leftbar}
\begin{c++}
message ExtendSessionRequest {
  string session_handle = 1;

  // REQUIRED: The nodes to be added to the session's graph. 
  // If any node has the same name as an existing node, 
  // the operation will fail with ILLEGAL\_ARGUMENT.
  GraphDef graph_def = 2;

  // REQUIRED: The version number of the graph to be extended. 
  // This will be tested against the current server-side version 
  // number, and the operation will fail with FAILED\_PRECONDITION 
  // if they do not match.
  int64 current_graph_version = 3;
}

message ExtendSessionResponse {
  // The new version number for the extended graph, 
  // to be used in the next call to ExtendSession.
  int64 new_graph_version = 4;
}
\end{c++}
\end{leftbar}

\subsubsection{RunStep}
In general, \code{RunStep} is iteratively executed on the client side. As shown in \refig{dist-ms-run-step-req}, during each \code{RunStep} execution, \ascii{Client} carries \code{feed, fetch, target} in the request message, respectively Indicates the list of \ascii{NamedTensor} entered, the name list of \ascii{Tensor} to be output, the name list of \ascii{OP} to be executed, and \code{tensor} in the response message, corresponding to \code{fetch A list of names for }, a list of \ascii{Tensor} output.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/dist-ms-run-step-req.png}
  \caption{\code{RunStep}}
  \label{fig:dist-ms-run-step-req}
\end{figure}

\begin{leftbar}
\begin{c++}
message RunStepRequest {
  string session_handle = 1;

  repeated NamedTensorProto feed = 2;
  repeated string fetch = 3;
  repeated string target = 4;

  RunOptions options = 5;
  string partial_run_handle = 6;
}

message RunStepResponse {
  repeated NamedTensorProto tensor = 1;
  RunMetadata metadata = 2;
}
\end{c++}
\end{leftbar}


\subsubsection{CloseSession}
When the calculation is complete, you need to close the session and release the system computing resources. As shown in \refig{dist-ms-closs-sess}, \ascii{Client} initiates the release of computing resources by sending \code{CloseSession} to \ascii{Master}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/dist-ms-closs-sess.png}
  \caption{\code{CloseSession}}
  \label{fig:dist-ms-closs-sess}
\end{figure}

\begin{leftbar}
\begin{c++}
message CloseSessionRequest {
  string session_handle = 1;
}

message CloseSessionResponse {
}
\end{c++}
\end{leftbar}

\end{content}


\section{Worker service}
\begin{content}
\code{WorkerService} is also a \ascii{gRPC} service that dispatches local device sets to execute local submaps. It defines the interface specification for accessing \ascii{Worker}, which is defined in \code{master\_service.proto}.

\ascii{Master} finds other \ascii{Server} instances in the cluster based on the \code{ClusterSpec} information, at which point these \ascii{Server} instances will play the role of \ascii{Worker}. \ascii{Master} distributes the submap to each \ascii{Worker} node and starts the execution of the subgraph calculation for each \ascii{Worker} node.

If there is a data dependency between \ascii{Worker}, the interaction is done through interprocess communication. Among them, between \ascii{Master} and \ascii{Worker}, the interaction between \ascii{Worker} and \ascii{Worker} follows the interface specification defined by \code{WorkerService}.


\subsection{Interface definition}
In the \code{worker\_service.proto} file, all interfaces of \code{WorkerService} are defined; in the \code{worker.proto} file, the body of each interface is defined.

\begin{leftbar}
\begin{c++}
service WorkerService {
  rpc GetStatus(GetStatusRequest) 
      returns (GetStatusResponse);

  rpc CreateWorkerSession(CreateWorkerSessionRequest)
      returns (CreateWorkerSessionResponse);

  rpc RegisterGraph(RegisterGraphRequest) 
      returns (RegisterGraphResponse);

  rpc DeregisterGraph(DeregisterGraphRequest) 
      returns (DeregisterGraphResponse);

  rpc RunGraph(RunGraphRequest) 
      returns (RunGraphResponse);

  rpc CleanupGraph(CleanupGraphRequest) 
      returns (CleanupGraphResponse);

  rpc CleanupAll(CleanupAllRequest) 
      returns (CleanupAllResponse);

  rpc RecvTensor(RecvTensorRequest) 
      returns (RecvTensorResponse) {
  }

  rpc Logging(LoggingRequest) 
      returns (LoggingResponse);

  rpc Tracing(TracingRequest) 
      returns (TracingResponse);
}
\end{c++}
\end{leftbar}


\subsection{Access Service}
In general, \ascii{Master/Worker} uses the interface \code{WorkerInterface} to get the service of the remote \code{WorkerService}. Where \code{WorkerInterface} defines the interface for asynchronous access to \code{WorkerService}; similar to \code{MasterInterface}, since \code{RunGraphRequest/RunGraphResponse} may contain a larger \code{Tensor}, in order to avoid A necessary copy of the object, specializing in the wrapper that implements the message.

\begin{leftbar}
\begin{c++}
struct WorkerInterface {
  // async interfaces.
  virtual void GetStatusAsync(
      const GetStatusRequest* request,
      GetStatusResponse* response,
      StatusCallback done) = 0;

  virtual void CreateWorkerSessionAsync(
      const CreateWorkerSessionRequest* request,
      CreateWorkerSessionResponse* response, 
      StatusCallback done) = 0;

  virtual void RegisterGraphAsync(
      const RegisterGraphRequest* request,
      RegisterGraphResponse* response,
      StatusCallback done) = 0;

  virtual void DeregisterGraphAsync(
      const DeregisterGraphRequest* request,
      DeregisterGraphResponse* response,
      StatusCallback done) = 0;

  virtual void RunGraphAsync(
      CallOptions * opts, 
      RunGraphRequestWrapper* request,
      MutableRunGraphResponseWrapper* repsonse,
      StatusCallback done) = 0;

  // Wrapper classes for the `WorkerService.RunGraph` message.
  //
  // The `RunGraphRequest/RunGraphResponse` message can contain 
  // potentially large tensor data as part of its `send/response`
  // submessages.
  virtual void RunGraphAsync(
      CallOptions * opts, 
      const RunGraphRequest* request,
      RunGraphResponse* response, 
      StatusCallback done) {
    RunGraphRequestWrapper* wrapped_request = 
        new ProtoRunGraphRequest(request);
    MutableRunGraphResponseWrapper* wrapped_response =
        new NonOwnedProtoRunGraphResponse(response);
    RunGraphAsync(opts, wrapped_request, wrapped_response,
        [wrapped_request, wrapped_response, done](const Status& s) {
            done(s);
            delete wrapped_request;
            delete wrapped_response;
        });
  }

  // Returns a request object for use in calls to
  // `RunGraphAsync()`. Ownership is transferred to the caller.
  virtual MutableRunGraphRequestWrapper* CreateRunGraphRequest() {
    return new MutableProtoRunGraphRequest;
  }

  // Returns a response object for use in calls to
  // `RunGraphAsync()`. Ownership is transferred to the caller.
  virtual MutableRunGraphResponseWrapper* CreateRunGraphResponse() {
    return new OwnedProtoRunGraphResponse;
  }

  virtual void CleanupGraphAsync(
      const CleanupGraphRequest* request,
      CleanupGraphResponse* response,
      StatusCallback done) = 0;

  virtual void CleanupAllAsync(
      const CleanupAllRequest* request,
      CleanupAllResponse* response,
      StatusCallback done) = 0;

  virtual void RecvTensorAsync(
      CallOptions * opts,
      const RecvTensorRequest* request,
      TensorResponse* response,
      StatusCallback done) = 0;

  virtual void LoggingAsync(
      const LoggingRequest* request,
      LoggingResponse* response, 
      StatusCallback done) = 0;

  virtual void TracingAsync(
      const TracingRequest* request,
      TracingResponse* response, 
      StatusCallback done) = 0;
};
\end{c++}
\end{leftbar}


\code{WorkerInterface} also defines a synchronous access interface. The synchronization interface is implemented indirectly on the asynchronous interface through the adapter of \code{CallAndWait}. In particular, the synchronous interface makes \ascii{Master/Worker} call the remote \code{WorkerService} as if it were a local function.

\begin{leftbar}
\begin{c++}
struct WorkerInterface {
  // sync interfaces.
  Status GetStatus(
      const GetStatusRequest* request,
      GetStatusResponse* response) {
    return CallAndWait(&ME::GetStatusAsync, request, response);
  }

  Status CreateWorkerSession(
      const CreateWorkerSessionRequest* request,
      CreateWorkerSessionResponse* response) {
    return CallAndWait(&ME::CreateWorkerSessionAsync, request, response);
  }

  Status RegisterGraph(
      const RegisterGraphRequest* request,
      RegisterGraphResponse* response) {
    return CallAndWait(&ME::RegisterGraphAsync, request, response);
  }

  Status DeregisterGraph(
      const DeregisterGraphRequest* request,
      DeregisterGraphResponse* response) {
    return CallAndWait(&ME::DeregisterGraphAsync, request, response);
  }

  Status CleanupGraph(
      const CleanupGraphRequest* request,
      CleanupGraphResponse* response) {
    return CallAndWait(&ME::CleanupGraphAsync, request, response);
  }

  Status CleanupAll(
      const CleanupAllRequest* request,
      CleanupAllResponse* response) {
    return CallAndWait(&ME::CleanupAllAsync, request, response);
  }

  Status Logging(
      const LoggingRequest* request, 
      LoggingResponse* response) {
    return CallAndWait(&ME::LoggingAsync, request, response);
  }

  Status Tracing(
      const TracingRequest* request, 
      TracingResponse* response) {
    return CallAndWait(&ME::TracingAsync, request, response);
  }
 
 private:
  typedef WorkerInterface ME;

  template <typename Method, typename Req, typename Resp>
  Status CallAndWait(Method func, const Req* req, Resp* resp) {
    Status right;
    Notification n;
    (this->*func)(req, resp, [&ret, &n](const Status& s) {
      right = s;
      n.Notify();
    });
    n.WaitForNotification();
    return right;
  }
};
\end{c++}
\end{leftbar}

In particular, instances generated by \code{WorkerInterface} are deleted by \code{WorkerCacheInterface::ReleaseWorker}. Therefore, in order to avoid external illegal deletion of the \code{WorkerInterface} instance, limit the destructor of \code{WorkerInterface} to \code{protected} and declare \code{WorkerCacheInterface} as a friend.

\begin{leftbar}
\begin{c++}
struct WorkerInterface {
 protected:
  virtual ~WorkerInterface() {}
  friend class WorkerCacheInterface;
};
\end{c++}
\end{leftbar}

As shown by \refig{dist-worker-interface}, there are two implementations of \code{WorkerService}. Among them, in local mode, \code{GrpcWorker} is used directly; in distributed mode, \ascii{Worker} is deployed in a different process, using \code{GrpcRemoteWorker}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-worker-interface.png}
  \caption{\code{WorkerInterface}interface}
  \label{fig:dist-worker-interface}
\end{figure}


\subsection{RPC procedure}
As shown in \refig{dist-worker-interaction}, in distributed mode, \code{GrpcRemoteWorker} is an implementation of the \ascii{gRPC} client, which eventually gets the remote \ascii{ via \code{Stub} The \code{GrpcWorkerService} service on Worker} makes its behavior behave like a native function call. Among them, \code{GrpcWorkerService} implements all the service interfaces defined by \code{WorkerService}.

\begin{remark}
Strictly speaking, \script{GrpcRemoteWorker} is part of the \ascii{Master} or peer\ascii{Worker} implementation.
\end{remark}

In the local mode, the \ascii{WorkerService} service is directly obtained through the function call of \code{GrpcWorker}, which avoids extra network transmission overhead.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/dist-worker-interaction.png}
  \caption{Get the RPC process of \code{MasterService}}
  \label{fig:dist-worker-interaction}
\end{figure}


\subsection{Message definition}
Next, we will take a closer look at the message definitions for each interface of \code{WorkerService}. Among them, the most important thing is to identify the identity of each service. When \code{WorkerSession} is created, the \code{MasterSession} identifier is passed to \ascii{Worker}, which implements \code{MasterSession} to uniformly manage multiple dependent \code{WorkerSession} instances.

When \code{Worker} first completes \code{RegisterGraph}, it returns a unique \code{graph\_handle} to \code{Master} to identify the graph instance. Therefore, the \code{(session\_handle, graph\_handle)} binary group can be used to uniquely identify the graph instance within the cluster.

When \ascii{Master} broadcasts each \ascii{Worker} concurrently to \code{RunGraph}. To distinguish between different \code{step}, \ascii{Master} generates a globally unique \code{step\_id} and passes it to each \ascii{Worker} via \code{RunGraph}.

\begin{enum}
  \eitem{\code{session\_handle}: Automatically generated when creating a \code{MasterSession} instance, carried to \ascii{Client} via \code{CreateSessionResponse}; \ascii{Worker} via \code{CreateWorkerSessionRequest}; }
  \eitem{\code{graph\_id}: The first time \code{RegisterGraph} is generated by \code{Worker}, passed to \code{Master} via \code{RegisterGraphResponse};}
  \eitem{\code{step\_id}: Each time \code{RunStep}, a unique identifier is generated by \code{Master}, which is carried to \ascii{Worker} via \code{RunGraphRequest}. }
\end{enum}


\subsubsection{CreateWorkerSession}
As shown in \refig{dist-worker-create-worker-sess}, the \code{CreateWorkerSessionRequest} message carries the \code{session\_handle} assigned by \code{MasterSession}. When \ascii{Worker} receives the request message, it generates a \code{WorkerSession} instance and uniquely identifies the instance within \ascii{Worker} using \code{session\_handle}.

In the same cluster, for a \code{MasterSession} instance, the other \ascii{Worker} receives the same \code{session\_handle}. In this way, the \code{MasterSession} instance can uniformly manage all \code{WorkerSession} instances that belong to it.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-worker-create-worker-sess.png}
  \caption{\code{CreateWorkerSession}}
  \label{fig:dist-worker-create-worker-sess}
\end{figure}

\begin{leftbar}
\begin{c++}
message CreateWorkerSessionRequest {
  string session_handle = 1;
  ServerDef server_def = 2;
}

message CreateWorkerSessionResponse {
}
\end{c++}
\end{leftbar}


\subsubsection{RegisterGraph}
As shown in \refig{dist-worker-register-graph}, the \code{RegisterGraphRequest} message carries the \code{session\_handle} assigned by \code{MasterSession}, and its subgraph instance \ascii{graph\_def} . When \code{Worker} completes the subgraph registration and its initialization, it returns \code{graph\_handle} of the submap to \ascii{Master}.

It should be noted that \code{Master} will only execute \code{RegisterGraph} once, unless the node of the calculation graph is reorganized, or the \code{Master} process is restarted.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-worker-register-graph.png}
  \caption{\code{RegisterGraph}}
  \label{fig:dist-worker-register-graph}
\end{figure}

\begin{leftbar}
\begin{c++}
message RegisterGraphRequest {
  string session_handle = 1;

  GraphDef graph_def = 2;
  bool has_control_flow = 3 [deprecated = true];

  GraphOptions graph_options = 4;
  DebugOptions debug_options = 5;
}

message RegisterGraphResponse {
  string graph_handle = 1;
}
\end{c++}
\end{leftbar}


\subsubsection{DeregisterGraph}
As shown in \refig{dist-worker-deregister-graph}, when the submap on the \code{Worker} node is no longer needed (for example, the calculation graph is rescheduled, the nodes in the graph are rearranged), at this time\ Code{Master} sends a \code{DeregisterGraph} message to \ascii{Worker} so that \code{Worker} unregisters the submap instance.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-worker-deregister-graph.png}
  \caption{\code{DeregisterGraph}}
  \label{fig:dist-worker-deregister-graph}
\end{figure}

\begin{leftbar}
\begin{c++}
message DeregisterGraphRequest {
  string session_handle = 2;
  string graph_handle = 1;
}

message DeregisterGraphResponse {
}
\end{c++}
\end{leftbar}


\subsubsection{RunGraph}
When executing the submap registered on the \ascii{Worker} node, in order to distinguish between different \code{step}, \ascii{Master} generates a unique \code{step\_id} and passes it to each \ascii{Worker}, each \ascii {Worker} implements data collaboration through \code{step\_id}.

In addition, \code{RunGraphRequest} carries \code{send, recv\_key}, which respectively represent the \code{Tensor} identifier and data entered by the submap, and the \code{Tensor} identifier of the submap output. \code{RunGraphResponse} returns a list of \code{Tensor} corresponding to \code{recv\_key}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-worker-run-graph.png}
  \caption{\code{RunGraph}}
  \label{fig:dist-worker-run-graph}
\end{figure}

\begin{leftbar}
\begin{c++}
message RunGraphRequest {
  string session_handle = 8;
  string graph_handle = 1;
  int64 step_id = 2;

  ExecutorOpts exec_opts = 5;

  repeated NamedTensorProto send = 3;
  repeated string recv_key = 4;

  bool is_partial = 6;
  bool is_last_partial_run = 7;
}

message RunGraphResponse {
  repeated NamedTensorProto recv = 1;

  // execution stats
  StepStats step_stats = 2;
  CostGraphDef cost_graph = 3;
  repeated GraphDef partition_graph = 4;
}
\end{c++}
\end{leftbar}


\subsubsection{RecvTensor}
When executing \code{step} one time, if two \ascii{Worker} need to interact with data, the consumer sends a \code{RecvTensorRequest} message to the producer, by carrying \code{(step\_id, rendezvous\_key) } Binary group, request the corresponding \code{Tensor} data of the peer \ascii{Worker} and return it via \code{RecvTensorResponse}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-worker-recv-tensor.png}
  \caption{\code{RecvTensor}}
  \label{fig:dist-worker-recv-tensor}
\end{figure}

\begin{leftbar}
\begin{c++}
message RecvTensorRequest {
  int64 step_id = 1;
  string rendezvous_key = 2;

  // If true, use an out-of-band DMA mechanism to transfer the
  // received tensor.
  bool dma_ok = 3;

  // Optional information on client-side device locality.
  DeviceLocality client_locality = 4;

  // Optional information on server-side device locality.
  DeviceLocality server_locality = 5;

  // Optional information needed by the RPC subsystem.
  google.protobuf.Any transport_options = 6;
}

message RecvTensorResponse {
  // The tensor as a proto.
  TensorProto tensor = 1;

  // If true, this tensor was the output of a dead node, and the
  // content is invalid.
  bool is_dead = 2;

  // The time at which tensor was available and started to be returned.
  int64 send_start_micros = 3;

  // Optional additional information about how to receive the tensor,
  // e.g. in the event that `RecvTensorRequest.dma\_ok` was true.
  google.protobuf.Any transport_options = 4;
}
\end{c++}
\end{leftbar}

\end{content}


\section{Server}
\begin{content}
\code{Server} is a server based on \ascii{gRPC} that manages the local device set. It provides the \code{MasterService} service and the \code{WorkerService} service externally, with the roles of \ascii{Master} and \ascii{Worker}.


\subsection{Domain model}
As shown by \refig{cc-server-model}, \code{GrpcServer} provides the \code{MasterService} service when it plays the role of \ascii{Master}; where it is the \ascii{Client for each access. } Start a \code{MasterSession} instance and identify it with a globally unique \code{session\_handle}. In other words, \ascii{Master} can access multiple \ascii{Client}, while a \ascii{Client} can only access a specific \ascii{Master}.

\code{GrpcServer} provides the \code{WorkerService} service when playing the role of \ascii{Worker}; where each \ascii{Worker} can provide computing services for multiple \ascii{Master}, it is for each Generate a corresponding \code{WorkerSession} instance from the \code{MasterSession} requesting the computing service, and wait for the corresponding \code{MasterSession} to issue the \emph{registration} and \emph{execute} commands for the calculation graph.

The entire \code{GrpcServer} instance is hosted on the \code{grpc::Server} process, which listens for messages on specific ports and automatically dispatches the corresponding message to \code{MasterService} or \code{WorkerService} when the message arrives. The callback function being processed.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/cc-server-model.png}
  \caption{Server domain model}
  \label{fig:cc-server-model}
\end{figure}


\subsubsection{ProtobufDescription}
When \code{protocol} is \code{grpc}, the \code{GrpcServer} instance based on \ascii{gRPC} is enabled when the system is running. In addition, the configuration of runtime parameters can be implemented via \code{ConfigProto}. In other words, the \tf{} architecture is open to the public. For example, by extending \code{protocol} to support new communication protocols, implement a \code{Server} instance based on the new protocol.

\begin{leftbar}
\begin{python}
message ServerDef {
  ClusterDef cluster = 1;
  
  string job_name = 2;
  int32 task_index = 3;

  ConfigProto default_session_config = 4;
  string protocol = 5;
}
\end{python}
\end{leftbar}


\subsubsection{Service Interconnect}
As shown by \refig{cc-server-interact}, an \ascii{Server} instance is interconnected with other \ascii{Server} instances in the cluster via \code{tf.train.ClusterSpec}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/cc-server-interact.png}
  \caption{service interconnect}
  \label{fig:cc-server-interact}
\end{figure}

As shown in \refig{cc-server-interact-1}, when \ascii{Client} accesses one of \ascii{Server}, it plays the role of \ascii{Master}, and other \ascii{Server} Then played the role of \ascii{Worker}. In particular, \ascii{Server} accessed by \ascii{Client} also plays the role of \ascii{Worker}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/cc-server-interact-1.png}
  \caption{Single Client Access Cluster}
  \label{fig:cc-server-interact-1}
\end{figure}

As shown by \refig{cc-server-interact-2}, there may be multiple \ascii{Client} accessing different \ascii{Server} instances. At this point, the \ascii{Server} instance accessed by \ascii{Client} plays the \ascii{Master} role. However, the \ascii{Server} instance plays the \ascii{Worker} role relative to the other \ascii{Server} instances in the cluster.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/cc-server-interact-2.png}
  \caption{Multi-Client Access Cluster}
  \label{fig:cc-server-interact-2}
\end{figure}

In particular, \ascii{Client} and \ascii{Master} can be deployed in the same process. At this point, the interaction between \ascii{Client} and \ascii{Master} is much simpler, and the two directly use function calls, avoiding the extra overhead of \ascii{gRPC} interaction. And so on, in the same \ascii{Server}, \ascii{Master} and \ascii{Worker} can be deployed in the same process. At this point, the function call is used directly between \ascii{Master} and \ascii{Worker}.


\subsection{State machine}
As shown by \refig{dist-grpc-server-state-machine}, \code{GrpcServer} is a server based on \code{grpc::Server} that manages and maintains a simple state machine.

\code{GrpcServer} starts the \code{grpc::Server} service on the \code{New} state, but does not provide services externally; instead, it starts the service on the \code{Started} state and provides \code{MasterService } and \code{WorkerService}'s \code{RPC} message service; eventually, the \code{MasterService} and \code{WorkerService} services are stopped in the \code{Stopped} state.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/dist-grpc-server-state-machine.png}
  \caption{GrpcServer state machine}
  \label{fig:dist-grpc-server-state-machine}
\end{figure}


\subsubsection{Create Service}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/dist-grpc-server-factory.png}
  \caption{Polymorphic creation of Server instance}
  \label{fig:dist-grpc-server-factory}
\end{figure}

\begin{leftbar}
\begin{c++}
struct GrpcServerFactory : ServerFactory {
  bool AcceptsOptions(const ServerDef& server_def) override {
    return server_def.protocol() == "grpc";
  }

  Status NewServer(const ServerDef& server_def,
      std::unique_ptr<ServerInterface>* out_server) override {
    GrpcServer::Create(server_def, Env::Default(), out_server);
    return Status::OK();
  }
};
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
void GrpcServer::Create(
    const ServerDef& server_def, Env* env,
    std::unique_ptr<ServerInterface>* out_server) {
  auto ret = std::make_unique<GrpcServer>(server_def, env);
  direction> Init ();
  *out_server = std::move(ret);
}
\end{c++}
\end{leftbar}

As shown by \refig{cc-server-model}, \code{GrpcServer::Init} will complete the initialization of the \code{GrpcServer} domain object, including the following \ascii{3} basic procedures.

\begin{enum}
  \eitem{initialize \code{MasterEnv} instance;}
  \eitem{Initialize \code{WorkerEnv} instance;}
  \eitem{Create and start \code{grpc::Server}}
    \begin{enum}
    \eitem{initialize\code{MasterService}}
    \begin{nitemize}
      \eitem{Create a \code{Master} instance;}
      \eitem{Create a \code{MasterService} instance;}
    \end{nitemize}
    \eitem{initialize\code{WorkerService}}
    \begin{nitemize}
      \eitem{Create a \code{Worker} instance;}
      \eitem{Create a \code{WorkerService} instance. }
    \end{nitemize}
    \end{enum}
\end{enum}

In order to better understand the initialization process of the entire \code{GrpcServer} instance, the implementation is partially refactored here. First, it initializes the \code{MasterEnv, WorkerEnv} instance; then, creates and starts the \code{grpc::Server} server.

\begin{leftbar}
\begin{c++}
void GrpcServer::Init() {
  InitMasterEnv();
  InitWorkerEnv ();
  StartGrpcServer ();
}
\end{c++}
\end{leftbar}


\subsubsection{Initialize MasterEnv}
\code{MasterEnv} holds the context of the \code{Master} runtime, which has the same lifecycle as \code{GrpcServer}, so the entire \code{Master} runtime is visible.

As shown by \refig{dist-master-env}, \code{LocalDevices} is used to get the local device set; \code{WorkerCacheFactory} is used to create the \code{WorkerCacheInterface} instance; \code{WorkerCacheInterface} is used to create the \code {MasterInterface} instance, the latter is used to call the remote \code{MasterService} service; \code{MasterSessionFactory} is used to create the \code{MasterSession} instance; \code{OpRegisteryInterface} is used to query the specific \code{OP} element Data; \code{Env} is used to get the cross-platform \ascii{API} interface. Among them, the following will focus on the creation process of \code{WorkerCacheInterface}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/dist-master-env.png}
  \caption{\code{MasterEnv} model}
  \label{fig:dist-master-env}
\end{figure}


\subsubsection{Initialize WorkerEnv}
\code{WorkerEnv} holds the context of the \code{Worker} runtime, which has the same lifecycle as \code{GrpcServer}, so the entire \code{Worker} runtime is visible.

As shown in \refig{dist-worker-env}, \code{LocalDevices} is used to get the local device set; \code{DeviceManager} is used to manage the local device set and the remote device set; \code{SessionManager} is used for management \code{WorkerSession} collection; \code{RendezvousManager} is used to manage the \code{Rendezvous} instance set; \code{ThreadPool} will automatically assign a thread from the compute pool, starting \ascii{OP}\ascii{ The execution of the Kernel} operator; \code{Env} is used to get the cross-platform \ascii{API} interface.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/dist-worker-env.png}
  \caption{\code{WorkerEnv} model}
  \label{fig:dist-worker-env}
\end{figure}


\subsubsection{Start grpc::Server}
The system implementation uses the builder to create a \code{grpc::Server} instance. First, configure the \code{grpc::Server} service options; then, build the \code{MasterService} instance and the \code{WorkerService} instance respectively. Finally, call the \code{builder.BuildAndStart} method to start the \code{grpc::Server} server.

It should be noted that \code{GrpcServer} is still in the \code{New} state when \code{grpc::Server} is started.
\code{grpc::Server} has not yet provided the \code{MasterService} service and the \code{WorkerService} service. Until \code{GrpcServer} is migrated to the \code{Started} state location, \code{grpc::Server} will actually provide the \code{MasterService} service and \code{WorkerService} service.

\begin{leftbar}
\begin{c++}
void InitServerBuilder(::grpc::ServerBuilder& builder) {
  builder.AddListeningPort(
    strings::StrCat("0.0.0.0:", GetRequestedPort()),
    GetServerCredentials(server_def_), &bound_port_);
  builder.SetMaxMessageSize(std::numeric_limits<int32>::max());
  builder.SetOption(
      std::unique_ptr<::grpc::ServerBuilderOption>(new NoReusePortOption));
}

void GrpcServer :: StartGrpcServer () {
  ::grpc::ServerBuilder builder;

  InitServerBuilder(builder);
  InitMasterService(builder);
  InitWorkerService(builder);

  server_ = builder.BuildAndStart();  
}
\end{c++}
\end{leftbar}

It's easy to see that the \code{grpc::Server} entity that provides the \code{MasterService} service is the \code{GrpcMasterService} instance. When the message arrives, the corresponding message handler in the \code{GrpcMasterService} instance is automatically called back. Among them, in the message processing function, the processing of its business logic is completely dependent on the domain object of \code{Master}.

\begin{leftbar}
\begin{c++}
std::unique_ptr<Master> GrpcServer::CreateMaster(
    MasterEnv* master_env) {
  return std::make_unique<Master>(master_env);
}

AsyncServiceInterface* NewGrpcMasterService(
    Master* master, ::grpc::ServerBuilder* builder) {
  return new GrpcMasterService(master, builder);
}

void GrpcServer::InitMasterService() {
  master_impl_ = CreateMaster(&master_env_);
  master_service_ = NewGrpcMasterService(
      master_impl_.get(), &builder);  
}
\end{c++}
\end{leftbar}

By analogy, \code{grpc::Server} provides the \code{WorkerService} service entity to the \code{GrpcWorkerService} instance. When the message arrives, the corresponding message handler in the \code{GrpcWorkerService} instance is automatically called back. Among them, in the message processing function, the processing of its business logic is completely dependent on the domain object of \code{GrpcWorker}.

\begin{leftbar}
\begin{c++}
std::unique_ptr<GrpcWorker> NewGrpcWorker(WorkerEnv* env) {
  return std::unique_ptr<GrpcWorker>(new GrpcWorker(env));
}

AsyncServiceInterface* NewGrpcWorkerService(
    GrpcWorker* worker, ::grpc::ServerBuilder* builder) {
  return new GrpcWorkerService(worker, builder);
}

void GrpcServer::InitWorkerService(::grpc::ServerBuilder& builder) {
  worker_impl_ = NewGrpcWorker(&worker_env_);
  worker_service_ = NewGrpcWorkerService(
    worker_impl_.get(), &builder);
}
\end{c++}
\end{leftbar}


\subsubsection{Starting Service}
In the \code{New} state, \code{grpc::Server} has been started, but the \code{MasterService} service and \code{WorkerService} service are not provided externally. After calling the \code{GrpcServer::Start} method, the state of \code{GrpcServer} is migrated from \code{New} to the \code{Started} state, and two separate threads are started, starting \code{MasterService} respectively. And the message processor of \code{WorkerService}. At this point, \code{GrpcServer} officially provides \code{MasterService} and \code{WorkerService}.

\begin{leftbar}
\begin{c++}
Status GrpcServer::Start() {
  mutex_lock l(mu_);
  switch (state_) {
    case NEW: {
      master_thread_.reset(
          env_->StartThread(ThreadOptions(), "TF_master_service",
                            [this] { master_service_->HandleRPCsLoop(); }));
      worker_thread_.reset(
          env_->StartThread(ThreadOptions(), "TF_worker_service",
                            [this] { worker_service_->HandleRPCsLoop(); }));
      state_ = STARTED;
      return Status::OK();
    }
    case STARTED:
      LOG(INFO) << "Server already started(" << target() << ")";    
      return Status::OK();
    case STOPPED:
    default:
      CHECK(false);
  }
}
\end{c++}
\end{leftbar}


\subsubsection{Wait for terminating service}
In order to permanently provide the \code{MasterService} service and the \code{WorkerService} service, you need to implement the \code{join} operation on the threads \code{TF\_master\_service} and \code{TF\_worker\_service} respectively. Causes the main thread to hang until the two threads terminate.

By calling the \code{GrpcServer::Join} method, when \code{GrpcServer} is in the \code{Started} or \code{Stoped} state, it will automatically call the \code{Thread} destructor.

\begin{leftbar}
\begin{c++}
Status GrpcServer::Join() {
  mutex_lock l(mu_);
  switch (state_) {
    case NEW:
      // Prevent the server from being started subsequently.
      state_ = STOPPED;
      return Status::OK();
    case STARTED:
    case STOPPED:
      master_thread_.reset();
      worker_thread_.reset();
      return Status::OK();
    default:
      CHECK(false);
  }
}
\end{c++}
\end{leftbar}

For example, in \code{StdThread} based on the \code{C++} standard library implementation, its destructor will call the \code{join} method of \code{std::thread}.

\begin{leftbar}
\begin{c++}
struct StdThread : Thread {
  StdThread(const ThreadOptions&, const string&, 
      std::function<void()> fn)
    : thread_(fn) {
  }

  ~StdThread() override { 
    thread_.join(); 
  }

 private:
  std::thread thread_;
};
\end{c++}
\end{leftbar}


\subsubsection{Terminate service}
Unfortunately, the current \code{GrpcServer} does not gracefully exit. Therefore, in the engineering practice environment, the distributed runtime of \tf{} often requires the help of \code{Kubernetes} to implement automatic management of the \code{GrpcServer} service.

\begin{leftbar}
\begin{c++}
Status GrpcServer::Stop() {
  mutex_lock l(mu_);
  switch (state_) {
    case NEW:
      state_ = STOPPED;
      return Status::OK();
    case STARTED:
      return errors::Unimplemented(
          "Clean shutdown is not currently implemented");
    case STOPPED:
      LOG(INFO) << "Server already stopped(" << target() << ")";
      return Status::OK();
    default:
      CHECK(false);
  }
}
\end{c++}
\end{leftbar}


\subsection{Create WorkerCacheInterface}
After introducing the \code{GrpcServer} state machine model, go back to a previous question. \code{MasterEnv} holds the \code{WorkerCacheInterface} instance, which is used to query or delay the creation of \code{WorkerInterface}; where \code{WorkerInterface} is used to access the remote \code{WorkerService} service.


\subsubsection{Factory method: GrpcServer::WorkerCacheFactory}
When initializing \code{MasterEnv}, create a \code{WorkerCacheInterface} instance by calling the factory method \code{GrpcServer::WorkerCacheFactory}. Where \code{WorkerCacheFactoryOptions} is equivalent to \code{ServerDef}, which contains \code{ClusterDef} and its \code{job\_name:task\_index} information. Therefore, the \code{GrpcChannelSpec} instance obtained via \code{ParseChannelSpec} is equivalent to \code{ClusterSpec}, which contains the basic configuration information of the cluster.

\begin{leftbar}
\begin{c++}
Status GrpcServer::WorkerCacheFactory(
    const WorkerCacheFactoryOptions& options,
    WorkerCacheInterface** worker_cache) {

  GrpcChannelSpec channel_spec;
  TF_RETURN_IF_ERROR(ParseChannelSpec(options, &channel_spec));

  std::unique_ptr<GrpcChannelCache> channel_cache(
      NewGrpcChannelCache(channel_spec, GetChannelCreationFunction()));

  string name_prefix = strings::StrCat(
      "/job:", *options.job_name, "/replica:0",
      "/task:", options.task_index);

  *worker_cache = NewGrpcWorkerCacheWithLocalWorker(
      channel_cache.release(), worker_impl_.get(), name_prefix);
  return Status::OK();
}
\end{c++}
\end{leftbar}


\subsubsection{Factory method: NewGrpcChannelCache}
\code{NewGrpcChannelCache} is used to create \code{GrpcChannelCache} instances, and \code{GrpcChannelCache} can get or delay the creation of the corresponding \code{grpc::Channel} instance based on the name of \code{Worker}. Among them, a \ascii{Job} creates a \code{SparseGrpcChannelCache} instance, \code{MultiGrpcChannelCache} holds multiple \code{SparseGrpcChannelCache}, which is a typical combination mode application, which will be explained in detail below. The design of \code{GrpcChannelCache}.

\begin{leftbar}
\begin{c++}
GrpcChannelCache* NewGrpcChannelCache(
    const GrpcChannelSpec& spec,
    ChannelCreationFunction channel_func) {
  std::vector<GrpcChannelCache*> caches;
  for (auto& job : spec.host_ports_jobs()) {
    caches.push_back(
        new SparseGrpcChannelCache(
            job.job_id, job.host_ports, channel_func));
  }
  return new MultiGrpcChannelCache(caches);
}
\end{c++}
\end{leftbar}


\subsubsection{Factory method: NewGrpcWorkerCacheWithLocalWorker}
The factory method \code{NewGrpcWorkerCacheWithLocalWorker} is used to create a \code{GrpcWorkerCache} instance with local \code{Worker}.

\begin{leftbar}
\begin{c++}
WorkerCacheInterface* NewGrpcWorkerCacheWithLocalWorker(
    GrpcChannelCache* cc, WorkerInterface* local_worker,
    const string& local_target) {
  return new GrpcWorkerCache(cc, local_worker, local_target);
}
\end{c++}
\end{leftbar}


\subsubsection{Factory method: GrpcServer::GetChannelCreationFunction}
The design of \code{GetChannelCreationFunction} uses the idea of ​​\ascii{C++} functional programming, which returns a function object for creating a \code{grpc::Channel} instance. Unfortunately, the existing \code{NewHostPortGrpcChannel} function does not match the \code{ChannelCreationFunction} interface. Therefore, an adapter called \code{ConvertToChannelCreationFunction} is used here to transform \code{NewHostPortGrpcChannel} into \code{ChannelCreationFunction}.

\begin{leftbar}
\begin{c++}
using SharedGrpcChannelPtr = std::shared_ptr<::grpc::Channel>;
using ChannelCreationFunction = std::function<SharedGrpcChannelPtr(string)>;

Status NewHostPortGrpcChannel(const string& target,
    SharedGrpcChannelPtr* channel) {
  ::grpc::ChannelArguments args;
  args.SetInt("grapc.arg.max.message_length", 
              std::numeric_limits<int32>::max());
  args.SetInt("grpc.testing.fixed_reconnect_backoff_ms", 
              1000);

  *channel = ::grpc::CreateCustomChannel(
      "dns:///" + target, ::grpc::InsecureChannelCredentials(), args);
  return Status::OK();
}

ChannelCreationFunction ConvertToChannelCreationFunction(
  const std::function<Status(string, SharedGrpcChannelPtr*)>& new_channel) {
  return [new_channel_func](const string& target) -> SharedGrpcChannelPtr {
    SharedGrpcChannelPtr channel_ptr;
    if (new_channel(target, &channel_ptr).ok()) {
      return channel_ptr;
    } else {
      return nullptr;
    }
  };
}

ChannelCreationFunction GrpcServer::GetChannelCreationFunction() const {
  return ConvertToChannelCreationFunction(NewHostPortGrpcChannel);
}
\end{c++}
\end{leftbar}

At this point, we have straightened out the creation process of \code{GrpcChannelCache} and \code{WorkerCacheInterface}, but what are they used for? In fact, \code{WorkerCacheInterface} is used to get the \code{WorkerInterface} instance, which is used to access the remote \code{WorkerSerivice} service, which works very simply.

\begin{enum}
  \eitem{Get a list of all \code{Worker} names in the cluster;}
  \eitem{Create \ascii{RPC} channel according to the name of \code{Worker};}
  \eitem{Create a \code{GrpcRemoteWorker} instance from the \ascii{RPC} channel of \code{Worker}. }
\end{enum}

Where \code{GrpcRemoteWorker} is a concrete implementation of \code{WorkerInterface}; \code{GrpcChannelCache} is responsible for getting the name of \code{Worker} and its corresponding \code{grpc::Channel} for creating \code{Worker} .


\subsection{Create Worker's RPC Channel}
\code{GrpcChannelCache} is used to get or create the \ascii{RPC} channel of the remote \code{Worker} in the cluster. Where \code{ListWorkers} is used to return a list of names for \code{Worker} in the cluster. \code{TranslateTask} is used to convert the name of \code{Worker} to the address information of \code{host:port}. \code{FindWorkerChannel} looks for a \code{grpc::Channel} instance from the cache; if not found, dynamically creates a \code{grpc::Channel} instance based on the address information and adds it to the cache.

\begin{leftbar}
\begin{c++}
typedef std::shared_ptr<::grpc::Channel> SharedGrpcChannelPtr;

struct GrpcChannelCache {
  virtual ~GrpcChannelCache() {}
  virtual void ListWorkers(std::vector<string>* workers) const = 0;
  virtual SharedGrpcChannelPtr FindWorkerChannel(const string& target) = 0;
  virtual string TranslateTask(const string& task) = 0;
};
\end{c++}
\end{leftbar}


\subsubsection{Implicit tree}
As shown in \refig{dist-grpc-channel-cache}, the \code{GrpcChannelCache} class hierarchy follows an implicit "tree" structure, \code{SparseGrpcChannelCache} is a tree node, and each instance corresponds to a \ascii {Job} instance. And \code{MultiGrpcChannelCache} holds multiple \code{SparseGrpcChannelCache} instances, each of which corresponds to multiple \code{Job} instances.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/dist-grpc-channel-cache.png}
  \caption{Combination to create GRPC channel}
  \label{fig:dist-grpc-channel-cache}
\end{figure}


\subsubsection{Cache Mechanism}
To avoid the overhead of creating a \code{grpc::Channel} instance each time in real time, \code{CachingGrpcChannelCache} was introduced, which uses caching techniques in the process of looking for \code{grpc::Channel}. When the lookup in the cache fails, the \code{grpc::Channel} instance is dynamically created by calling \code{FindChannelOnce} and added to the cache.

\begin{leftbar}
\begin{c++}
struct CachingGrpcChannelCache : GrpcChannelCache {
  SharedGrpcChannelPtr FindWorkerChannel(const string& target) override {
    SharedGrpcChannelPtr ch = nullptr;
    {
      mutex_lock l(mu_);
      ch = gtl::FindPtrOrNull(channels_, target);
      if (ch) {
        return ch;
      }
    }
    ch = FindChannelOnce(target);
    if (ch) {
      mutex_lock l(mu_);
      channels_.insert({target, ch});
    }
    return ch;
  }

 protected:
  virtual SharedGrpcChannelPtr FindChannelOnce(const string& target) = 0;

 private:
  mutex mu_;
  std::unordered_map<string, SharedGrpcChannelPtr> channels_;
};
\end{c++}
\end{leftbar}


\subsubsection{Foliage node}
Each instance of \code{SparseGrpcChannelCache} corresponds to a \ascii{Job} instance, which creates a corresponding \code{grpc::Channel} instance set for a \ascii{Job}, one for each \ascii{Task} \code{grpc::Channel}.

Where \code{FindChannelOnce} extracts the corresponding \code{task\_id} from the \code{Worker} name by calling \code{TranslateTask}, and then indexes it from \code{host\_ports\_}\ The address information of code{host:port}, call the factory method \code{channel\_func\_} to create the corresponding \code{grpc::Channel} instance. Therefore, it is mainly responsible for the following three responsibilities:

\begin{enum}
  \eitem{returns the list of \ascii{Task} names corresponding to \ascii{Job} via \code{ListWorkers}; for example, \code{/job:ps} returns \code{[/job:ps/replica:0/ Task:0, /job:ps/replica:0/task:1]};}
  \eitem{Index the address information of \code{host:port} by \code{TranslateTask} and according to the specific \ascii{Task} name; for example, \code{/job:ps/replica:0/task:0} The address of the index is \code{ps0:2222};}
  \eitem{Create a corresponding \code{grpc::Channel} instance via \code{FindChannelOnce} and based on the specific \ascii{Task} name. For example, \code{/job:ps/replica:0/task:0} creates a \code{grpc::Channel} instance with the address \code{ps0:2222}. }
\end{enum}

\begin{leftbar}
\begin{c++}
static string MakeAddress(const string& job, int task) {
  return strings::StrCat("/job:", job, "/replica:0/task:", task);
}

struct SparseGrpcChannelCache : CachingGrpcChannelCache {
  SparseGrpcChannelCache(
      const string& job_id,
      const std::map<int, string>& host_ports,
      ChannelCreationFunction channel_func)
      : job_id_(job_id), host_ports_(host_ports),
        channel_func_(std::move(channel_func)) {
  }

  void ListWorkers(std::vector<string>* workers) const override {
    workers->reserve(workers->size() + host_ports_.size());
    for (const auto& id_host_port : host_ports_) {
      workers->emplace_back(MakeAddress(job_id_, id_host_port.first));
    }
  }

  string TranslateTask(const string& target) override {
    DeviceNameUtils::ParsedName parsed;
    if (!DeviceNameUtils::ParseFullName(target, &parsed)) {
      return "";
    }
    auto iter = host_ports_.find(parsed.task);
    return iter == host_ports_.end() ? "" : iter->second;
  }

 protected:
  SharedGrpcChannelPtr FindChannelOnce(const string& target) override {
    auto host_port = TranslateTask(target);
    if (host_port.empty()) {
      return nullptr;
    }
    return channel_func_(host_port);
  }

 private:
  const string job_id_;
  const std::map<int, string> host_ports_;
  const ChannelCreationFunction channel_func_;
};
\end{c++}
\end{leftbar}


\subsubsection{Non-leaf node}
\code{MultiGrpcChannelCache} holds multiple \code{SparseGrpcChannelCache} instances via \code{caches\_} to create a combination of \code{grpc::Channel} for all \ascii{Worker} nodes of the entire cluster. To further improve the lookup process for the \code{SparseGrpcChannelCache} instance, \code{MultiGrpcChannelCache} caches the visited \code{SparseGrpcChannelCache} instance; only when the cache finds a \code{SparseGrpcChannelCache} instance fails, it tries to get from \code The corresponding \code{SparseGrpcChannelCache} instance is indexed in the {caches\_} list and automatically added to the cache.

\begin{leftbar}
\begin{c++}
class MultiGrpcChannelCache : public CachingGrpcChannelCache {
 public:
  explicit MultiGrpcChannelCache(
      const std::vector<GrpcChannelCache*>& caches) 
      : caches {}

  ~MultiGrpcChannelCache() override {
    for (auto cache : caches_) {
      delete cache;
    }
  }

  void ListWorkers(std::vector<string>* workers) const override {
    for (auto cache : caches_) {
      cache->ListWorkers(workers);
    }
  }

  string TranslateTask(const string& target) override {
    mutex_lock l(mu_);  // could use reader lock
    auto cache = gtl::FindPtrOrNull(target_caches_, target);
    if (cache == nullptr) {
      for (auto c : caches_) {
        string r = c->TranslateTask(target);
        if (!r.empty()) {
          target_caches_.insert({target, c});
          cache = c;
          break;
        }
      }
    }
    return cache->TranslateTask(target);
  }

 protected:
  SharedGrpcChannelPtr FindChannelOnce(const string& target) override {
    for (auto cache : caches_) {
      auto ch = cache->FindWorkerChannel(target);
      if (ch) {
        mutex_lock l(mu_);
        target_caches_.insert({target, cache});
        return ch;
      }
    }
    return nullptr;
  }

 private:
  // List of channels used by this MultiGrpcChannelCache.
  const std::vector<GrpcChannelCache*> caches_;

  mutex mu_;
  // The same GrpcChannelCache can appear multiple times in the cache.
  std::unordered_map<string, GrpcChannelCache*> target_caches_;
};
\end{c++}
\end{leftbar}


\subsection{Create WorkerInterface}
As shown by \refig{dist-worker-cache-interface} , \code{GrpcWorkerCache} holds the \code{GrpcChannelCache} object and creates a \code{grpc::Channel} instance from it, thus implementing \code{GrpcRemoteWorker } Dynamic creation of instances.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/dist-worker-cache-interface.png}
  \caption{polymorphic creation\code{WorkerInterface} instance}
  \label{fig:dist-worker-cache-interface}
\end{figure}

\begin{leftbar}
\begin{c++}
struct GrpcWorkerCache : WorkerCachePartial {
  GrpcWorkerCache(
      GrpcChannelCache* channel_cache,
      WorkerInterface* local_worker,
      const string& local_target)
      : local_target_(local_target),
        local_worker_(local_worker),
        channel_cache_(channel_cache) {}

  ~GrpcWorkerCache() override {
    live_rpc_counter_.WaitUntilUnused();
    delete channel_cache_;
  }

  void ListWorkers(std::vector<string>* workers) const override {
    channel_cache_->ListWorkers(workers);
  }

  WorkerInterface* CreateWorker(const string& target) override {
    if (target == local_target_) {
      return local_worker_;
    } else {
      auto channel = channel_cache_->FindWorkerChannel(target);
      if (!channel) return nullptr;
      return new GrpcRemoteWorker(&live_rpc_counter_, std::move(channel),
                                  &completion_queue_, &logger_);
    }
  }

  void ReleaseWorker(const string& target, 
      WorkerInterface* worker) override {
    if (target != local_target_) {
      WorkerCacheInterface::ReleaseWorker(target, worker);
    }
  }

 private:
  string local_target_;
  WorkerInterface* local_worker_;  // Not owned.
  GrpcCounter live_rpc_counter_;
  GrpcChannelCache* channel_cache_;  // Owned.
  ::grpc::CompletionQueue completion_queue_;
  WorkerCacheLogger logger_;
};
\end{c++}
\end{leftbar}

\end{content}


\section{Session control}
\begin{content}
\emph{session control} is the core of the \tf{} distributed runtime and the critical path for the entire \tf{} execution engine. In order to streamline the context of session control, the next article will focus on the detailed process of the entire session control.


\subsection{Session collaboration}
As shown in \refig{dist-session-overview}, in distributed mode, session control is implemented through the synergy between \code{GrpcSession, MasterSession, WorkerSession}, which reside in \code{Client, Master, respectively. On Worker}, use the same \code{session\_handle} to work together.

Among them, \code{tf.Session} is implemented using \ascii{Python}, which is \ascii{API} provided by \tf{}. It is in the same process as \code{GrpcSession} and directly holds the handle (or pointer) of \code{GrpcSession}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-session-overview-1.png}
  \caption{session collaboration}
  \label{fig:dist-session-overview}
\end{figure}

As shown in \refig{dist-multi-client-conn}, in distributed mode, there may be multiple \ascii{Client} connected to one \ascii{Master} at the same time, \ascii{Master} for each connection The \ascii{Client} entered creates a \code{MasterSession} instance. \ascii{Worker} may also provide computing services for multiple \ascii{Master}, and \ascii{Worker} creates a \code{WorkerSession} instance for each of the requested \ascii{Master}. To distinguish between different \ascii{Client} computing services, use a different \code{session\_handle} distinction.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/dist-multi-client-conn.png}
  \caption{session control: domain model}
  \label{fig:dist-multi-client-conn}
\end{figure}


\subsection{Lifecycle}
\code{GrpcSession} controls the session lifecycle of \ascii{Client}, \code{MasterSession} controls the session lifecycle of \ascii{Master}, \code{WorkerSession} controls the session lifecycle of \ascii{Worker}, they are Synergy is achieved through \code{session\_handle}.


\subsubsection{GrpcSession Lifecycle}
In distributed mode, the runtime of \code{Client} is controlled by \code{GrpcSession}, and the lifecycle of \code{GrpcSession} is shown in \refig{dist-grpc-session-life-cycle}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/dist-grpc-session-life-cycle.png}
  \caption{\code{GrpcSession}Lifecycle}
  \label{fig:dist-grpc-session-life-cycle}
\end{figure}


\subsubsection{MasterSession Lifecycle}
In distributed mode, the runtime of \code{Master} is controlled by \code{MasterSession}, and the \code{MasterSession} lifecycle process is shown in \refig{dist-master-session-life-cycle}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/dist-master-session-life-cycle.png}
  \caption{\code{MasterSession}Lifecycle}
  \label{fig:dist-master-session-life-cycle}
\end{figure}


\subsubsection{WorkerSession Lifecycle}
In distributed mode, the runtime of \code{Worker} is controlled by \code{WorkerSession}, and the \code{WorkerSession} lifecycle process is shown in \refig{dist-worker-session-life-cycle}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/dist-worker-session-life-cycle.png}
  \caption{\code{WorkerSession}Lifecycle}
  \label{fig:dist-worker-session-life-cycle}
\end{figure}


\subsection{Session process}
In the user programming environment, \ascii{Client} starts from \code{tf.Session(target)} and starts iterative execution via \code{Session.run}. After the final calculation is completed, call \code{Session.close} to close. Conversation. However, in the implementation of a distributed execution engine, the process is much more complicated.

\begin{nitemize}
  \eitem{Create session}
    \begin{enum}
      \eitem{创建 \code{GrpcSession};}
      \eitem{Get the remote device set;}
      \eitem{Create\code{MasterSession};}
      \eitem{Create\code{WorkerSession};}
    \end{enum}
  \eitem{iteration execution}
    \begin{enum}
      \eitem{Start execution;}
      \eitem{图剪枝;}
      \eitem{Figure split;}
      \eitem{Register submap;}
      \eitem{Run submap;}
    \end{enum}
  \eitem{Close session}
    \begin{enum}
      \eitem{关闭 \code{GrpcSession};}
      \eitem{close\code{MasterSession};}
      \eitem{关闭\code{WorkerSession}；}
    \end{enum}
\end{nitemize}

\end{content}


\section{Create session}
\begin{content}
Before starting the calculation, you need to create a \code{GrpcSession} instance on \ascii{Client}, a \code{MasterSession} instance on \ascii{Master}, and \code{WorkerSession} on each \ascii{Worker} For example, the three implement the collaboration through \code{MasterSession}'s \code{session\_handle} to serve the \ascii{Client} instance of the access.


\subsection{Create GrpcSession}
When \ascii{Client} calls \code{tf.Session(target)}, the \code{GrpcSession} instance is triggered by calling the \ascii{C API} interface of \code{TF\_NewDeprecatedSession}. Among them, \ascii{C API} is a standard interface for multi-language programming provided by the \tf{} back-end system. Finally, \code{tf.Session} will directly hold the handle to \code{GrpcSession} as shown in \refig{dist-create-grpc-session-1}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-create-grpc-session-1.png}
  \caption{Create\code{GrpcSession}: \code{tf.Session} holds \code{GrpcSession} handle}
  \label{fig:dist-create-grpc-session-1}
\end{figure}

\begin{leftbar}
\begin{c++}
Status NewSession(const SessionOptions& options, Session** out_session) {
  SessionFactory* factory;
  Status s = SessionFactory::GetFactory(options, &factory);
  if (!s.ok()) {
    *out_session = nullptr;
    return s;
  }
  *out_session = factory->NewSession(options);
  if (!*out_session) {
    return errors::Internal("Failed to create session.");
  }
  return Status::OK();
}

TF_DeprecatedSession* TF_NewDeprecatedSession(
  const TF_SessionOptions* opt, TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    return new TF_DeprecatedSession({session});
  } else {
    return nullptr;
  }
}
\end{c++}
\end{leftbar}

As shown by \refig{dist-grpc-session-factory}, \code{GrpcSession} is created by \code{GrpcSessionFactory} polymorphism. When \code{target} starts with \code{grpc://}, \code{SessionFactory::GetFactory} returns the \code{GrpcSessionFactory} instance, and \code{GrpcSessionFactory::NewSession}'s factory method delegate\code{ The static factory method of GrpcSession::Create} creates a \code{GrpcSession} instance.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/dist-grpc-session-factory.png}
  \caption{Polymorphic creation of GrpcSession}
  \label{fig:dist-grpc-session-factory}
\end{figure}

\begin{leftbar}
\begin{c++}
const char* kSchemePrefix = "grpc://";

struct GrpcSessionFactory : SessionFactory {
  bool AcceptsOptions(const SessionOptions& options) override {
    return StringPiece(options.target).starts_with(kSchemePrefix);
  }

  Session* NewSession(const SessionOptions& options) override {
    std::unique_ptr<GrpcSession> ret;
    Status s = GrpcSession::Create(options, &ret);
    if (s.ok()) {
      return ret.release ();
    } else {
      return nullptr;
    }
  }
};
\end{c++}
\end{leftbar}

The \code{GrpcSession::Create} static factory method is primarily responsible for creating the \code{GrpcSession} instance and completing the corresponding initialization. In the initialization process, the most important thing is to build the \code{MasterInterface} instance; where \code{MasterInterface} is used for \ascii{Client} to access the \code{MasterService} remote service on \ascii{Master}, which exists Two subclasses are implemented, which correspond to two different application scenarios:

\begin{enum}
  \eitem{\code{LocalMaster}:\ascii{Client} is in the same process as \ascii{Master}, calling \code{LocalMaster::Lookup} to get the \code{LocalMaster} instance directly;}
  \eitem{\code{GrpcRemoteMaster}:\ascii{Client} is not in the same process as \ascii{Master}, call the factory method \code{NewGrpcMaster} to generate the \code{GrpcRemoteMaster} instance. }
\end{enum}

The \code{GrpcRemoteMaster} instance is a client implementation of \ascii{RPC}. When creating a \code{GrpcRemoteMaster} instance, you need to create the \ascii{Master} address and service port specified by \code{target} first. Connected \ascii{RPC} channel.

\begin{leftbar}
\begin{c++}
Status GrpcSession::Create(
    const SessionOptions& options,
    std::unique_ptr<GrpcSession>* out_session) {
  std::unique_ptr<GrpcSession> session(new GrpcSession(options));
  std::unique_ptr<MasterInterface> master;
  // intra-process between client and master.
  if (!options.config.rpc_options().use_rpc_for_inprocess_master()) {
    master = LocalMaster::Lookup(options.target);
  }
  // inter-process between client and master.
  if (!master) {
    SharedGrpcChannelPtr master_channel;
    TF_RETURN_IF_ERROR(NewHostPortGrpcChannel(
        options.target.substr(strlen(kSchemePrefix)), &master_channel));
    master.reset(NewGrpcMaster(master_channel));
  }
  session->SetRemoteMaster(std::move(master));
  *out_session = std::move(session);
  return Status::OK();
}
\end{c++}
\end{leftbar}


\subsection{Create MasterSession}
As shown in \refig{dist-create-master-session-1}, when the \code{GrpcSession} instance is created successfully, the call to \code{GprcSession::Create} will be triggered, and the initial calculation graph will be passed to \code. The {CreateSessionRequst} message is sent to \ascii{Master}; when \ascii{Master} receives the \code{CreateSessionRequst} message, it generates the corresponding \code{MasterSession} instance and uses the globally unique \code{session\_handle } Identifies the instance and finally brings it back to \code{GrpcSession} via the \code{CreateSessionResponse} message.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-create-master-session-1.png}
  \caption{Create\code{MasterSession}}
  \label{fig:dist-create-master-session-1}
\end{figure}

% \begin{figure}[H]
% \centering
% \includegraphics[width=1.0\textwidth]{figures/dist-create-master-session.png}
% \caption{Create \code{MasterSession}}
%  \label{fig:dist-create-master-session}
% \end{figure}


\subsubsection{GrpcSesion::Create(graph\_def)}
The \code{GrpcSession::Create(graph\_def)} method is mainly used for \code{Client} requests\code{Master} to create \code{MasterSession} instances. First, the \code{GrpcSession::Create} method finishes constructing the \code{CreateSessionRequst} message and then sends it to \ascii{Master} via \code{GrpcRemoteMaster}.

When \code{GrpcSession} receives the \code{CreateSessionResponse} message, save \code{handle} of \code{MasterSession} and its version number \code{graph\_version}. Where \code{handle} is used to identify the \code{MasterSession} instance on the \ascii{Master} side, and \code{graph\_version} is used for subsequent extended calculation graphs.

\begin{leftbar}
\begin{c++}
void GrpcSession::BuildCreateSessionReq(
    const GraphDef& graph,
    CreateSessionRequest& req) {
  *req.mutable_config() = options_.config;
  *req.mutable_graph_def() = graph;
  req.set_target(options_.target);
}

void GrpcSession::SaveCreateSessionRsp(
    CreateSessionResponse& rsp) {
  mutex_lock l(mu_);
  swap(handle_, *(resp.mutable_session_handle()));
  current_graph_version_ = resp.graph_version();
}

Status GrpcSession::CreateImpl(CallOptions* call_options,
                               const GraphDef& graph) {
  CreateSessionRequest req;
  CreateSessionResponse resp;

  BuildCreateSessionReq(graph, req);
  Status s = master_->CreateSession(call_options, &req, &resp);
  if (s.ok()) {
    SaveCreateSessionRsp (respectively);
  }
  return s;
}

Status GrpcSession::Create(const RunOptions& run_options,
                           const GraphDef& graph) {
  CallOptions call_options;
  call_options.SetTimeout(run_options.timeout_in_ms());
  return CreateImpl(&call_options, graph);
}

Status GrpcSession::Create(const GraphDef& graph) {
  CallOptions call_options;
  call_options.SetTimeout(options_.config.operation_timeout_in_ms());
  return CreateImpl(&call_options, graph);
}
\end{c++}
\end{leftbar}


\subsubsection{GrpcRemoteMaster::CreateSession}
\code{GrpcRemoteMaster} is a client implementation of \ascii{gRPC}. Its implementation is very simple, call the corresponding service interface of the remote \ascii{Master} via a \code{stub} of \ascii{gRPC}.

\begin{leftbar}
\begin{c++}
Status GrpcRemoteMaster::CreateSession(
    CallOptions * call_options,
    const CreateSessionRequest* request,
    CreateSessionResponse* response) override {
  ::grpc::ClientContext ctx;
  SetClientContext(*call_options, ctx);
  return FromGrpcStatus(stub_->CreateSession(&ctx, *request, response));
}
\end{c++}
\end{leftbar}


\subsubsection{GrpcMasterService::CreateSessionHandler}
\code{GrpcMasterService} is a \ascii{gRPC} service that implements the \ascii{RPC} service interface for \code{MasterService}. When the \code{CreateSession} message is received, it will be processed by the \code{GrpcMasterService::CreateSessionHandler} callback, which will delegate \code{Master} to process the message.

When \code{Master} is processed, the \ascii{lambda} expression at the completion of the callback is returned, and the response message of \code{CreateSessionResponse} is returned to \ascii{Client}.

\begin{leftbar}
\begin{c++}
void GrpcMasterService::CreateSessionHandler(
  MasterCall<CreateSessionRequest, CreateSessionResponse>* call) {
  master_impl_->CreateSession(
    &call->request, &call->response,
    [call](const Status& status) {
        call->SendResponse(ToGrpcStatus(status));
    });
  ENQUEUE_REQUEST(CreateSession, true);
}
\end{c++}
\end{leftbar}


\subsubsection{Master::CreateSession}
\code{Master::CreateSession} will start a thread in the thread pool and look for all \ascii{Worker} in the thread according to the \code{cluster\_spec} information to collect information about the remote device set. Finally, a \code{MasterSession} was created.

\begin{remark}
The process of finding a remote device set is described in the next section. The implementation of this part of the code is omitted from the sample code in this section.
\end{remark}

When \code{MasterSession} is created successfully, \ascii{Master} will save the binary information of \code{(handle, master\_session)} so that subsequent \ascii{Master} can be indexed by \code{handle} The \code{MasterSession} instance.

\begin{leftbar}
\begin{c++}
using RemoveDevices = unique_ptr<vector<unique_ptr<Device>>>;

void Master::CreateSession(const CreateSessionRequest* req,
                           CreateSessionResponse* resp, MyClosure done) {
  SchedClosure([this, req, resp, done]() {
    // 1. Find all remote devices. 
    WorkerCacheInterface* worker_cache = env_->worker_cache;
    RemoveDevices remote_devices(new vector<unique_ptr<Device>>());

    Status status = DeviceFinder::GetRemoteDevices(
        req->config().device_filters(), env_,
        worker_cache, remote_devices.get())

    if (!status.ok()) return;

    // 2. Build DeviceSet
    std::unique_ptr<DeviceSet> device_set(new DeviceSet);
    for (auto&& d : *remote_devices) {
      device_set->AddDevice(d.get());
    }

    int num_local_devices = 0;
    for (Device* d : env_->local_devices) {
      device_set->AddDevice(d);
      if (num_local_devices == 0) {
        // Uses the first local device as the client device.
        device_set->set_client_device(d);
      }
      num_local_devices++;
    }

    // 3. Create MasterSession
    SessionOptions options;
    options.config = req->config();
    
    MasterSession* session = env_->master_session_factory(
        options, env_, std::move(remote_devices), 
        std::move(worker_cache_ptr), std::move(device_set));

    GraphDef* gdef =
        const_cast<CreateSessionRequest*>(req)->mutable_graph_def();
    
    // ignore worker\_cache\_factory\_options implements.
    WorkerCacheFactoryOptions worker_cache_factory_options;
    Status status = session->Create(gdef, worker_cache_factory_options);
    resp->set_session_handle(session->handle());
    
    // 4. Store <handle, master\_session> pair.
    {
      mutex_lock l(mu_);
      CHECK(sessions_.insert({session->handle(), session}).second);
    }
  });
}
\end{c++}
\end{leftbar}


\subsubsection{MasterSession::Create(graph\_def)}
\code{MasterSession::Create(graph\_def)} mainly accomplishes two things.

\begin{enum}
  \eitem{Initialize the calculation graph and generate an instance of \code{SimpleGraphExecutionState};}
  \eitem{If the cluster is dynamically configured, broadcast all \ascii{Worker} to create the corresponding \code{WorkerSession} instance. }
\end{enum}

The \code{SimpleGraphExecutionState::MakeForBaseGraph} implementation is the same as the local mode and will not be repeated here.

\begin{leftbar}
\begin{c++}
Status MasterSession::Create(
    GraphDef* graph_def,
    const WorkerCacheFactoryOptions& options) {
  SimpleGraphExecutionStateOptions execution_options;
  execution_options.device_set = devices_.get();
  execution_options.session_options = &session_opts_;
  {
    mutex_lock l(mu_);
    TF_RETURN_IF_ERROR(SimpleGraphExecutionState::MakeForBaseGraph(
        graph_def, execution_options, &execution_state_));
  }

  // CreateWorkerSessions should be called only with
  // dynamic cluster membership.
  if (options.cluster_def != nullptr) {
    return CreateWorkerSessions(options);
  }
  return Status::OK();
}
\end{c++}
\end{leftbar}


\subsection{Get remote device set}
As shown in \refig{dist-worker-get-status}, \code{MasterSession} polls all \code{Worker} instances before creating \code{MasterSession}, getting all remote \code{Worker} Device information. Get the remote device set by calling \code{DeviceFinder::GetRemoteDevices} with the device finder of \code{DeviceFinder}.

It works very simply, it gets a list of all \ascii{Worker} names in the cluster according to \code{GrpcWorkerCache::ListWorkers}; then, according to the name of \code{worker\_name}, call \code{GrpcWorkerCache: The :CreateWorker} factory method creates a \code{WorkerInterface} instance, which is used to access the remote \code{WorkerService} service. Finally, the \code{GetStatusRequest} request message is broadcasted to the remote \ascii{Worker} list via \code{WorkerInterface} to obtain the remote device set.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/dist-worker-get-status.png}
  \caption{Get remote device set}
  \label{fig:dist-worker-get-status}
\end{figure}


\subsubsection{Device Finder}
\code{DeviceFinder} implements a function object that implements the algorithm for remote device lookups. The process is divided into three steps:

\begin{enum}
  \eitem{\code{Start}: concurrently broadcast \code{GetStatusRequest} to all \code{Worker} instances in the cluster;}
  \eitem{\code{Wait}: Collect all \code{GetStatusResponse} messages returned by \code{Worker};}
  \eitem{\code{GetRemoteDevices}: Get the query results and return them to the client;}
\end{enum}

\begin{leftbar}
\begin{c++}
struct DeviceFinder {
  static Status DeviceFinder::GetRemoteDevices(
      MasterEnv* env,
      WorkerCacheInterface* worker_cache,
      std::vector<std::unique_ptr<Device>>* out_remote) {
    DeviceFinder finder(env, worker_cache);
    finder.Start ();
    TF_RETURN_IF_ERROR(finder.Wait());
    finder.GetRemoteDevices(env->local_devices, out_remote);
    return Status::OK();
  }
};
\end{c++}
\end{leftbar}

In order to control the \code{GetStatusResponse} message of multiple \code{Worker}, the counter of \code{num\_pending\_} is used here, and the initial value set by \code{DeviceFinder::Start} is \code The number of {Worker}.

When a \code{GetStatusResponse} message from a \code{Worker} is received, the callback \code{WhenDone} is decremented by \ascii{1}. When the counter is reduced to \ascii{0}, wake up the \code{pending\_zero\_.wait\_for} statement by calling \code{pending\_zero\_.notify\_all}, then you can pass \ Code{finder.GetRemoteDevices} gets the query results.

Among them, in \code{DeviceFinder::Start}, all \code{Worker} broadcast \code{GetStatusRequest} messages are queried for corresponding device information through \code{NewRemoteDevices}. The next section will focus on the implementation process.

\begin{leftbar}
\begin{c++}
struct DeviceFinder {
 private:
  explicit DeviceFinder(
      MasterEnv* env,
      WorkerCacheInterface* worker_cache)
      : env_(env), worker_cache_(worker_cache) {
    worker_cache->ListWorkers(&targets_);
    seen_targets_.assign(targets_.size(), false);
  }

  ~DeviceFinder() {
    for (auto dev : found_) delete dev;
  }

  void Start() {
    {
      mutex_lock l(mu_);
      num_pending_ = targets_.size();
    }

    // Talk to all workers to get the list of available devices.
    using std::placeholders::_1;
    using std::placeholders::_2;
    for (size_t i = 0; i < targets_.size(); ++i) {
      NewRemoteDevices(env_->env, worker_cache_, targets_[i],
                       std::bind(&ME::WhenFound, this, i, _1, _2));
    }
  }

  // The caller takes the ownership of returned remote devices.
  void GetRemoteDevices(
      const std::vector<Device*>& local,
      std::vector<std::unique_ptr<Device>>* remote) {
    std::unordered_set<string> names(local.size());
    for (auto dev : local) {
      names.insert(dev->name());
    }

    mutex_lock l(mu_);
    for (auto dev : found_) {
      auto& name = dev->name();
      if (names.insert(name).second) {
        remote->push_back(std::unique_ptr<Device>(dev));
      } else {
        delete dev;
      }
    }
    found_.clear();
  }

  Status Wait() {
    mutex_lock l(mu_);
    while (num_pending_ != 0) {
      pending_zero_.wait_for(l, std::chrono::milliseconds(10 * 1000));
      if (num_pending_ != 0) {
        for (size_t i = 0; i < targets_.size(); ++i) {
          if (!seen_targets_[i]) {
            LOG(INFO)
                << "CreateSession still waiting for response from worker: "
                << targets_[i];
          }
        }
      }
    }
    return status_;
  }

  void WhenFound(int target_index, const Status& s,
                 std::vector<Device*>* devices) {
    mutex_lock l(mu_);
    seen_targets_[target_index] = true;
    if (!s.ok()) {
      status_.Update(s);
    } else {
      found_.insert(found_.end(), devices->begin(), devices->end());
      devices->clear();
    }
    --num_pending_;
    if (num_pending_ == 0) {
      pending_zero_.notify_all();
    }
  }

  typedef DeviceFinder ME;
  const MasterEnv* env_;
  WorkerCacheInterface* worker_cache_;

  mutex mu_;
  int num_pending_ GUARDED_BY(mu_);
  condition_variable pending_zero_;
  std::vector<Device*> found_ GUARDED_BY(mu_);

  std::vector<string> targets_;
  std::vector<bool> seen_targets_ GUARDED_BY(mu_);
  Status status_;
};
\end{c++}
\end{leftbar}

Tips: When the \code{num\_pending\_} counter is not zero, the main thread periodically sleeps for \code{10} seconds. If you find that \ascii{Worker} has not returned a response message when you wake up, Then print the names of those \ascii{Worker}. When you see the following information printed cyclically, you should understand whether \code{Server} corresponding to \code{/job:worker/task:2} exits abnormally, or \ascii corresponding to \ascii{Master} Whether there is an abnormality in the network between {Worker}, etc., analyze and process according to the specific situation.

\begin{leftbar}
\begin{python}
CreateSession still waiting for response from worker: /job:worker/task:2
\end{python}
\end{leftbar}


\subsubsection{NewRemoteDevices}
\code{NewRemoteDevices} will look for the \code{WorkerInterface} instance based on \code{worker\_name} and send a \code{GetStatusRequest} message to the corresponding \code{Worker} to get its device information. When the message is returned, the function object of \code{cb} will be called back. The device information obtained from the remote \code{Worker} is not complete. It does not contain the information of \code{worker\_name}, so it needs to be manually added.

\begin{leftbar}
\begin{c++}
void NewRemoteDevices(
    Env* env, WorkerCacheInterface* worker_cache,
    const string& worker_name, NewRemoteDevicesDone done) {
  struct Call {
    GetStatusRequest req;
    GetStatusResponse resp;
  };

  WorkerInterface* wi = worker_cache->CreateWorker(worker_name);
  Call* call = new Call;
  auto cb = [env, worker_cache, &worker_name, &done, wi, call](
      const Status& status) {
    Status s = status;
    std::vector<Device*> remote_devices;
    auto cleanup = gtl::MakeCleanup(
        [worker_cache, &worker_name, wi, &done, &remote_devices, &s, call] {
          worker_cache->ReleaseWorker(worker_name, wi);
          done(s, &remote_devices);
          delete call;
        });
    if (s.ok()) {
      DeviceNameUtils::ParsedName worker_name_parsed;
      DeviceNameUtils::ParseFullName(worker_name, &worker_name_parsed);

      remote_devices.reserve(call->resp.device_attributes_size());

      for (auto& da : call->resp.device_attributes()) {
        DeviceNameUtils::ParsedName device_name_parsed;
        DeviceNameUtils::ParseFullName(da.name(), &device_name_parsed);
        
        DeviceAttributes da_rewritten = da;
        da_rewritten.set_name(DeviceNameUtils::FullName(
            worker_name_parsed.job, worker_name_parsed.replica,
            worker_name_parsed.task, device_name_parsed.type,
            device_name_parsed.id));
        auto d = new RemoteDevice(env, da_rewritten);
        remote_devices.push_back(d);
      }
    }
  };
  wi->GetStatusAsync(&call->req, &call->resp, cb);
}
\end{c++}
\end{leftbar}


\subsubsection{GrpcRemoteWorker::GetStatusAsync}
\code{GrpcRemoteWorker} is a concrete implementation of \code{WorkerInterface}, which is a client implementation of \ascii{gRPC}, which calls the corresponding service interface of the remote \code{WorkerService} via \code{stub}.

\begin{leftbar}
\begin{c++}
struct GrpcRemoteWorker : WorkerInterface {
  void GetStatusAsync(const GetStatusRequest* request,
                      GetStatusResponse* response,
                      StatusCallback done) override {
    IssueRequest(request, response, getstatus_, std::move(done));
  }
}
\end{c++}
\end{leftbar}


\subsubsection{GrpcRemoteWorker::GetStatusAsync}
\code{GrpcWorkerService} is a concrete implementation of \code{WorkerService}. When the \code{GetStatusRequest} message is received, it will be handled by the \code{GetStatusHandler} callback.

\begin{leftbar}
\begin{c++}
struct GrpcWorkerService : AsyncServiceInterface {
  void GetStatusHandler(WorkerCall<GetStatusRequest, GetStatusResponse>* call) {
    Schedule([this, call]() {
      Status s = worker_->GetStatus(&call->request, &call->response);
      call->SendResponse(ToGrpcStatus(s));
    });
    ENQUEUE_REQUEST(GetStatus, false);
  }
};
\end{c++}
\end{leftbar}


\subsubsection{Worker::GetStatusAsync}
\code{Worker::GetStatusAsync} will delegate \code{DeviceMgr} to the local device information summary, and finally return to the peer through the \code{GetStatusResponse} message.

\begin{leftbar}
\begin{c++}
void Worker::GetStatusAsync(const GetStatusRequest* request,
                            GetStatusResponse* response, StatusCallback done) {
  std::vector<DeviceAttributes> devices;
  env_->device_mgr->ListDeviceAttributes(&devices);
  response->mutable_device_attributes()->Reserve(devices.size());
  for (auto& d : devices) {
    response->add_device_attributes()->Swap(&d);
  }
  done(Status::OK());
}
\end{c++}
\end{leftbar}

\code{DeviceMgr} holds a local device set and is extremely simple to implement.

\begin{leftbar}
\begin{c++}
void DeviceMgr::ListDeviceAttributes(
    std::vector<DeviceAttributes>* devices) const {
  devices->reserve(devices_.size());
  for (auto dev : devices_) {
    devices->emplace_back(dev->attributes());
  }
}
\end{c++}
\end{leftbar}


\subsection{Create WorkerSession}
When \code{MasterSession} is created successfully, if there is no dynamic configuration of the cluster (the default distributed configuration environment), all \ascii{Worker} will not be broadcast dynamically to create \code{WorkerSession}. In fact, every \ascii{Worker} has a \code{SessionMgr} instance that holds a \code{WorkerSession} instance named \code{legacy\_session\_}. Therefore, each \ascii{Worker} has a globally unique \code{WorkerSession} instance.

\begin{leftbar}
\begin{c++}
SessionMgr::SessionMgr(
    WorkerEnv* worker_env, 
    const string& default_worker_name,
    std::unique_ptr<WorkerCacheInterface> default_worker_cache,
    WorkerCacheFactory worker_cache_factory)
    : worker_env_(worker_env),
      legacy_session_(
          default_worker_name, 
          std::move(default_worker_cache),
          std::unique_ptr<DeviceMgr>(worker_env->device_mgr),
          std::unique_ptr<GraphMgr>(
              new GraphMgr(worker_env, 
              worker_env->device_mgr))),
      worker_cache_factory_(std::move(worker_cache_factory)) {}
\end{c++}
\end{leftbar}

As shown in \refig{dist-create-worker-session}, if there is a dynamic cluster configuration, \ascii{Master} broadcasts each \ascii{Worker} to create a \code{WorkerSession} instance and uses \code{ Sessin\_handle} identifies the \code{WorkerSession}. These \code{WorkerSession} are part of this \code{MasterSession} instance because they use the same \code{session\_handle} identifier as the \code{MasterSession} instance.

Among them, \code{MasterSession} introduces the \code{BlockingCounter} counter in order to collect all the \code{CreateWorkerSessionResponse} messages returned by \ascii{Worker}. The initial value of the \code{BlockingCounter} counter is the number of \ascii{Worker}. When receiving the response message of each \ascii{Worker}, the counter is decremented by \ascii{1} until the counter is \ascii{0}. \code{done.Wait()} is woken up.

In addition, the \code{WorkerInterface} instance is queried or created via \code{WorkerCacheInterface}, which will be explained in more detail later.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/dist-create-worker-session.png}
  \caption{Dynamic creation\code{WorkerSession}}
  \label{fig:dist-create-worker-session}
\end{figure}

\begin{leftbar}
\begin{c++}
struct MasterSession::Worker {
  Worker(MasterSession* sess, const string& name,
         const DeviceNameUtils::ParsedName& parsed_name,
         const WorkerCacheFactoryOptions& opts)
      : sess(sess), name(&name), worker(GetOrCreateWorker()) {
    BuildRequest(parsed_name, opts);
  }

  void CreateWorkerSession(BlockingCounter& done, Status& status) {
    auto cb = [&status, &done](const Status& s) {
      status.Update(s);
      done.DecrementCount();
    };
    // IMPORTANT: notify worker to create worker session.
    worker->CreateWorkerSessionAsync(&request, &response, cb);
  }

  void Release() {
    if (worker != nullptr) {
      sess->worker_cache_->ReleaseWorker(*name, worker);
    }
  }

 private:
  WorkerInterface* GetOrCreateWorker() {
    return sess->worker_cache_->CreateWorker(*name);
  }

  void BuildRequest(const DeviceNameUtils::ParsedName& parsed_name,
                    const WorkerCacheFactoryOptions& opts) {
    request.set_session_handle(sess->handle_);
    BuildServerDef(parsed_name, opts, request.mutable_server_def());
  }

  void BuildServerDef(const DeviceNameUtils::ParsedName& parsed_name,
                      const WorkerCacheFactoryOptions& opts,
                      ServerDef* server_def) {
    *server_def->mutable_cluster() = *opts.cluster_def;
    server_def->set_protocol(*opts.protocol);
    server_def->set_job_name(parsed_name.job);
    server_def->set_task_index(parsed_name.task);
  }

 private:
  MasterSession * sex;

  // The worker name. (Not owned.)
  const string* name;

  // The worker referenced by name. (Not owned.)
  WorkerInterface* worker = nullptr;

  // Request and responses used for a given worker.
  CreateWorkerSessionRequest request;
  CreateWorkerSessionResponse response;
};

struct MasterSession::WorkerGroup {
  WorkerGroup (MasterSession * sex): gender (sex) {}

  Status CreateWorkerSessions(const WorkerCacheFactoryOptions& opts) {
    TF_RETURN_IF_ERROR(CreateWorkers(opts));
    TF_RETURN_IF_ERROR(BroadcastWorkers());
    return Status::OK();
  }

  void ReleaseWorkers() {
    for (auto& worker : workers) {
      worker.Release();
    }
  }

 private:
  Status CreateWorkers(const WorkerCacheFactoryOptions& opts) {
    sess->worker_cache_->ListWorkers(&worker_names);
    for (auto& worker_name : worker_names) {
      TF_RETURN_IF_ERROR(AppendWorker(worker_name, opts));
    }
    return Status::OK();
  }

  // broadcast all workers to create worker session.
  Status BroadcastWorkers() {
    Status status = Status::OK();
    BlockingCounter done(workers.size());
    for (auto& worker : workers) {
      worker.CreateWorkerSession(done, status);
    }
    done.Wait();
    return status;
  }

  Status AppendWorker(const string& worker_name,
                    const WorkerCacheFactoryOptions& opts) {
    DeviceNameUtils::ParsedName parsed_name;
    TF_RETURN_IF_ERROR(ParseWorkerName(worker_name, &parsed_name));
    workers.emplace_back(Worker(sess, worker_name, parsed_name, opts));
    return Status::OK();
  }

  Status ParseWorkerName(const string& worker_name,
                         DeviceNameUtils::ParsedName* parsed_name) {
    if (!DeviceNameUtils::ParseFullName(worker_name, parsed_name)) {
      return errors::Internal("Could not parse name ", worker_name);
    }
    if (!parsed_name->has_job || !parsed_name->has_task) {
      return errors::Internal("Incomplete worker name ", worker_name);
    }
    return Status::OK();
  }

 private:
  MasterSession * sex;
  std::vector<string> worker_names;
  std::vector<Worker> workers;
};

Status MasterSession::CreateWorkerSessions(
    const WorkerCacheFactoryOptions& options) {
  CHECK(worker_cache_) << "CreateWorkerSessions should be called only with "
                       << "dynamic cluster membership.";

  WorkerGroup worker_group(this);

  // Release the workers.
  auto cleanup = gtl::MakeCleanup([&worker_group] {
    worker_group.ReleaseWorkers();
  });

  return worker_group.CreateWorkerSessions(options);
}
\end{c++}
\end{leftbar}


\subsubsection{GrpcRemoteWorker}
\code{GrpcRemoteWorker} is the \ascii{gRPC} client that accesses the remote \ascii{Worker}. It calls the corresponding \code{stub} to call the remote service.

\begin{leftbar}
\begin{c++}
struct GrpcRemoteWorker : WorkerInterface {
  void CreateWorkerSessionAsync(
      const CreateWorkerSessionRequest* request,
      CreateWorkerSessionResponse* response,
      StatusCallback done) override {
    IssueRequest(request, response, createworkersession_, std::move(done));
  }
};
\end{c++}
\end{leftbar}


\subsubsection{GrpcWorkerService::CreateWorkerSessionHandler}
On the \ascii{Worker} side, the \code{CreateWorkerSession} message is handled by the \code{CreateWorkerSessionHandler} callback. It starts a runnable thread in the thread pool and triggers \code{Worker} to dynamically create a \code{WorkerSession} instance.

\begin{leftbar}
\begin{c++}
struct GrpcWorkerService : AsyncServiceInterface {
  void CreateWorkerSessionHandler(
      WorkerCall<CreateWorkerSessionRequest, CreateWorkerSessionResponse>*
          call) {
    Schedule([this, call]() {
      Status s = worker_->CreateWorkerSession(&call->request, &call->response);
      call->SendResponse(ToGrpcStatus(s));
    });
    ENQUEUE_REQUEST(CreateWorkerSession, false);
  }
};
\end{c++}
\end{leftbar}


\subsubsection{Create a WorkerSession instance}
\code{Worker} delegates the responsibility for creating a \code{WorkerSession} instance to \code{SessionMgr}, which manages and maintains the lifecycle of all \code{WorkerSession} instances. As shown by \refig{dist-worker-session-manager}, \code{SessionMgr} may hold multiple instances of \code{WorkerSession}, and each \code{WorkerSession} instance is identified by \code{session\_handle}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-worker-session-manager.png}
  \caption{\code{Session}Manager}
  \label{fig:dist-worker-session-manager}
\end{figure}

\begin{leftbar}
\begin{c++}
void Worker::CreateWorkerSessionAsync(
    const CreateWorkerSessionRequest* request,
    CreateWorkerSessionResponse* response,
    StatusCallback done) {
  Status s = env_->session_mgr->CreateSession(
      request->session_handle(),
      request->server_def());
  done(s);
}
\end{c++}
\end{leftbar}

As shown in \refig{dist-worker-session-model}, \code{WorkerSession} holds a \code{GraphMgr} instance for registering and running multiple graph instances. Among them, each graph instance uses the \code{graph\_handle} identifier. At the same time, each \code{WorkerSession} holds a \code{DeviceMgr} instance that manages the collection of local computing devices.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/dist-worker-session-model.png}
  \caption{\code{WorkerSession} domain model: multiple graph instances can be registered and run}
  \label{fig:dist-worker-session-model}
\end{figure}

\begin{leftbar}
\begin{c++}
Status SessionMgr::CreateSession(const string& session,
                                 const ServerDef& server_def) {
  mutex_lock l(mu_);

  // 1. Create WorkerCacheInterface
  WorkerCacheInterface* worker_cache = nullptr;
  TF_RETURN_IF_ERROR(worker_cache_factory_(server_def, &worker_cache));

  // 2. Rename local devices  
  auto worker_name = WorkerNameFromServerDef(server_def);
  std::vector<Device*> renamed_devices;
  for (Device* d : worker_env_->local_devices) {
    renamed_devices.push_back(
        RenamedDevice::NewRenamedDevice(worker_name, d, false));
  }
  std::unique_ptr<DeviceMgr> device_mgr(new DeviceMgr(renamed_devices));

  // 3. Create GraphMgr
  std::unique_ptr<GraphMgr> graph_mgr(
      new GraphMgr(worker_env_, device_mgr.get()));
  
  // 4. Create WorkerSession
  std::unique_ptr<WorkerSession> worker_session(new WorkerSession(
      worker_name, std::unique_ptr<WorkerCacheInterface>(worker_cache),
      std::move(device_mgr), std::move(graph_mgr)));

  // 5. Store (session\_handle, WorkerSession) pair.
  sessions_.insert(std::make_pair(session, std::move(worker_session)));
  return Status::OK();
}
\end{c++}
\end{leftbar}

\end{content}


\section{Iteration execution}
\begin{content}


\subsection{Start execution}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dist-run-step-stage-1.png}
  \caption{GprcSession: 启动 RunStep}
  \label{fig:dist-run-step-stage-1}
\end{figure}


\subsubsection{GrpcSession::Run}

\begin{leftbar}
\begin{c++}
namespace {
  using TensorIndex = std::unordered_map<string, int>;

  void BuildReqOptions(const SessionOptions& sess_options,
      const RunOptions& run_options, 
      RunOptions& options) {
    options = run_options;
    if (run_options.timeout_in_ms() == 0) {
      options.set_timeout_in_ms(
          sess_options.config.operation_timeout_in_ms());
    }    
  }

  void BuildReqFeeds(const vector<pair<string, Tensor>>& inputs,
      MutableRunStepRequestWrapper* req) {
    for (auto& it : inputs) {
      req->add_feed(it.first, it.second);
    }
  }

  void BuildReqFetches(const std::vector<string>& output_names,
      MutableRunStepRequestWrapper* req) {
    for (int i = 0; i < output_names.size(); ++i) {
      req->add_fetch(output_names[i]);
  }

  void BuildReqTargets(const std::vector<string>& target_names,
      MutableRunStepRequestWrapper* req) {
    for (string& target : target_names) {
      req->add_target(target);
    }
  }

  void BuildRunStepReq(
      const SessionOptions& sess_options,
      const RunOptions& run_options,
      const vector<pair<string, Tensor>>& inputs,
      const std::vector<string>& output_names,
      const std::vector<string>& target_names,
      MutableRunStepRequestWrapper* req) {
    BuildReqOptions(sess_options, run_options, 
        req->mutable_options());
    BuildReqFeeds(inputs, req);
    BuildReqFetches(output_names, req);
    BuildReqTargets(target_names, req); 
  }

  void BuildOuputNamesIndex(
      const std::vector<string>& output_names,
      TensorIndex& tensor_index) {
    for (int i = 0; i < output_names.size(); ++i) {
      const string& name = output_names[i];
      tensor_index.insert(make_pair(name, i));
    }
  }

  void BuildCallOptions(const RunOptions& options, 
      CallOptions & call_options) {
    call_options.SetTimeout(options.timeout_in_ms());
  }

  Status DoSaveOutputs(const TensorIndex& tensor_index,
      const std::vector<string>& output_names,
      MutableRunStepResponseWrapper* resp,
      std::vector<Tensor>* outputs) {
    for (size_t i = 0; i < resp->num_tensors(); ++i) {
      auto fetch_it = tensor_index.find(resp->tensor_name(i));
      if (fetch_it == tensor_index.end()) {
        return errors::Internal(
           "unrequested fetch: ", resp->tensor_name(i));
      }

      Tensor output;
      TF_RETURN_IF_ERROR(resp->TensorValue(i, &output));
      (*outputs)[fetch_it->second] = output;
    }  
  }

  Status SaveOutputs(const TensorIndex& tensor_index,
      const std::vector<string>& output_names,
      MutableRunStepResponseWrapper* resp,
      std::vector<Tensor>* outputs) {
    if (!output_names.empty()) {
      outputs->resize(output_names.size());
    }
    return DoSaveOutputs(tensor_index, 
        output_names, rsep, outputs);
  }

  void SaveRunMetaData(MutableRunStepResponseWrapper* resp,
      RunMetadata* run_metadata) {
    if (run_metadata) {
      run_metadata->Swap(resp->mutable_metadata());
    }
  }
  
  Status SaveRspToOutputs(const TensorIndex& tensor_index,
      const std::vector<string>& output_names,
      MutableRunStepResponseWrapper* resp,
      std::vector<Tensor>* outputs,
      RunMetadata* run_metadata) {
    SaveRunMetaData(resp, run_metadata);
    return SaveOutputs(tensor_index, output_names, rsep, outputs);
  }
}

Status GrpcSession::Run(
    const RunOptions& run_options,
    const vector<pair<string, Tensor>>& inputs,
    const vector<string>& output_names,
    const vector<string>& target_names,
    std::vector<Tensor>* outputs,
    RunMetadata* run_metadata) {
  // 1. Build run step request.
  unique_ptr<MutableRunStepRequestWrapper> req(
      master_->CreateRunStepRequest());

  unique_ptr<MutableRunStepResponseWrapper> resp(
      master_->CreateRunStepResponse());

  BuildRunStepReq(options_, run_options, inputs, 
      output_names, target_names, req.get());

  // 2. Build output tensor names index.
  TensorIndex tensor_index;
  BuildOuputNamesIndex(output_names, tensor_index);

  // 3. Build call options.
  CallOptions call_options;
  BuildCallOptions(req->options(), call_options)

  // 4. Do run step.
  TF_RETURN_IF_ERROR(RunProto(&call_options, 
      req.get(), resp.get()));

  // 5. Save response to outputs.
  return SaveRspToOutputs(tensor_index, output_names, 
      resp.get(), outputs, run_metadata);
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
Status GrpcSession::RunProto(
    CallOptions * call_options,
    MutableRunStepRequestWrapper* req,
    MutableRunStepResponseWrapper* resp) {
  {
    mutex_lock l(mu_);
    req->set_session_handle(handle_);
  }
  return master_->RunStep(call_options, req, resp);
}
\end{c++}
\end{leftbar}


\subsubsection{GrpcRemoteMaster::RunStep}

\begin{leftbar}
\begin{c++}
struct GrpcRemoteMaster : MasterInterface {
  using MasterServiceStub = ::grpc::MasterService::Stub;

  Status RunStep(CallOptions* call_options, RunStepRequestWrapper* request,
                 MutableRunStepResponseWrapper* response) override {
    ::grpc::ClientContext ctx;
    return Call(&ctx, call_options, &request->ToProto(),
                get_proto_from_wrapper(response),
                & MasterServiceStub :: RunStep);
  }
};
\end{c++}
\end{leftbar}


\subsubsection{GrpcMasterService::RunStepHandler}

\begin{leftbar}
\begin{c++}
struct GrpcMasterService : AsyncServiceInterface {
  using RunStepCall = MasterCall <RunStepRequest, RunStepResponse>;
 
  void RunStepHandler (RunStepCall * call) {
    CallOptions* call_opts = CreateCallOptions(call);

    RunStepRequestWrapper* wrapped_request =
        new ProtoRunStepRequest(&call->request);

    MutableRunStepResponseWrapper* wrapped_response =
        new NonOwnedProtoRunStepResponse(&call->response);
  
    call->SetCancelCallback([call_opts]() { 
        call_opts->StartCancel(); 
    });

    master_impl_->RunStep(call_opts, wrapped_request, wrapped_response,
      [call, call_opts, wrapped_request, wrapped_response](
          const Status& status) {
        call->ClearCancelCallback();
        delete call_opts;
        delete wrapped_request;
        call->SendResponse(ToGrpcStatus(status));
      });
    ENQUEUE_REQUEST(RunStep, true);
  }

 private:
  CallOptions * CreateCallOptions (RunStepCall * call) {
    CallOptions * call_opts = new CallOptions;
    if (call->request.options().timeout_in_ms() > 0) {
      call_opts->SetTimeout(call->request.options().timeout_in_ms());
    } else {
      call_opts->SetTimeout(default_timeout_in_ms_);
    }
    return call_opts; 
  }
};
\end{c++}
\end{leftbar}


\subsubsection{Master::RunStep}

\begin{leftbar}
\begin{c++}
void Master::RunStep(CallOptions* opts, 
    const RunStepRequestWrapper * req,
    MutableRunStepResponseWrapper* resp, 
    DoneClosure done) {
  auto session = FindMasterSession(req->session_handle());
  SchedClosure([this, session, opts, req, resp, done]() {
    Status status = session->Run(opts, *req, resp);
    session->Unref();
    done(status);
  });
}
\end{c++}
\end{leftbar}


\subsubsection{MasterSession::Run}

\begin{leftbar}
\begin{c++}
Status MasterSession::Run(
    CallOptions * opts, 
    const RunStepRequestWrapper & req,
    MutableRunStepResponseWrapper* resp) {
  Status status;
  if (!req.partial_run_handle().empty()) {
    status = DoPartialRun(opts, req, resp);
  } else {
    status = DoRunWithLocalExecution(opts, req, resp);
  }
  return status;
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
Status MasterSession::DoRunWithLocalExecution(
    CallOptions* opts, const RunStepRequestWrapper& req,
    MutableRunStepResponseWrapper* resp) {

  // 1. Prune: build ReffedClientGraph. 
  BuildGraphOptions bgopts;
  BuildBuildGraphOptions(req, &bgopts);
  
  ReffedClientGraph* rcg = nullptr;
  int64 count = 0;
  TF_RETURN_IF_ERROR(StartStep(bgopts, &count, &rcg, false));

  // 2. Build and Register partitions to workers. 
  core::ScopedUnref unref(rcg);
  TF_RETURN_IF_ERROR(BuildAndRegisterPartitions(rcg));

  // 3. Run partitions: notify all of workers to run partitions.
  uint64 step_id = (random::New64() & ((1uLL << 56) - 1)) | (1uLL << 56);
  Status s = rcg->RunPartitions(env_, step_id, count, &pss, opts, req, resp,
                                &cancellation_manager_, false);
  // 4. Cleaup Partitions: notify all of workers to clearup partitions.
  Ref();
  rcg->Ref();
  rcg->CleanupPartitionsAsync(step_id, [this, rcg](const Status& s) {
    rcg->Unref();
    Unref();
  });
  return s;
}
\end{c++}
\end{leftbar}


\subsubsection{MasterSession::BuildAndRegisterPartitions}

\begin{leftbar}
\begin{c++}
Status MasterSession::BuildAndRegisterPartitions(ReffedClientGraph* rcg) {
  PartitionOptions popts;
  popts.node_to_loc = SplitByWorker; // IMPORTANT
  popts.flib_def = rcg->client_graph()->flib_def.get();
  popts.control_flow_added = false;

  popts.new_name = [this](const string& prefix) {
    mutex_lock l(mu_);
    return strings::StrCat(prefix, "_S", next_node_id_++);
  };

  popts.get_incarnation = [this](const string& name) -> int64 {
    auto d = devices_->FindDeviceByName(name);
    return d->attributes().incarnation();
  };

  TF_RETURN_IF_ERROR(rcg->RegisterPartitions(popts));
  return Status::OK();
}
\end{c++}
\end{leftbar}


\subsubsection{ReffedClientGraph::RegisterPartitions}

\begin{leftbar}
\begin{c++}
Status ReffedClientGraph::RegisterPartitions(
    const PartitionOptions& popts) {
  { 
    mu_.lock();
    if (!init_started_) {
      init_started_ = true;
      Mu_.unlock();

      std::unordered_map<string, GraphDef> graph_defs;
      Status s = DoBuildPartitions(popts, &graph_defs);
      if (s.ok()) {
        s = DoRegisterPartitions(popts, std::move(graph_defs));
      }

      mu_.lock();
      init_result_ = s;
      init_done_.Notify();
    } else {
      Mu_.unlock();
      init_done_.WaitForNotification();
      mu_.lock();
    }
    Status result = init_result_;
    Mu_.unlock();
    return result;
  }
}
\end{c++}
\end{leftbar}


\subsection{Graph split: SplitByWorker}


\subsubsection{ReffedClientGraph::DoBuildPartitions}

\begin{leftbar}
\begin{c++}
Status MasterSession::ReffedClientGraph::DoBuildPartitions(
    PartitionOptions popts,
    std::unordered_map<string, GraphDef>* out_partitions) {
  // split full graph by worker name.
  return Partition(popts, &client_graph_->graph, out_partitions);
}
\end{c++}
\end{leftbar}


\subsection{Register Graph}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/dist-run-step-stage-2.png}
  \caption{RegisterGraph}
  \label{fig:dist-run-step-stage-2}
\end{figure}


\subsubsection{ReffedClientGraph::DoRegisterPartitions}

\begin{leftbar}
\begin{c++}
Status ReffedClientGraph::DoRegisterPartitions(
    const PartitionOptions& popts,
    std::unordered_map<string, GraphDef> graph_partitions) {
  partitions_.reserve(graph_partitions.size());
  Status s;
  for (auto& name_def : graph_partitions) {
    partitions_.resize(partitions_.size() + 1);
    Part* part = &partitions_.back();
    part->name = name_def.first;
    TrackFeedsAndFetches(part, name_def.second, popts);
    part->worker = worker_cache_->CreateWorker(part->name);
  }

  struct Call {
    RegisterGraphRequest req;
    RegisterGraphResponse resp;
    Status status;
  };

  const int num = partitions_.size();
  gtl::InlinedVector<Call, 4> calls(num);

  BlockingCounter done(num);
  for (int i = 0; i < num; ++i) {
    const Part& part = partitions_[i];
    Call* c = &calls[i];
    
    c->req.set_session_handle(session_handle_);
    c->req.mutable_graph_def()->Swap(&graph_partitions[part.name]);
    *c->req.mutable_graph_options() = session_opts_.config.graph_options();
    *c->req.mutable_debug_options() = debug_opts_;

    auto cb = [c, &done](const Status& s) {
      c->status = s;
      done.DecrementCount();
    };
    part.worker->RegisterGraphAsync(&c->req, &c->resp, cb);
  }
  done.Wait();

  for (int i = 0; i < num; ++i) {
    Call* c = &calls[i];
    s.Update(c->status);
    partitions_[i].graph_handle = c->resp.graph_handle();
  }
  return s;
}
\end{c++}
\end{leftbar}


\subsubsection{GrpcRemoteWorker::RegisterGraphAsync}

\begin{leftbar}
\begin{c++}
class GrpcRemoteWorker : public WorkerInterface {
  void RegisterGraphAsync(const RegisterGraphRequest* request,
                          RegisterGraphResponse* response,
                          StatusCallback done) override {
    IssueRequest(request, response, registergraph_, std::move(done));
  }

  void IssueRequest(const protobuf::Message* request,
                    protobuf::Message* response, const ::grpc::string& method,
                    StatusCallback done, CallOptions* call_opts = nullptr) {
    new RPCState<protobuf::Message>(counter_, &stub_, cq_, method, *request,
                                    response, std::move(done), call_opts);
  }
};
\end{c++}
\end{leftbar}


\subsubsection{GrpcWorkerService::RegisterGraphHandler}

\begin{leftbar}
\begin{c++}
class GrpcWorkerService : public AsyncServiceInterface {
  void RegisterGraphHandler(
      WorkerCall<RegisterGraphRequest, RegisterGraphResponse>* call) {
    Schedule([this, call]() {
      Status s = worker_->RegisterGraph(&call->request, &call->response);
      call->SendResponse(ToGrpcStatus(s));
    });
    ENQUEUE_REQUEST(RegisterGraph, false);
  }
};
\end{c++}
\end{leftbar}


\subsubsection{Worker::RegisterGraphAsync}

\begin{leftbar}
\begin{c++}
void Worker::RegisterGraphAsync(
    const RegisterGraphRequest* request,
    RegisterGraphResponse* response,
    StatusCallback done) {
  auto session = FindWorkerSession(request);
  Status s = session->graph_mgr->Register(
      request->session_handle(), 
      request->graph_def(), 
      request->graph_options(),
      response->mutable_graph_handle());
  done(s);
}
\end{c++}
\end{leftbar}


\subsubsection{GraphMgr::Register}

\begin{leftbar}
\begin{c++}
Status GraphMgr::Register(
    const string& session, 
    const GraphDef& gdef,
    const GraphOptions& graph_options,
    string* handle) {
  Item* item = new Item;
  Status s = InitItem (session, gdef, graph_options, item);
  if (!s.ok()) {
    item->Unref();
    return s;
  }

  // Generate unique graph\_handle, 
  // and register [graph\_handle, graph\_def] to table.
  {
    mutex_lock l(mu_);
    *handle = strings::Printf("%016llx", ++next_id_);
    item->handle = *handle;
    CHECK(table_.insert({*handle, item}).second);
  }
  return Status::OK();
}
\end{c++}
\end{leftbar}

\subsection{Graph split: SplitByDevice}

\begin{leftbar}
\begin{c++}
Status GraphMgr :: InitItem (
    const string& session, const GraphDef& gdef,
    const GraphOptions& graph_options,
    Item* item) {
  item->session = session;
  item->lib_def.reset(
      new FunctionLibraryDefinition(OpRegistry::Global(), gdef.library()));

  item->proc_flr.reset(new ProcessFunctionLibraryRuntime(
      device_mgr_, worker_env_->env, gdef.versions().producer(),
      item->lib_def.get(), graph_options.optimizer_options()));

  // 1. Constructs the full graph out of "gdef"
  Graph graph(OpRegistry::Global());
  GraphConstructorOptions opts;
  opts.allow_internal_ops = true;
  opts.expect_device_spec = true;
  TF_RETURN_IF_ERROR(ConvertGraphDefToGraph(opts, gdef, &graph));

  // 2. Splits "graph" into multiple subgraphs by device names.
  std::unordered_map<string, GraphDef> partitions;
  PartitionOptions popts;
  popts.node_to_loc = SplitByDevice;  // IMPORTANT.
  popts.new_name = [this](const string& prefix) {
    mutex_lock l(mu_);
    return strings::StrCat(prefix, "_G", next_id_++);
  };
  popts.get_incarnation = [this](const string& name) -> int64 {
    Device* device = nullptr;
    Status s = device_mgr_->LookupDevice(name, &device);
    if (s.ok()) {
      return device->attributes().incarnation();
    } else {
      return PartitionOptions::kIllegalIncarnation;
    }
  };
  popts.flib_def = &graph.flib_def();
  popts.control_flow_added = true;
  popts.scheduling_for_recvs = graph_options.enable_recv_scheduling();
  
  // IMPORTANT.  
  TF_RETURN_IF_ERROR(Partition(popts, &graph, &partitions));

  // 3. convert GraphDef partitions to Graph partitions.
  std::unordered_map<string, std::unique_ptr<Graph>> partition_graphs;
  for (const auto& partition : partitions) {
    std::unique_ptr<Graph> device_graph(new Graph(OpRegistry::Global()));
    GraphConstructorOptions device_opts;
    // There are internal operations (e.g., send/recv) that we now allow.
    device_opts.allow_internal_ops = true;
    device_opts.expect_device_spec = true;
    TF_RETURN_IF_ERROR(ConvertGraphDefToGraph(device_opts, partition.second,
                                              device_graph.get()));
    partition_graphs.emplace(partition.first, std::move(device_graph));
  }

  // 4. Build executors\_and\_partitions(item->units) = [(e0, p0), 
  // (e1, p1), ...], and (e\_n, p\_n) is called ExecutionUnit.
  LocalExecutorParams params;
  item->units.reserve(partitions.size());
  item->graph_mgr = this;

  for (auto& p : partition_graphs) {
    const string& device_name = p.first;
    std::unique_ptr<Graph>& subgraph = p.second;
    item->units.resize(item->units.size() + 1);
    ExecutionUnit* unit = &(item->units.back());

    // Construct the root executor for the subgraph.
    params.device = unit->device;
    params.function_library = lib;
    params.create_kernel = [session, lib, opseg](
        const NodeDef& ndef, OpKernel** kernel) {
      // Caches the kernel only if the node is stateful.
      if (!lib->IsStateful(ndef.op())) {
        return lib->CreateKernel(ndef, kernel);
      }
      auto create_fn = [lib, &ndef](OpKernel** kernel) {
        return lib->CreateKernel(ndef, kernel);
      };
      // Kernels created for subgraph nodes need to be cached.  On
      // cache miss, create\_fn() is invoked to create a kernel based
      // on the function library here + global op registry.
      return opseg->FindOrCreate(session, ndef.name(), kernel, create_fn);
    };

    params.delete_kernel = [lib](OpKernel* kernel) {
      // If the node is stateful, opseg owns it. Otherwise, delete it.
      if (kernel && !lib->IsStateful(kernel->type_string())) {
        delete kernel;
      }
    };

    unit->graph = subgraph.get();
    TF_RETURN_IF_ERROR(
        NewLocalExecutor(params, subgraph.release(), &unit->root));
  }
  return Status::OK();
}
\end{c++}
\end{leftbar}


\subsection{Run Graph}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/dist-run-step-stage-3.png}
  \caption{RunGraph}
  \label{fig:dist-run-step-stage-3}
\end{figure}


\subsubsection{ReffedClientGraph::RunPartitions}

\begin{leftbar}
\begin{c++}
Status MasterSession::ReffedClientGraph::RunPartitions(
    const MasterEnv* env, int64 step_id, int64 execution_count,
    PerStepState* pss, CallOptions* call_opts, const RunStepRequestWrapper& req,
    MutableRunStepResponseWrapper* resp, CancellationManager* cm,
    const bool is_last_partial_run) {


  // 1. Prepares a number of calls to workers. 
  //    One call per partition.
  const int num = partitions_.size();
  RunManyGraphs calls(num);

  for (int i = 0; i < num; ++i) {
    const Part& part = partitions_[i];
    RunManyGraphs::Call* c = calls.get(i);
    c->req.reset(part.worker->CreateRunGraphRequest());
    c->resp.reset(part.worker->CreateRunGraphResponse());
    if (is_partial_) {
      c->req->set_is_partial(is_partial_);
      c->req->set_is_last_partial_run(is_last_partial_run);
    }
    c->req->set_session_handle(session_handle_);
    c->req->set_graph_handle(part.graph_handle);
    c->req->set_step_id(step_id);
    
    for (const auto& feed_key : part.feed_key) {
      const string& feed = feed_key.first;
      const string& key = feed_key.second;
      const int64 feed_index = feeds[feed];
      TF_RETURN_IF_ERROR(
          c->req->AddSendFromRunStepRequest(req, feed_index, key));
    }

    for (const auto& key_fetch : part.key_fetch) {
      const string& key = key_fetch.first;
      c->req->add_recv_key(key);
    }
  }

  // 2. Issues RunGraph calls.
  for (int i = 0; i < num; ++i) {
    const Part& part = partitions_[i];
    RunManyGraphs::Call* call = calls.get(i);
    part.worker->RunGraphAsync(
        &call->opts, call->req.get(), call->resp.get(),
        std::bind(&RunManyGraphs::WhenDone, &calls, i, std::placeholders::_1));
  }

  // 3. Waits for the RunGraph calls.
  call_opts->SetCancelCallback([&calls]() { calls.StartCancel(); });
  auto token = cm->get_cancellation_token();
  bool success =
      cm->RegisterCallback(token, [&calls]() { calls.StartCancel(); });
  if (!success) {
    calls.StartCancel();
  }

  calls.Wait();

  call_opts->ClearCancelCallback();
  if (success) {
    cm->DeregisterCallback(token);
  } else {
    return errors::Cancelled("Step was cancelled");
  }

  // 4. Collects fetches.
  Status status = calls.status();
  if (status.ok()) {
    for (int i = 0; i < num; ++i) {
      const Part& part = partitions_[i];
      MutableRunGraphResponseWrapper* run_graph_resp = calls.get(i)->resp.get();
      for (size_t j = 0; j < run_graph_resp->num_recvs(); ++j) {
        auto iter = part.key_fetch.find(run_graph_resp->recv_key(j));
        if (iter == part.key_fetch.end()) {
          status.Update(errors::Internal("Unexpected fetch key: ",
                                         run_graph_resp->recv_key(j)));
          break;
        }
        const string& fetch = iter->second;
        status.Update(
            resp->AddTensorFromRunGraphResponse(fetch, run_graph_resp, j));
        if (!status.ok()) {
          break;
        }
      }
    }
  }
  return status;
}
\end{c++}
\end{leftbar}


\subsubsection{GrpcRemoteWorker::RunGraphAsync}

\begin{leftbar}
\begin{c++}
struct GrpcRemoteWorker : public WorkerInterface {
  void RunGraphAsync(
      CallOptions * call_opts, 
      RunGraphRequestWrapper* request,
      MutableRunGraphResponseWrapper* response,
      StatusCallback done) override {
    IssueRequest(&request->ToProto(), 
        get_proto_from_wrapper(response),
        rungraph_, std::move(done), call_opts);
  }
};
\end{c++}
\end{leftbar}


\subsubsection{GrpcWorkerService::RunGraphHandler}

\begin{leftbar}
\begin{c++}
struct GrpcWorkerService : AsyncServiceInterface {
  void RunGraphHandler(WorkerCall<RunGraphRequest, RunGraphResponse>* call) {
    Schedule([this, call]() {
      auto wrapped_req = new ProtoRunGraphRequest(&call->request);
      auto wrapped_rsp = new NonOwnedProtoRunGraphResponse(&call->response);
      
      auto call_opts = new CallOptions;
      call->SetCancelCallback([call_opts]() { 
          call_opts->StartCancel(); 
      });

      worker_->RunGraphAsync(call_opts, wrapped_req, wrapped_rsp, 
        [call, call_opts, wrapped_req, wrapped_rsp](const Status& s) {
            call->ClearCancelCallback();
            delete call_opts;
            delete wrapped_req;
            delete wrapped_rsp;
            call->SendResponse(ToGrpcStatus(s));
        });
    });
    ENQUEUE_REQUEST(RunGraph, true);
  }
};
\end{c++}
\end{leftbar}


\subsubsection{Worker::RunGraphAsync}

\begin{leftbar}
\begin{c++}
void Worker::RunGraphAsync(
    CallOptions * opts, 
    RunGraphRequestWrapper* request,
    MutableRunGraphResponseWrapper* response,
    StatusCallback done) {
  if (request->is_partial()) {
    DoPartialRunGraph(opts, request, response, std::move(done));
  } else {
    DoRunGraph(opts, request, response, std::move(done));
  }
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
void Worker::DoRunGraph(
    CallOptions * opts, 
    RunGraphRequestWrapper* request,
    MutableRunGraphResponseWrapper* response,
    StatusCallback done) {
  const int64 step_id = request->step_id();

  // 1. Prepare inputs and outputs.
  GraphMgr::NamedTensors in;
  GraphMgr::NamedTensors* out = new GraphMgr::NamedTensors;
  Status s = PrepareRunGraph(request, &in, out);
  if (!s.ok()) {
    delete out;
    done(s);
    return;
  }
  
  // 2. Register Cancellation callback.
  CancellationManager* cm = new CancellationManager;
  opts->SetCancelCallback([this, cm, step_id]() {
    cm->StartCancel();
    AbortStep(step_id);
  });

  CancellationToken token;
  {
    mutex_lock l(mu_);
    token = cancellation_manager_->get_cancellation_token();
    bool already_cancelled = !cancellation_manager_->RegisterCallback(
        token, [cm]() { cm->StartCancel(); });
    if (already_cancelled) {
      opts->ClearCancelCallback();
      delete cm;
      delete out;
      done(errors::Aborted("Call was aborted"));
      return;
    }
  }

  // 3. Start Execution.
  auto session =
      FindWorkerSession(request);

  session->graph_mgr->ExecuteAsync(
      request->graph_handle(), step_id, session, 
      request->exec_opts(), response, cm, in,
      [ this, step_id, response, session, cm, 
        out, token, opts, done](Status s) {
        
        // 4. Receive output tensors from grpc remote rendezvous.
        if (s.ok()) {
          s = session->graph_mgr->RecvOutputs(step_id, out);
        }

        // 5. Unregister Cancellation callback
        opts->ClearCancelCallback();
        {
          mutex_lock l(mu_);
          cancellation_manager_->DeregisterCallback(token);
        }
        delete cm;

        // 6. Save to RunStepResponse.
        if (s.ok()) {
          for (const auto& p : *out) {
            const string& key = p.first;
            const Tensor& val = p.second;
            response->AddRecv(key, val);
          }
        }
        delete out;
        done(s);
      });
}
\end{c++}
\end{leftbar}


\subsubsection{GraphMgr}

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/dist-run-step-overview.png}
  \caption{Worker: RunStep Formalization}
  \label{fig:dist-run-step-overview}
\end{figure}

\begin{leftbar}
\begin{c++}
void GraphMgr::ExecuteAsync(
    const string& handle, const int64 step_id,
    WorkerSession* session, const ExecutorOpts& opts,
    MutableRunGraphResponseWrapper* response,
    CancellationManager* cancellation_manager,
    const NamedTensors& in, StatusCallback done) {
  // 1. Lookup an item. Holds one ref while executing.
  //    One item per registered graph.
  Item* item = nullptr;
  {
    mutex_lock l(mu_);
    auto iter = table_.find(handle);
    if (iter != table_.end()) {
      item = iter->second;
      item->Ref();
    }
  }

  RemoteRendezvous* rendezvous = worker_env_->rendezvous_mgr->Find(step_id);
  Status s = rendezvous->Initialize(session);

  // 2. Sends inputs to rendezvous.
  if (s.ok()) {
    s = SendInputsToRendezvous(rendezvous, in);
  }

  // 3. Start parallel executors.
  StartParallelExecutors(
      handle, step_id, item, rendezvous, collector,
      cost_graph, cancellation_manager,
      [this, item, rendezvous, done](const Status& s) {
          // 4. Recvs outputs from rendezvous.
          done(s);
          rendezvous->Unref();
          item->Unref();
      });
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
Status GraphMgr::SendInputsToRendezvous(
    Rendezvous* rendezvous, const NamedTensors& in) {
  Rendezvous::ParsedKey parsed;
  for (auto& p : in) {
    auto& key = p.first;
    auto& val = p.second;

    Status s = Rendezvous::ParseKey(key, &parsed);
    if (s.ok()) {
      s = rendezvous->Send(parsed, Rendezvous::Args(), val, false);
    }
    if (!s.ok()) {
      return s;
    }
  }
  return Status::OK();
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
void GraphMgr::StartParallelExecutors(
    const string& handle, int64 step_id,
    Item* item, Rendezvous* rendezvous,
    StepStatsCollector* collector,
    CancellationManager* cancellation_manager,
    StatusCallback done) {
  
  // 1. Wait until pending == 0, with default is num\_units,
  // `pending -= 1` when one partition graph is done. 
  int num_units = item->units.size();
  ExecutorBarrier* barrier =
      new ExecutorBarrier(
          num_units, rendezvous, [done](const Status& s) {
              done(s);
          });

  Executor::Args args;
  {
    mutex_lock l(mu_);
    args.step_id = ++next_id_;
  }
  args.rendezvous = rendezvous;
  args.cancellation_manager = cancellation_manager;
  args.stats_collector = collector;
  args.step_container = step_container;
  args.sync_on_finish = sync_on_finish_;

  using std::placeholders::_1;
  args.runner = std::bind(
      &thread::ThreadPool::Schedule, 
      worker_env_->compute_pool, _1);

  2. Broadcast all partitions to run
  for (const auto& unit : item->units) {
    unit.root->RunAsync(args, barrier->Get());
  }
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
Status GraphMgr::RecvOutputsFromRendezvous(
    Rendezvous* rendezvous, NamedTensors* out) {
  // Receives values requested by the caller.
  Rendezvous::ParsedKey parsed;
  for (auto& p : *out) {
    auto& key = p.first;
    auto& val = p.second;

    bool is_dead = false;
    Status s = Rendezvous::ParseKey(key, &parsed);
    if (s.ok()) {
      s = rendezvous->Recv(parsed, Rendezvous::Args(), &val, &is_dead);
    }

    if (is_dead) {
      s = errors::InvalidArgument("The tensor returned for ", key,
                                  " was not valid.");
    }

    if (!s.ok()) {
      return s;
    }
  }
  return Status::OK();
}

Status GraphMgr::RecvOutputs(int64 step_id, NamedTensors* out) {
  Rendezvous* rendezvous = worker_env_->rendezvous_mgr->Find(step_id);
  Status s = RecvOutputsFromRendezvous(rendezvous, out);
  rendezvous->Unref();
  return s;
}
\end{c++}
\end{leftbar}


\subsection{Rendzvous}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/rendezvous-hierarchy.png}
  \caption{Rendezvous hierarchy}
  \label{fig:rendezvous-hierarchy}
\end{figure}


\subsubsection{Polymorphic creation}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/rendezvous-remote-mgr.png}
  \caption{RemoteRendezvous polymorphic creation}
  \label{fig:rendezvous-remote-mgr}
\end{figure}


\subsubsection{Send}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/rendzvous-send.png}
  \caption{Rendezvous send}
  \label{fig:rendzvous-send}
\end{figure}


\subsubsection{Receive}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/rendezvous-recv-case-1.png}
  \caption{Rendezvous receiving: distributed sender and receiver are in the same worker}
  \label{fig:rendezvous-recv-case-1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\textwidth]{figures/rendezvous-recv-case-2.png}
  \caption{Rendezvous receiving: The distributed sender and receiver are not in the same worker}
  \label{fig:rendezvous-recv-case-2}
\end{figure}


\subsection{Logout Map}

\end{content}


\section{Close session}
\begin{content}


\subsubsection{GrpcSession}

\begin{leftbar}
\begin{c++}
Status GrpcSession::Close() {
  CloseSessionRequest req;
  {
    mutex_lock l(mu_);
    if (handle_.empty()) {
      return errors::InvalidArgument("A session is not created yet....");
    }
    req.set_session_handle(handle_);
    handle_.clear();
  }
  CloseSessionResponse resp;
  CallOptions call_options;
  call_options.SetTimeout(options_.config.operation_timeout_in_ms());
  return master_->CloseSession(&call_options, &req, &resp);
}
\end{c++}
\end{leftbar}


\subsubsection{GrpcRemoteMaster}

\begin{leftbar}
\begin{c++}
struct GrpcRemoteMaster : MasterInterface {
  Status CloseSession(CallOptions* call_options,
                      const CloseSessionRequest* request,
                      CloseSessionResponse* response) override {
    ::grpc::ClientContext ctx;
    ctx.set_fail_fast(false);
    SetDeadline(&ctx, call_options->GetTimeout());
    return FromGrpcStatus(stub_->CloseSession(&ctx, *request, response));
  }
};
\end{c++}
\end{leftbar}


\subsubsection{GrpcMasterService}

\begin{leftbar}
\begin{c++}
struct GrpcMasterService : AsyncServiceInterface {
  void CloseSessionHandler(
      MasterCall<CloseSessionRequest, CloseSessionResponse>* call) {
    master_impl_->CloseSession(&call->request, &call->response,
                               [call](const Status& status) {
                                 call->SendResponse(ToGrpcStatus(status));
                               });
    ENQUEUE_REQUEST(CloseSession, false);
  }
};
\end{c++}
\end{leftbar}


\subsubsection{Master}

\begin{leftbar}
\begin{c++}
void Master::CloseSession(const CloseSessionRequest* req,
                          CloseSessionResponse* resp, MyClosure done) {
  MasterSession* session = nullptr;
  {
    mu_.lock();
    auto iter = sessions_.find(req->session_handle());
    if (iter == sessions_.end()) {
      Mu_.unlock();
      done(errors::Aborted(
          "Session ", req->session_handle(),
          " is not found. Possibly, this master has restarted."));
      return;
    }
    session = iter->second;
    sessions_.erase(iter);
    Mu_.unlock();
  }

  // Session Close() blocks on thread shutdown. Therefore, we need to
  // delete it in non-critical thread.
  SchedClosure([session, done]() {
    Status s = session->Close();
    session->Unref();
    done(s);
  });
}
\end{c++}
\end{leftbar}


\subsubsection{MasterSession}

\begin{leftbar}
\begin{c++}
Status MasterSession::Close() {
  {
    mutex_lock l(mu_);
    closed_ = true;  // All subsequent calls to Run() or Extend() will fail.
  }
  cancellation_manager_.StartCancel();
  std::vector<ReffedClientGraph*> to_unref;
  {
    mutex_lock l(mu_);
    while (num_running_ != 0) {
      num_running_is_zero_.wait(l);
    }
    ClearRunsTable(&to_unref, &run_graphs_);
    ClearRunsTable(&to_unref, &partial_run_graphs_);
  }
  for (ReffedClientGraph* rcg : to_unref) rcg->Unref();
  return Status::OK();
}
\end{c++}
\end{leftbar}


\subsubsection{ReffedClientGraph}

\begin{leftbar}
\begin{c++}
ReffedClientGraph::~ReffedClientGraph() { 
  DeregisterPartitions (); 
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
void ReffedClientGraph::DeregisterPartitions() {
  struct Call {
    DeregisterGraphRequest req;
    DeregisterGraphResponse resp.
  };
  for (Part& part : partitions_) {
    if (!part.graph_handle.empty()) {
      Call* c = new Call;
      c->req.set_session_handle(session_handle_);
      c->req.set_graph_handle(part.graph_handle);

      WorkerCacheInterface* worker_cache = worker_cache_;
      const string name = part.name;
      WorkerInterface* w = part.worker;

      auto cb = [worker_cache, c, name, w](const Status& s) {
        if (!s.ok()) {
          // This error is potentially benign, so we don't log at the
          // error level.
          LOG(INFO) << "DeregisterGraph error: " << s;
        }
        delete c;
        worker_cache->ReleaseWorker(name, w);
      };
      w->DeregisterGraphAsync(&c->req, &c->resp, cb);
    }
  }
}
\end{c++}
\end{leftbar}


\subsubsection{GrpcWorkerService}

\begin{leftbar}
\begin{c++}
struct GrpcWorkerService : AsyncServiceInterface {
  void CreateWorkerSessionHandler(
      WorkerCall<CreateWorkerSessionRequest, CreateWorkerSessionResponse>*
          call) {
    Schedule([this, call]() {
      Status s = worker_->CreateWorkerSession(&call->request, &call->response);
      call->SendResponse(ToGrpcStatus(s));
    });
    ENQUEUE_REQUEST(CreateWorkerSession, false);
  }
};
\end{c++}
\end{leftbar}


\subsubsection{Worker}

\begin{leftbar}
\begin{c++}
void Worker::DeregisterGraphAsync(const DeregisterGraphRequest* request,
                                  DeregisterGraphResponse* response,
                                  StatusCallback done) {
  WorkerSession* session =
      env_->session_mgr->WorkerSessionForSession(request->session_handle());
  Status s = session->graph_mgr->Deregister(request->graph_handle());

  done(s);
}
\end{c++}
\end{leftbar}


\subsubsection{GraphMgr}

\begin{leftbar}
\begin{c++}
Status GraphMgr::Deregister(const string& handle) {
  Item* item = nullptr;
  {
    mutex_lock l(mu_);
    auto iter = table_.find(handle);
    if (iter == table_.end()) {
      return errors::Aborted("Graph handle is not found: ", handle,
                             ". Possibly, this worker just restarted.");
    }
    item = iter->second;
    table_.erase(iter);
  }
  item->Unref();
  return Status::OK();
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
GraphMgr::Item::~Item() {
  for (const auto& unit : this->units) {
    delete unit.root;
    unit.device->op_segment()->RemoveHold(this->session);
  }
}
\end{c++}
\end{leftbar}

\end{content}
