\begin{savequote}[45mm]
\ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
\qauthor{\ascii{- Martin Flower}}
\end{savequote}

\chapter{local execution} 
\label{ch:local}

\begin{content}

\tf{} can be run independently in a process to complete the execution of the calculation graph. This chapter will focus on the basic architecture and operational mechanisms of the local runtime; focus on the implementation details of computational graph pruning, splitting, optimization, execution, etc.; and explore in detail the data between \ascii{OP} across devices. The working mechanism of the interaction, and its \ascii{OP} orchestration on the device set (\ascii{placement}) algorithm.

\end{content}

\section{Local Mode}
\label{sec:local-runtime}

\begin{content}

As shown in \refig{local}, in local mode, \ascii{Client, Master, Worker} is deployed in the same process on the same machine, and these three roles are played by \code{DirectSession} at the same time. \code{DirectSession} runs in a separate process, with function call relationships between service entities.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/local.png}
\caption{local mode}
 \label{fig:local}
\end{figure}

\ascii{Client} is responsible for computing the construction of the graph, starting the execution of the graph by calling \code{Session.run}. As shown in \refig{local-runtime}, during the execution of \code{run\_step}, there are three important stages involved in the pruning, splitting, and execution of computational graphs.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/local-runtime.png}
\caption{Local mode: Figure operation}
 \label{fig:local-runtime}
\end{figure}

\subsection{Partial execution}

\ascii{Master} starts the pruning operation of the calculation graph after receiving the calculation diagram execution command. It reverses the traversal map based on the input and output of the computed graph, looking for a minimally dependent subgraph, often called \code{ClientGraph}.

That is to say, each time you execute \code{run\_step}, the entire calculation graph (\code{FullGraph}) is not executed, but a partial subgraph is executed. The pruning reflects the design concept of the \tf{} part of the implementation.

\subsection{Concurrent execution}

Then, the runtime completes the splitting of the graph according to the current device set, and generates a lot of subgraphs, each of which is called \code{PartitionGraph}; then triggers each \ascii{Worker} to execute each \code{PartitionGraph} concurrently; For each \ascii{PartitionGraph}, the runtime will launch a \ascii{Executor} to complete the execution of \code{PartitionGraph} according to its topology.

In other words, splitting and execution embody the design philosophy of \tf{} concurrent execution.

\section{session control}

In local mode, its runtime is controlled by \code{DirectSession}. In general, when \code{DirectSession} executes a calculation graph, each component is a function call relationship. However, \code{DirectSession} also has a clear lifecycle management mechanism, as shown by \refig{local-direct-session-lifecycle}.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/local-direct-session-lifecycle.png}
\caption{DirectSession lifecycle}
 \label{fig:local-direct-session-lifecycle}
\end{figure}

\subsection{domain model}

As shown by \refig{local-direct-session-model}, \code{DirectSession} holds the \code{SimpleGraphExecutionState} instance, which is responsible for computing the pruning of the graph and generating the \code{ClientGraph} instance.

\code{DirectSession} holds a set of thread pools at the same time, but when \code{DirectSession.run} is not used, it selects one of the thread pool groups to provide services according to the externally configured index. Because \code{DirectSession} is thread-safe and supports multiple concurrent executions of \code{DirectSession.run}, multiple thread pool instances can be run simultaneously.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/local-direct-session-model.png}
\caption{DirectSession Domain Model}
 \label{fig:local-direct-session-model}
\end{figure}

\subsection{Create session}

As shown by \refig{local-direct-session-factory}, \code{DirectSession} is created by \code{DirectSessionFactory} polymorphism. Where \code{DeviceFactory::AddDevices} will create a local device set.

Among them, \code{DirectSession} mainly completes the creation of the thread pool group.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/local-direct-session-factory.png}
\caption{Polymorphic creation of DirectSession}
 \label{fig:local-direct-session-factory}
\end{figure}

\begin{leftbar}
\begin{c++}
struct DirectSessionFactory : SessionFactory {
  bool AcceptsOptions(const SessionOptions& options) override {
    return options.target.empty();
  }

  Session* NewSession(const SessionOptions& options) override {
    std::vector<Device*> devices;
    DeviceFactory::AddDevices(
        options, "/job:localhost/replica:0/task:0", &devices);
    return new DirectSession(options, new DeviceMgr(devices));
  }
};
\end{c++}
\end{leftbar}

Where \code{DirectSessionFactory::NewSession} is called by \ascii{C API}.

\begin{leftbar}
\begin{c++}
Status NewSession(const SessionOptions& options, Session** out_session) {
  SessionFactory* factory;
  Status s = SessionFactory::GetFactory(options, &factory);
  if (!s.ok()) {
    *out_session = nullptr;
    return s;
  }
  *out_session = factory->NewSession(options);
  if (!*out_session) {
    return errors::Internal("Failed to create session.");
  }
  return Status::OK();
}

TF_DeprecatedSession* TF_NewDeprecatedSession(
  const TF_SessionOptions* opt, TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    return new TF_DeprecatedSession({session});
  } else {
    return nullptr;
  }
}
\end{c++}
\end{leftbar}

In the constructor of \code{DirectSession}, it is mainly responsible for the initialization of its domain model, including the creation of the thread pool, and the construction of the \code{CancellationManager} instance.

\begin{leftbar}
\begin{c++}
DirectSession::DirectSession(
    const SessionOptions& options,
    const DeviceMgr* device_mgr)
    : options_(options),
      device_mgr_(device_mgr),
      cancellation_manager_(new CancellationManager()) {
  // thread\_pools\_ = ... 
}
\end{c++}
\end{leftbar}

\subsection{destroy session}

\code{DirectSession} from \ascii{SessionFactory}\code{new} is responsible for \code{delete} by \ascii{C API}.

\begin{leftbar}
\begin{c++}
void TF_DeleteDeprecatedSession(TF_DeprecatedSession* s, TF_Status* status) {
  status->status = Status::OK();
  delete s->session;  // delete DirectSession
  delete s;
}
\end{c++}
\end{leftbar}

Subsequently, the destructor of \code{DirectSession} is called, which is responsible for cleaning up the system resources it is responsible for. It mainly includes the \code{Executor} list, the \code{ThreadPool} list, and the \code{CancellationManager} instance.

\begin{leftbar}
\begin{c++}
DirectSession::~DirectSession() {
  for (auto& it : partial_runs_) {
    it.second.reset(nullptr);
  }
  
  for (auto& it : executors_) {
    it.second.reset();
  }
  
  for (auto d : device_mgr_->ListDevices()) {
    d->op_segment()->RemoveHold(session_handle_);
  }
  
  delete cancellation_manager_;
  
  for (const auto& p_and_owned : thread_pools_) {
    if (p_and_owned.second) delete p_and_owned.first;
  }

  execution_state_.reset(nullptr);
  flib_def_.reset(nullptr);
}
\end{c++}
\end{leftbar}

\subsection{Create/Extension Map}

The first expansion of the diagram is equivalent to creating a diagram. The expanded graph is based on the original calculation graph, and a new subgraph is added. Of course, the nodes included in the added subgraph should not exist in the original calculation graph.


\begin{leftbar}
\begin{c++}
Status DirectSession::Create(const GraphDef& graph) {
  if (graph.node_size() > 0) {
    mutex_lock l(graph_def_lock_);
    return ExtendLocked(graph);
  }
  return Status::OK();
}

Status DirectSession::Extend(const GraphDef& graph) {
  mutex_lock l(graph_def_lock_);
  return ExtendLocked(graph);
}
\end{c++}
\end{leftbar}

When creating a calculation graph, \code{DirectSession} mainly completes the creation of the \code{SimpleGraphExecutionState} instance. As shown in \refig{local-simple-graph-execution-state-model}, the \code{SimpleGraphExecutionState} instance holds instances of \code{FullGraph}: \code{Graph} and \code{GraphDef}, It is responsible for managing and maintaining the lifecycle of \code{FullGraph}.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/local-simple-graph-execution-state-model.png}
\caption{Create a SimpleGraphExecutionState instance}
 \label{fig:local-simple-graph-execution-state-model}
\end{figure}

Among them, the main responsibilities of \code{SimpleGraphExecutionState} include:

\begin{enum}
  \eitem{construct \code{FullGraph}}: Occurs in \code{DirectSession.Create};
  \eitem{Execute simple \ascii{OP} orchestration algorithm}: Occurs in \code{DirectSession.Create};
  \eitem{Execute pruning operation}: Occurs in \code{DirectSession.Run}.
\end{enum}

When \code{DirectSession::Create} is executed, the \code{SimpleGraphExecutionState} instance is created and the build and initialization of the \code{FullGraph} instance is completed.

\begin{leftbar}
\begin{c++}
Status SimpleGraphExecutionState::MakeForBaseGraph(
    GraphDef* graph_def, const SimpleGraphExecutionStateOptions& opts,
    std::unique_ptr<SimpleGraphExecutionState>* out_state) {
  auto ret = std::make_unique<SimpleGraphExecutionState>(graph_def, opts));

  AddDefaultAttrsToGraphDef(&ret->original_graph_def_, *ret->flib_def_, 0));
  if (!ret->session_options_->config.graph_options().place_pruned_graph()) {
    direction> InitBaseGraph ();
  }
  *out_state = std::move(ret);
  return Status::OK();
}
\end{c++}
\end{leftbar}

Among them, \code{SimpleGraphExecutionState::InitBaseGraph} completes the format conversion of \code{FullGraph} from \code{GraphDef} to \code{Graph} and starts the \ascii{OP} orchestration algorithm of \code{SimplePlacer}.

\begin{leftbar}
\begin{c++}
Status SimpleGraphExecutionState::InitBaseGraph() {
  auto ng = std::make_unique<Graph>(OpRegistry::Global());

  GraphConstructorOptions opts;
  ConvertGraphDefToGraph(opts, *original_graph_def_, ng.get());

  SimplePlacer placer(ng.get(), device_set_, session_options_);
  placer.Run ();

  this-> graph_ = ng.release ();
  return Status::OK();
}
\end{c++}
\end{leftbar}

\subsubsection{Figure construction: GraphDef -> Graph}

At the beginning, \code{SimpleGraphExecutionState} got \code{GraphDef}, which is the original primitive structure. It is serialized by \ascii{Client} and passed to the backend \ascii{C++}, which is then deserialized by the backend.

As shown in \refig{local-graph-def-to-graph}, convert the \code{GraphDef} instance to the equivalent \code{Graph} instance by calling \code{ConvertGraphDefToGraph}; similarly, you can call \code {Graph.ToGraphDef} transforms the \code{Graph} instance into an equivalent \code{GraphDef} instance.

Where \code{GraphDef} is a graph structure that exists in the \ascii{protobuf} format, which contains all the metadata of the graph; and \code{Graph} is the domain object used to describe the graph structure in the runtime system. Not only holds the metadata of \code{GraphDef}, but also contains other information about other graph structures.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/local-graph-def-to-graph.png}
Format conversion between \caption{\code{GraphDef} and \code{Graph}}
 \label{fig:local-graph-def-to-graph}
\end{figure}

\subsubsection{OP orchestration: SimplePlacer}

The \ascii{OP} orchestration (\ascii{placement}) refers to the computation of the resources in the most efficient way by placing the \ascii{OP} contained in the calculation graph on the appropriate computing device. Utilization, which can be formally described as \refig{local-cost-model}.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/local-cost-model.png}
\caption{cost model}
 \label{fig:local-cost-model}
\end{figure}

To find the optimal arrangement, I guess this is a \ascii{NP} problem. This problem depends on the characteristics of the computational graph, network topology and bandwidth, the number of samples, and many other complex factors, which is one of the most active issues in the community.

\subsection{iteration execution}

\code{DirectSession.Run} is the key path to the \tf{} runtime and is responsible for completing an iterative calculation. First, \code{DirectSession} prunes the \code{FullGraph} based on the input/output to generate \code{ClientGraph}; then, splits \code{ClientGraph} into multiple \codes based on the set of local devices held. {PartitionGraph}; The runtime starts a \code{Executor} instance for each of its \code{PartitionGraph}, which executes the topological sorting algorithm of \code{PartitionGraph} to complete the execution of the computed graph.

For specific implementation, please refer to \refsec{graph-operation-prune}, \refsec{graph-operation-split}, \refsec{graph-operation-exec}.

\subsubsection{图操作}

As shown in \refig{local-graph-transformation}, in local mode, the computational graph undergoes three forms of transformation and is ultimately decomposed into computational devices to enable concurrent execution of subgraphs on various computing devices.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/local-graph-transformation.png}
\caption{图转换}
 \label{fig:local-graph-transformation}
\end{figure}

\begin{itemize}
  \item \code{FullGraph}: \ascii{Client} is responsible for constructing the complete calculation graph, often called \code{FullGraph}; however, once \code{Session.run} does not execute the entire calculation graph;
  \item \code{ClientGraph}: \ascii{Master} passes the \code{feeds, fetches} input and output list according to \code{Session.run}, implements pruning operation on \ascii{FullGraph}, and calculates the local iteration execution. The least dependent subgraph, often called \code{ClientGraph};
  \item \code{PartitionGraph}: \ascii{Master} splits \code{ClientGraph} into multiple \code{PartitionGraph} based on the current set of computing devices and its \ascii{OP} device constraint specification; The computing device corresponds to a \code{PartitionGraph}, and the computing device is responsible for the execution of \code{PartitionGraph}.
\end{itemize}

However, the data structures of \code{FullGraph, ClientGraph, PartitionGraph} are the same, they are all three different manifestations of \code{Graph}, only the size and scope are different.

\subsubsection{formalization}

In real system implementations, the local mode runtime is implemented using \ascii{C++}. The key path to the \tf{} runtime is \code{run\_step}. Because the real system implementation involves too much detail, it is not easy to find the backbone and logic of the algorithm. To simplify the description of the problem, the implementation of \code{run\_step} will be described formally.

\begin{leftbar}
\ Begin {python}
def do_run_partitions(executors_and_partitions):
  barrier = ExecutorBarrier(executors_and_partitions.size())
  for (executor, partition) in executors_and_partitions:
    executor.run(partition, barrier)  
  barrier.wait()

def run_partitions(executors_and_partitions, inputs, outputs):
  frame = FunctionCallFrame()
  frame.set_args(inputs)
  do_run_partitions(executors_and_partitions)
  frame.get_ret_vals(outputs)

def run_step(devices, full_graph, inputs, outputs):
  client_graph = prune(full_graph, inputs, outputs)
  executors_and_partitions = split(client_graph, devices)
  run_partitions(executors_and_partitions, inputs, outputs)
\end{python}
\end{leftbar}

Among them, on each computing device, start a \code{Executor} to execute the \code{PartitionGraph} assigned to it. After a certain computing device executes the allocated \code{PartitionGraph}, the counter of \code{ExecutorBarrier} is added to \ascii{1} until all devices complete the execution of the \code{PartitionGraph} list, \code{barrier.wait ()} Blocking operation exits.

There may be data dependencies between \code{PartitionGraph} across devices, and they interact by inserting \code{Send/Recv} nodes. In fact, in local mode, \code{Send/Recv} does the data exchange via \code{Rendezvous}. \code{Send} puts the data on \code{Rendezvous}, and \code{Recv} takes it from \code{Rendezvous} according to the identifier. Where \code{Send} is not blocked, and \code{Recv} is blocked.

\subsection{Close session}

\begin{leftbar}
\begin{c++}
Status DirectSession::Close() {
  cancellation_manager_->StartCancel();
  {
    mutex_lock l(closed_lock_);
    if (closed_) return Status::OK();
    closed_ = true;
  }
  return Status::OK();
}
\end{c++}
\end{leftbar}

Register \ascii{Step} to \code{CancellationManager} of \code{DirectSession} as shown in \refig{local-cancellation-manager}. When \code{DirectSession} is closed, the \code{CancellationManager} of \code{DirectSession} will cancel the execution of this \ascii{step}.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/local-cancellation-manager.png}
\caption{How does CancellationManager work}
 \label{fig:local-cancellation-manager}
\end{figure}

\begin{leftbar}
\begin{c++}
Status DirectSession::Run(
   const NamedTensorList& inputs,
   const std::vector<string>& output_names,
   const std::vector<string>& target_nodes,
   std::vector<Tensor>* outputs) {
  // step\_cancellation\_manager is passed to `OpKernelContext`
  CancellationManager step_cancellation_manager;

  // Register this step with session's cancellation manager, so that
  // `Session::Close()` will cancel the step.
  CancellationToken cancellation_token =
      cancellation_manager_->get_cancellation_token();
  bool already_cancelled = !cancellation_manager_->RegisterCallback(
      cancellation_token, [&step_cancellation_manager]() {
        step_cancellation_manager.StartCancel();
      });
  // ignore others...
}
\end{c++}
\end{leftbar}

The \code{CancellationManager} of the current \ascii{Step} will eventually be passed to \code{OpKernelContext}. When \ascii{Kernel} implements the calculation, if the intermediate state is saved, you can register the corresponding callback hook with it. Among them, each callback hook has a unique \code{token} identifier.

When \ascii{Step} is canceled, the callback hook is called, and the \ascii{Kernel} can cancel the calculation of the \ascii{OP}. For example, when \code{FIFOQueue} implements \code{TryEnqueue}, a callback hook is registered to the \code{CancellationManager} of this \ascii{Step} to cancel the status information in the middle of \ascii{Kernel}.

\begin{leftbar}
\begin{c++}
void FIFOQueue::TryEnqueue(const Tuple& tuple, OpKernelContext* ctx,
                           DoneCallback callback) {
  CancellationManager* cm = ctx->cancellation_manager();
  CancellationToken token = cm->get_cancellation_token();
  bool already_cancelled;
  {
    mutex_lock l(mu_);
    already_cancelled = !cm->RegisterCallback(
        token, [this, cm, token]() { Cancel(kEnqueue, cm, token); });
  }
  // ignore others...
}
\end{c++}
\end{leftbar}


\section{Pruning}
\label{sec:graph-operation-prune}

When \code{DirectSession::Run} is executed, the construct of \code{ClientGraph} is completed first. In fact, the construction process of \code{ClientGraph} mainly completes the pruning algorithm of \code{FullGraph} and generates \code{ClientGraph}.

\subsection{Build ClientGraph}

As shown in \refig{local-simple-graph-execution-state}, the \code{SimpleGraphExecutionState} instance holds the \code{FullGraph} instance and generates \code{ClientGraph} based on the input/output list.

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{figures/local-simple-graph-execution-state.png}
\caption{Generate\code{ClientGraph}}
 \label{fig:local-simple-graph-execution-state}
\end{figure}

Where \code{BuildGraphOptions} contains the input/output list and calls \code{SimpleGraphExecutionState::BuildGraph} to generate the \code{ClientGraph} instance.

\begin{leftbar}
\begin{c++}
namespace {
  BuildGraphOptions build_graph_options(
    const NamedTensorList& inputs,
    const std::vector<string>& outputs,
    const std::vector<string>& targets) {
    // sort inputs/outputs/targets
    std::vector<string> inputs_sorted(inputs.begin(), inputs.end());
    std::sort(inputs_sorted.begin(), inputs_sorted.end());

    std::vector<string> outputs_sorted(outputs.begin(), outputs.end());
    std::sort(outputs_sorted.begin(), outputs_sorted.end());

    std::vector<string> tn_sorted(targets.begin(), targets.end());
    std::sort(tn_sorted.begin(), tn_sorted.end());

    // build graph options
    BuildGraphOptions options;
    options.feed_endpoints = inputs_sorted;
    options.fetch_endpoints = outputs_sorted;
    options.target_nodes = tn_sorted;
    options.use_function_convention = !run_state_args->is_partial_run;
    return options;
  }
}

Status DirectSession::Run(
  const RunOptions& run_options,
  const NamedTensorList& inputs,
  const std::vector<string>& output_names,
  const std::vector<string>& target_nodes,
  std::vector<Tensor>* outputs,
  RunMetadata* run_metadata) {

  // 1. prune graph
  // client\_graph = prune(full\_graph, inputs, outputs)
  std :: unique_ptr <SimpleClientGraph> client_graph;
  execution_state_->BuildGraph(
    build_graph_options(inputs, output_names, target_nodes), 
    &client_graph);
   
  // 2. split graph into partition by devices 
  // executors\_and\_partitions = split(client\_graph, devices)
  
  // 3. lauch executor per partition
  // def run\_partitions(executors\_and\_partitions, inputs, outputs):
  // \ \ frame = FunctionCallFrame()
  // \ \ frame.set\_args(inputs)
  // \ \ for (executor, partition) in executors\_and\_partitions: 
  // \ \ \ \ exec.run(part)
  // \ \ frame.get\_ret\_vals(outputs)

  return Status::OK();
}
\end{c++}
\end{leftbar}

\code{ClientGraph} is initially from the original \code{FullGraph}, calling the \code{RewriteGraphForExecution} function, which will rewrite the \code{ClientGraph} according to the input/output, including adding nodes, or deleting nodes, and finally generating\ Code{SimpleClientGraph} instance.

\begin{leftbar}
\begin{c++}
const DeviceAttributes& 
SimpleGraphExecutionState::local_device_attr() const {
  return device_set_->client_device()->attributes();
}

Status SimpleGraphExecutionState::BuildGraph(
  const BuildGraphOptions& options, 
  std::unique_ptr<SimpleClientGraph>* out) {
  // 1. create new\_graph from origin graph, 
  // which is client graph.
  std::unique_ptr<Graph> ng;
  ng.reset(new Graph(flib_def_.get()));
  CopyGraph (* graph_, ng.get ());

  // 2. prune the client graph
  subgraph::RewriteGraphForExecution(
    ng.get(), options.feed_endpoints, options.fetch_endpoints,
    options.target_nodes, local_device_attr(),
    options.use_function_convention);
  }

  // 3. create SimpleClientGraph, and return it.
  std::unique_ptr<SimpleClientGraph> dense_copy(
      new SimpleClientGraph(std::move(flib)));
  CopyGraph (* ng, & dense_copy-> graph);
  *out = std::move(dense_copy);

  return Status::OK();
}
\end{c++}
\end{leftbar}

Therefore, the \code{ClientGraph} process is constructed with the key path \code{RewriteGraphForExecution}, the pruning algorithm. The pruning algorithm traverses \ascii{FullGraph} in the inverse of the input/output list to find the smallest dependent subgraph \code{ClientGraph}.

In general, for the \code{ClientGraph} input node, the starting node is played; and the output node acts as the terminating node. So, with regard to input and output, there are two more difficult issues:

\begin{enum}
  \eitem{Input: How to pass \code{Tensor} to the input node} before the start of the \code{ClientGraph} calculation;
  \eitem{Output: When the \code{ClientGraph} is evaluated, how does the external runtime get \code{Tensor}} from the output node.
\end{enum}

There are two kinds of media: \code{FunctionCallFrame} and \code{Rendezvous}, and the external runtime and the input/output node can exchange data using one of the media.

\code{FunctionCallFrame} is used for the \code{Arg/RetVal} function call's \ascii{OP}, which is used to pass function argument values ​​when the function is called, and its return function value. However, they only apply to the single-process runtime environment.

\code{Rendezvous} is used for \code{Send/Recv} messaging \ascii{OP}, which is a more general way of communicating for distributed runtime environments.

\subsection{based on Rendezvous}

As shown by \refig{client-prune-graph}, according to the \code{fetches} list, the dependent nodes are searched backwards until \code{feeds}, and the sub-graph with the smallest dependency is calculated.

Perform pruning on the edge of \code{Feed}, such as the pruning \code{ina:0}) edge, and insert the node \code{Recv} here, and name the node by the name of the input side, for example \code {\_recv\_ina\_0}.

Similarly, pruning is also performed on the edges of \code{Fetch}, such as pruning the \code{f:0} edge, where the node \code{Send} node is inserted, and the node is named after the output edge. , for example, \code{\_send\_f\_0}.

Finally, by inserting the \code{Source/Sink} node, the subgraphs of each Unicom are summarized after pruning to form a complete \ascii{DAG} map.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/client-prune-graph.png}
\caption{Figure pruning: insert Send/Recv node}
 \label{fig:client-prune-graph}
\end{figure}

\subsection{Based on FunctionCallFrame}

However, there may be a performance bottleneck in the input/output exchange of data via \code{Rendezvous}. Because the \code{Tensor} to be sent needs to carry the sending device, the receiving device, and \code{TensorId}, together form a unique string identifier, and the data transmission and reception takes a long time to parse the string.

In particular, for local mode, there is an unnecessary performance penalty for exchanging data using \code{Rendezvous} within the same process. It can be replaced with a call based on the \code{FunctionCallFrame} function.

Therefore, in local mode, you can use \code{Arg/RetVal} instead of the \code{Send/Recv} node, which implements the way the function calls exchange data, replacing the original way of interacting with data based on \code{Rendezvous}. .

As shown in \refig{client-prune-graph-function-ops}. Pruning the edge of \code{Feed}, such as the pruning \code{ina:0}) edge, and inserting the node \code{Arg} here, and naming the node by the name of the input side, such as \code {\_arg\_ina\_0}.

Similarly, pruning is also performed on the edges of \code{Fetch}, such as pruning the \code{f:0} edge, where the node \code{RetVal} node is inserted, and the node is named after the output edge. , for example, \code{\_retval\_f\_0}.

Finally, by inserting the \code{Source/Sink} node, the subgraphs of each Unicom are summarized after pruning to form a complete \ascii{DAG} map.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/client-prune-graph-function-ops.png}
\caption{Figure pruning: insert Arg/RetVal node}
 \label{fig:client-prune-graph-function-ops}
\end{figure}

\subsection{Pruning algorithm implementation}

The pruning algorithm is mainly done by \code{RewriteGraphForExecution}, which mainly includes \ascii{3} sub-processes.

\begin{enum}
  \eitem{Additional input node}
  \eitem{Additional output node} 
  \eitem{reverse pruning}
\end{enum}

\begin{leftbar}
\begin{c++}
void RewriteGraphForExecution(Graph* g, bool use_function, 
    const ArraySlice<string>& fed_outputs,
    const ArraySlice<string>& fetch_outputs,
    const ArraySlice<string>& target_node_names,
    const DeviceAttributes& device_info) {
  FeedInputs(g, use_function, device_info, fed_outputs);

  std::vector<Node*> fetch_nodes;
  FetchOutputs(g, use_function, device_info, 
    fetch_outputs, &fetch_nodes);

  PruneForTargets(g, fetch_nodes, target_node_names);
}
\end{c++}
\end{leftbar}

\subsubsection{Add input node}

As shown in \refig{local-prune-feed}, when pruning is performed on any input edge, insert the corresponding \code{Arg} or \code{Recv} node, delete the existing edge, and reconnect the corresponding side.

In the calculation graph, an edge is uniquely identified by \code{TensorId}, which consists of a \code{op:src\_output} binary group. The former represents the upstream node of the edge, and the latter represents the edge of the upstream node.

The sample code removes some of the less important logic and relocates some of the functions, and tries partial function extraction locally to better restore the logic of the algorithm. Among them, suppose \code{Graph} can index nodes and edges according to \code{TensorId}.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/local-prune-feed.png}
\caption{Pruning: Input Edge}
 \label{fig:local-prune-feed}
\end{figure}

\begin{leftbar}
\begin{c++}
namespace {
  DataType data_type(Graph& g, const TensorId& tensor_id) {
    Node* upstream_node = g.upstream_node(tensor_id);
    return BaseType(upstream_node->output_type(tensor_id.src_output()));
  }

  Node* AppendRecvNode(Graph& g, 
    const TensorId& tensor_id, const DeviceAttributes& device_info) {
      Node * recv_node;
      NodeBuilder(strings::StrCat(
        "_recv_", tensor_id.op(), "_", tensor_id.src_output()), "_Recv")
        .Attr("tensor_type", data_type(g, tensor_id))
        .Attr("tensor_name", tensor_id.name())
        .Attr("send_device", device_info.name())
        .Attr("recv_device", device_info.name())
        .Attr("send_device_incarnation", device_info.incarnation())
        .Attr("client_terminated", true)
        .Finalize(g, &recv_node);
      return recv_node;
  }

  Node* AppendArgNode(Graph& g, size_t index, 
    const TensorId& tensor_id, const DeviceAttributes& device_info) {
    Node * arg_node;
    NodeBuilder(strings::StrCat(
      "_arg_", tensor_id.op(), "_", tensor_id.src_output()), "_Arg")
      .Attr("T", data_type(g, tensor_id))
      .Attr("index", index)
      .Finalize(g, &arg_node);
    return arg_node;
  }

  // 1. append arg/recv node
  Node* AppendNewNode(Graph& g, bool use_function, size_t index, 
    const TensorId& tensor_id，const DeviceAttributes& device_info) {
    if (use_function) {
      return AppendArgNode(g, index, tensor_id, device_info);
    } else {
      return AppendRecvNode(g, tensor_id, device_info);
    }
  }

  void AppendNewEdges(Graph& g, 
    Node* new_node, const TensorId& tensor_id) {
    // 2. add control edge between source node and new node.
    g.AddControlEdge(g.source_node(), new_node);

    Edge* old_edge = g.edge(tensor_id);
    
    // 3. add edge between new node and downstream node.
    g.AddEdge(new_node, 0, old_edge->dst(), old_edge->dst_input());
    
    // 4. remove old edge.
    g.RemoveEdge(old_edge);
  }
}

void FeedInputs(Graph& g, bool use_function,
  const DeviceAttributes& device_info,
  const ArraySlice<TensorId>& feeds) {
  for (size_t i = 0; i < feeds.size(); ++i) {
    Node* new_node = AppendNewNode(use_function, i, feeds[i]);
    AppendNewEdges(g, new_node, feeds[i]);
  }
}
\end{c++}
\end{leftbar}

\subsubsection{Add output node}

When pruning is performed on any of the output edges, insert the corresponding \code{RetVal} or \code{Send} node and connect it to the \code{Sink} node by controlling the dependent edges.

Perform pruning on the output side as shown in \refig{local-prune-fetch}. The connection relationship between the new node and the upstream node is specified by \code{Input} when constructing the new node. In addition, the function directly returns the new node (\code{RetVal/Send}) as the termination node, so there is no need to delete the original edge, and the algorithm has a subtle difference from the processing of the input edge.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/local-prune-feed.png}
\caption{Pruning: Output Edge}
 \label{fig:local-prune-fetch}
\end{figure}

\begin{leftbar}
\begin{c++}
namespace {
  Node* AppendSendNode(Graph& g, 
    const TensorId& tensor_id, const DeviceAttributes& device_info) {
    Node* send_node;
    NodeBuilder(strings::StrCat(
      "_send_", tensor_id.op(), "_", id.src_output()), "_Send")
      // 2. add edge between upstream node and send node.
      .Input(g.upstream_node(tensor_id), tensor_id.src_output())
      .Attr("tensor_name", tensor_id.name())
      .Attr("send_device", device_info.name())
      .Attr("recv_device", device_info.name())
      .Attr("send_device_incarnation",
            device_info.incarnation())
      .Attr("client_terminated", true)
      .Finalize(g, &send_node);
    return send_node;
  }

  Node* AppendRetvalNode(Graph& g, size_t index, 
    const TensorId& tensor_id, const DeviceAttributes& device_info) {
    Node* retval_node;
    NodeBuilder(strings::StrCat(
      "_retval_", tensor_id.op(), "_", tensor_id.src_output(), "_", index), 
      "_Retval")
      // 2. add edge between upstream node and retval node.
      .Input(g.upstream_node(tensor_id), tensor_id.src_output())
      .Attr("T", data_type(g, tensor_id))
      .Attr("index", index)
      .Finalize(g, &retval_node))
    return retval_node;
  }

  // 1. append retval/send node
  Node* AppendNewNode(Graph& g, bool use_function, size_t index, 
    const TensorId& tensor_id，const DeviceAttributes& device_info) {
    if (use_function) {
      return AppendRetvalNode(g, index, tensor_id, device_info);
    } else {
      return AppendSendNode(g, tensor_id, device_info);
    }
  }
}

void FetchOutputs(Graph& g, bool use_function,
  const DeviceAttributes& device_info,
  const ArraySlice<TensorId>& fetches,
  std::vector<Node*>& fetch_nodes) {
  for (size_t i = 0; i < fetches.size(); ++i) {
    Node* new_node = AppendNewNode(use_function, i, fetches[i]);
    
    // 3. add control edge between new node and sink node. 
    g->AddControlEdge(new_node, g->sink_node());

    fetch_nodes.push_back(new_node);
  }
}
\end{c++}
\end{leftbar}

\subsubsection{reverse pruning}

The pruning operation is essentially the \ascii{DAG} reverse width-first traversal algorithm. First, a queue is created with a \code{visited} array that records the nodes that have been traversed. At initialization, the queue contains only the output node and the input node (\code{targets}). When the graph is traversed, the node in \code{visited} is no longer present, indicating that the execution is not dependent on it, and the node and its associated edges should be removed from the graph.

After pruning, several \ascii{DAG} subgraphs will be formed. The node with the intrinsic degree of \code{0} is connected to the \code{Source} node by controlling the dependent edge; the node with the degree of \ascii{0} is connected to the \code{Sink} node by controlling the dependent edge. , eventually form a complete \ascii{DAG} map.

\begin{leftbar}
\begin{c++}
namespace {
  void ReverseBFS(
    Graph* g, std::unordered_set<const Node*>& visited) {
    std::deque<const Node*> queue(visited.begin(), visited.end());
    while (!queue.empty()) {
      const Node* n = queue.front();
      queue.pop_front();
      for (const Node* in : n->in_nodes()) {
        if (visited.insert(in).second) {
          queue.push_back(in);
        }
      }
    }
  }

  void RemoveUnvisitedNodes(
    Graph* g, std::unordered_set<const Node*>& visited) {
    for (Node* n : g->nodes()) {
      if (visited.count(n) == 0 && !n->IsSource() && !n->IsSink()) {
        g->RemoveNode(n);
      }
    }
  }

  void PruneForReverseReachability(
    Graph* g, std::unordered_set<const Node*>& visited) {
    ReverseBFS(g, visited);
    RemoveUnvisitedNodes(g, visited);
  }

  void FixupSourceEdges(Graph* g, Node* n) {
    if (!n->IsSource() && n->in_edges().empty()) {
      g->AddControlEdge(g->source_node(), n);
    }  
  }

  void FixupSinkEdges(Graph* g, Node* n) {
    if (!n->IsSink() && n->out_edges().empty()) {
      g->AddControlEdge(n, g->sink_node());
    }  
  }

  void FixupSourceAndSinkEdges(Graph* g) {
    for (Node* n : g->nodes()) {
      FixupSourceEdges(g, n);
      FixupSinkEdges(g, n);
    }
  }

  void AppendTargetNodes(Graph& g, 
    const ArraySlice<string>& target_names,
    std::unordered_set<const Node*>& targets) {
    for (auto name : target_names) {
      Node* target = g.GetNodeBy(name);
      targets.insert(target);
    }
  }  
}

void PruneForTargets(Graph* g, 
  std::vector<Node*>& fetch_nodes,
  const ArraySlice<string>& target_names) {
  std::unordered_set<const Node*> targets(
    begin(fetch_nodes), end(fetch_nodes));

  AppendTargetNodes(g, target_names, targets);
  PruneForReverseReachability(g, targets);
  FixupSourceAndSinkEdges(g);
}
\end{c++}
\end{leftbar}

\section{ split}
\label{sec:graph-operation-split}

As shown in \refig{local-graph-split-by-device}, if the \code{d} node is placed on \ascii{GPU0}, the other nodes are placed on \ascii{CPU0}. Among them, node \code{a} and \code{b} input data through \code{Arg}; node \code{f} outputs its result to \code{RetVal} node.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/local-graph-split-by-device.png}
\caption{Execute map split by local device set}
 \label{fig:local-graph-split-by-device}
\end{figure}

Therefore, there are several edges across the device in the calculation graph. For the edge across the device, the runtime splits it and inserts it into the \code{Send/Recv} side, which is used to send data on the original device and accept data on the target device to complete data exchange between devices. As shown in \refig{local-graph-split-insert-send-recv}.

The \code{Arg/RetVal} node exchanges data via the medium \code{FunctionCallFrame}; the \code{Send/Recv} node exchanges data via the medium \code{Rendezvous}.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/local-graph-split-insert-send-recv.png}
\caption{Insert a Send/Recv node between device OPs}
 \label{fig:local-graph-split-insert-send-recv}
\end{figure}

\subsection{Case 1}

In the simplest case, \code{src} is in the same \code{Partition} as \code{dst}. Therefore, you can directly assign it to the same \code{Partition}.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/split-graph-1.png}
\caption{Case 1: src and dst are in the same Partition}
 \label{fig:split-graph-1}
\end{figure}

\subsection{Case 2}

If \code{src} is not in the same \code{Partition} as \code{dst}, but the two are originally connected by ordinary edges. Therefore, you only need to add the \code{Send} and \code{Recv} nodes to them in two different \code{Partition}.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/split-graph-2.png}
\caption{Case 2: src and dst are not in the same Partition, but the normal edge between the two}
 \label{fig:split-graph-2}
\end{figure}

\subsection{Case 3}

If \code{src} is not in the same \code{Partition} as \code{dst}, but the two are originally connected by controlling the dependent edges.

At this point, you need to add a \code{DummyNode} of \code{Const} on the \code{src} side, and connect it as a downstream of \code{src} by controlling the dependency edges; finally, by \code{Send} Its value is sent to the peer.

On the \code{dst} side, \code{Recv} receives the value and consumes it using \code{Identity}; finally, the \code{Identity} and \code{dst} are connected to control the dependent edge.

Here, \code{Const} plays the producer and \code{Identity} plays the consumer role. It satisfies the requirements of communication between devices and the control dependency between \code{src} and \code{dst}. However, the downside is that there are subtle performance overheads.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/split-graph-3.png}
\caption{Case 3: src and dst are not in the same Partition, but between them is the control dependent edge}
 \label{fig:split-graph-3}
\end{figure}

\subsection{split algorithm implementation}

The splitting algorithm is also an algorithm that reverses the traversal map. For the currently traversed node, mark it as \code{dst}; then look for all input edges of \code{dst}; iterate over all the input edges to find the source node that is connected to the changed edge, marking it as \code {src}.

Therefore, in the three cases discussed above, iteratively implements the \code{Partition} partitioning algorithm before \code{src} and \code{dst}.

\begin{leftbar}
\begin{c++}
namespace {
  
  using Edges = std::vector<const Edge*>;
  using Partitions = std::unordered_map<string, GraphDef>;

  void AddInput(NodeDef* dst, StringPiece src_name, int src_slot) {
    if (src_slot == Graph::kControlSlot) {
      dst->add_input(strings::StrCat("^", src_name));
    } else if (src_slot == 0) {
      dst->add_input(src_name.data(), src_name.size());
    } else {
      dst->add_input(strings::StrCat(src_name, ":", src_slot));
    }
  }

  Edges InputsOf(const Node* dst) {
    Edges inputs(dst->num_inputs(), nullptr);
    for (auto edge : dst.in_edges()) {
      if (edge->IsControlEdge()) {
        inputs.push_back(e);
      } else {
        inputs[edge->dst_input()] = edge;
      }
    }
    return inputs;
  }

  NodeDef* InitDstNodeDef(const Node& dst, NodeDef* dst_def) {
    dst_def = dst.def();
    dst_def->set_device(dst.assigned_device_name());
    dst_def->clear_input();
    return dst_def;  
  }

  NodeDef* AddDummyConst(const PartitionOptions& opts, GraphDef* gdef,
                         const Edge* edge, Status* status) {
    const Node* src = edge->src();
    Tensor tensor(DT_FLOAT, TensorShape({0}));
    NodeDef* result = gdef->add_node();
    *status = NodeDefBuilder(opts.new_name(src->name()), "Const")
                  .Device(src->assigned_device_name())
                  .Attr("dtype", DT_FLOAT)
                  .Attr("value", tensor)
                  .Finalize(result);
    return result;
  }

  NodeDefBuilder::NodeOut BuildSendFrom(
      const PartitionOptions& opts,
      GraphDef* src_graph,
      const Edge* edge,
      NodeDefBuilder::NodeOut& send_from) {
    if (edge->IsControlEdge()) {
      // Case 3: DummyNode(Const) -ctrl-> src -> send  
      NodeDef* dummy = AddDummyConst(opts, src_graph, edge);
      AddInput(dummy, edge->src()->name(), Graph::kControlSlot);
      send_from.Reset(dummy->name(), 0, DT_FLOAT);
    } else {
      // Case 2: src -> send  
      send_from.Reset(edge->src()->name(),
                      edge->src_output(), 
                      EdgeType(edge));
    }
  }

  void SetSendRecvAttrs (
      const PartitionOptions& opts, 
      const Edge* edge,
      NodeDefBuilder* builder) {
    builder->Attr("tensor_name",
                  strings::StrCat("edge_", edge->id(), "_", edge->src()->name()));
    builder->Attr("send_device", edge->src()->assigned_device_name());
    builder->Attr("send_device_incarnation",
                  static_cast<int64>(
                      opts.get_incarnation(edge->src()->assigned_device_name())));
    builder->Attr("recv_device", edge->dst()->assigned_device_name());
    builder->Attr("client_terminated", false);
  }

  NodeDef* AddSend(
      const PartitionOptions& opts, 
      GraphDef* gdef, 
      const Edge* edge,
      NodeDefBuilder::NodeOut send_from) {
    NodeDef* send = gdef->add_node();
    NodeDefBuilder builder(opts.new_name(edge->src()->name()), "_Send");
    SetSendRecvAttrs (opts, edge, & builder);
    builder.Device(edge->src()->assigned_device_name())
           .Input(send_from)
           .Finalize(send);
    return send;
  }

  NodeDef* AddRecv(const PartitionOptions& opts, const GraphInfo& g_info,
                   GraphDef* gdef, const Edge* edge, NodeDef** real_recv,
                   Status* status) {
    NodeDef* recv = gdef->add_node();
    NodeDefBuilder builder(opts.new_name(src->name()), "_Recv");
    SetSendRecvAttrs (opts, edge, & builder);
    builder.Device(dst->assigned_device_name())
           .Attr("tensor_type", EdgeType(edge))
           .Finalize(recv);
    return recv;

    if (edge->IsControlEdge()) {
      // Case 3: Recv -> Identity -contrl-> dst
      NodeDef* id = gdef->add_node();
      NodeDefBuilder(opts.new_name(src->name()), "Identity")
          .Device(dst->assigned_device_name())
          .Input(recv->name(), 0, cast_dtype)
          .Finalize(id);
      return id;
    } else {
      return recv;
    }
  }

  void InsertSendRecv(
      const PartitionOptions& opts,
      GraphDef* src_graph, 
      Edge* edge, 
      GraphDef* dst_graph, 
      NodeDef * dst_def) {
    NodeDefBuilder::NodeOut send_from;
    BuildSendFrom(opts, src_graph, edge, send_from);

    NodeDef* send = AddSend(opts, src_graph, edge, send_from);
    NodeDef* recv = AddRecv(opts, dst_graph, edge);

    if (edge->IsControlEdge()) {
      // Case 3: In fact, recv is identity.
      AddInput(dst_def, recv->name(), Graph::kControlSlot);
    } else {
      AddInput(dst_def, recv->name(), 0);
    }
  }
}

Status Partition(const PartitionOptions& opts, 
                 Partitions& partitions, Graph& client_graph) {
  for (const Node* dst : client_graph.op_nodes()) {
    // 1. find dst node
    GraphDef* dst_graph = &partitions[opts.node_to_loc(dst)];
    NodeDef* dst_def = InitDstNodeDef(*dst, dst_graph->add_node());
    
    // 2. search all input edges.
    for (const Edge* edge : InputsOf(dst)) {
      // 3. find src node: edge->src()
      GraphDef* src_graph = &partitions[opts.node_to_loc(src)];

      // skip sink/source nodes.
      if (!edge->src()->IsOp()) 
        continue;  

      // Case 1: same partition
      if (src_graph == dst_graph) {
        AddInput(dst_def, src->name(), edge->src_output());
        continue;
      }

      // Case 2-3: different partition
      InsertSendRecv(opts, src_graph, edge, dst_graph, dst_def);
    }
  }
}
\end{c++}
\end{leftbar}

\subsection{callback function}

There are two important callback functions in \code{PartitionOptions}. \code{NodeToLocFunc} is used for graph splitting; \code{NewNameFunc} is used to name newly added nodes, such as \code{Send/Recv}.

\begin{leftbar}
\begin{c++}
struct PartitionOptions {
  typedef std::function<string(const Node*)> NodeToLocFunc;
  NodeToLocFunc node_to_loc = nullptr;

  typedef std::function<string(const string&)> NewNameFunc;
  NewNameFunc new_name = nullptr;

  // ignore others...
};
\end{c++}
\end{leftbar}

For graph splitting, there are two basic methods of splitting.

\begin{leftbar}
\begin{c++}
string SplitByDevice(const Node* node) {
  return node->assigned_device_name();
}

string SplitByWorker(const Node* node) {
  string task, device;
  DeviceNameUtils::SplitDeviceName(
      node->assigned_device_name(), &task, &device);
  return task;
}
\end{c++}
\end{leftbar}

In local mode, \code{NodeToLocFunc} is configured as \code{SplitByDevice}. As shown in \code{intraprocess-splity-by-device}.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/intraprocess-splity-by-device.png}
\caption{Local Mode: SplitByDevice}
 \label{fig:intraprocess-splity-by-device}
\end{figure}


In distributed mode, \code{NodeToLocFunc} of \code{Master} is configured as \code{SplitByWorker}; and \code{Worker}
\code{NodeToLocFunc} is configured as \code{SplitByDevice}.

Therefore, in distributed mode, the graph split undergoes two levels of separation. The first level is split according to \code{SplitByWorker}, and the picture is divided into various \code{Worker}; the second level is based on \code{SplitByDevice}, and then the picture is divided into various computing devices.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/interprocess-splity-by-worker.png}
\caption{Distributed mode: two-level split}
 \label{fig:interprocess-splity-by-worker}
\end{figure}

\section{execution}
\label{sec:graph-operation-exec}

Next, the runtime will execute each \code{PartitionGraph} concurrently. As shown in \refig{local-graph-execution}, each \code{PartitionGraph} starts a \code{Executor} to implement the calculation of the concurrent execution graph.

Each \code{Executor} will execute the topological sorting algorithm of \code{PartitionGraph}, append \ascii{OP} with degree \ascii{0} to \code{ready\_queue} and associate it with The degree of \ascii{OP} is reduced by \ascii{1}. Scheduler dispatch \code{ready\_queue}\ascii{OP
}, and put it into \code{ThreadPool} to execute the corresponding \ascii{Kernel} implementation.

Before all \code{Partition} starts concurrent execution, you need to pass its input to the corresponding \code{Arg} node externally; when all \code{Partition} finishes the calculation, the external is taken from the \code{RetVal} node. Take the data. Among them, the data between the \code{Arg/RetVal} nodes is interactive through \code{FunctionCallFrame}.

If \code{PartitionGraph} needs to exchange data across devices, the producer places it in the \code{Send} node, and the consumer gets the data through the \code{Recv} node. The sender does not block; if the receiver does not arrive, the blocking occurs until timeout. In addition, the data between the \code{Send/Recv} nodes is interactive through \code{Rendezvous}.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/local-graph-execution.png}
\caption{Execution map}
 \label{fig:local-graph-execution}
\end{figure}

Therefore, the implementation of graph calculation needs to solve the following \ascii{3} core issues:

\begin{enum}
  \eitem{input/output processing}
  \eitem{Inter-device data exchange} 
  \eitem{execute\code{PartitionGraph}}
\end{enum}

\subsection{input}

On a device, the starting node of \code{PartitionGraph} is the \code{Arg} node and the ending node is the \code{RetVal} node. The whole process can be seen as a function call procedure, \code{Arg} is used to pass function arguments, and \code{RetVal} is used to return function values.

More precisely, \code{Arg} completes the input of \code{PartitionGraph}, and \code{RetVal} completes
The output of \code{PartitionGraph}. For the \code{Arg} node, the calling sequence is: \code{set\_arg -> get\_arg}. Among them, the former is passed by \code{DirectSession} before calling the \code{Executor} list, passing the value of the input parameter list by calling \code{FunctionCallFrame.SetArgs(feeds)}; the latter is \ascii by \code{Arg} {Kernel} implementation call.

\begin{leftbar}
\begin{c++}
Status DirectSession::Run(
  const RunOptions& run_options,
  const NamedTensorList& inputs,
  const std::vector<string>& output_names,
  const std::vector<string>& target_nodes,
  std::vector<Tensor>* outputs,
  RunMetadata* run_metadata) {

  // 1. prune graph
  // client\_graph = prune(full\_graph, inputs, outputs)
   
  // 2. split graph into partition by devices 
  // executors\_and\_partitions = split(client\_graph, devices)
  ExecutorsAndKeys* executors_and_keys = ... // ignore implements...
  
  // 3. lauch executor per partition
  // def run\_partitions(executors\_and\_partitions, inputs, outputs):
  // \ \ frame = FunctionCallFrame()
  // \ \ frame.set\_args(inputs)
  // \ \ for (executor, partition) in executors\_and\_partitions: 
  // \ \ \ \ exec.run(part)
  // \ \ frame.get\_ret\_vals(outputs)

  // 3.1 construct FunctionCallFrame
  FunctionCallFrame call_frame(
    executors_and_keys->input_types,
    executors_and_keys->output_types);
  
  // 3.2 frame.set\_args(inputs)
  // 3.2.1 construct feeds list
  gtl::InlinedVector<Tensor, 4> feed_args(inputs.size());
  for (const auto& it : inputs) {
    // (first, second) => (tensor\_name, tensor)
    feed_args[executors_and_keys->input_name_to_index[it.first]] = it.second;
  }

  // 3.2.2 frame.set\_args(feeds)
  call_frame.SetArgs(feed_args);
  
  // 3.3 execution competitor
  // for (executor, partition) in executors\_and\_partitions:
  // \ \ executor.run(partition) 

  // 3.4 fetch outputs.
}
\end{c++}
\end{leftbar}

\code{frame.get\_arg} has \code{Arg} to get it, and \code{Arg} outputs it to the first compute node in \code{PartitionGraph}.

\begin{leftbar}
\begin{c++}
struct ArgOp : OpKernel {
  explicit ArgOp(OpKernelConstruction* ctx) : OpKernel(ctx) {
    ctx->GetAttr("T", &dtype_);
    ctx->GetAttr("index", &index_);
  }

  void Compute(OpKernelContext* ctx) override {
    auto frame = ctx->call_frame();

    Tensor val;
    frame->GetArg(index_, &val);

    // put it into downsteram op's input.
    ctx->set_output(0, val); 
  }

 private:
  int index_;
  DataType dtype_;
};
\end{c++}
\end{leftbar}

\subsection{Concurrent execution}

After the graph is split, the run starts a \code{Executor} for each \code{Partition}. In order to monitor whether all \code{Executor} is completed, a \code{ExecutorBarrier} is created. And after starting all \code{Executor}, call \code{executors\_done.Wait()} to block and wait for all \code{Executor} to complete execution.

When done in a \code{Executor}, the \code{ExecutorBarrier} calculator minus \ascii{1} (initial value is \code{num\_executors}) until it is \ascii{0}, it will be called to complete The hook finally triggers \code{executors\_done.Notify()}.

\begin{leftbar}
\begin{c++}
Status DirectSession::Run(
  const RunOptions& run_options,
  const NamedTensorList& inputs,
  const std::vector<string>& output_names,
  const std::vector<string>& target_nodes,
  std::vector<Tensor>* outputs,
  RunMetadata* run_metadata) {

  // 1. prune graph
  // client\_graph = prune(full\_graph, inputs, outputs)
   
  // 2. split graph into partition by devices 
  // executors\_and\_partitions = split(client\_graph, devices)
  ExecutorsAndKeys* executors_and_keys = ... // ignore implements...
  
  // 3. lauch executor per partition
  // def run\_partitions(executors\_and\_partitions, inputs, outputs):
  // \ \ frame = FunctionCallFrame()
  // \ \ frame.set\_args(inputs)
  // \ \ for (executor, partition) in executors\_and\_partitions: 
  // \ \ \ \ exec.run(part)
  // \ \ frame.get\_ret\_vals(outputs)

  // 3.1 construct FunctionCallFrame
  FunctionCallFrame call_frame(
    executors_and_keys->input_types,
    executors_and_keys->output_types);
  
  // 3.2 frame.set\_args(inputs)
  // 3.2.1 construct feeds list
  gtl::InlinedVector<Tensor, 4> feed_args(inputs.size());
  for (const auto& it : inputs) {
    // (first, second) => (tensor\_name, tensor)
    feed_args[executors_and_keys->input_name_to_index[it.first]] = it.second;
  }

  // 3.2.2 frame.set\_args(feeds)
  call_frame.SetArgs(feed_args);
  
  // 3.3 execution competitor
  // barrier = ExecutorBarrier(executors\_and\_partitions.size())
  // for (executor, partition) in executors\_and\_partitions:
  // \ \ executor.run(partition) 
  // barrier.wait()
  RunState run_state(&devices_);
  run_state.rendez = new IntraProcessRendezvous(device_mgr_.get());
  
  // 3.3.1 notify when finished.
  size_t num_executors = executors_and_keys->items.size();
  ExecutorBarrier* barrier = new ExecutorBarrier(
      num_executors, run_state.rendez, [&run_state](const Status& ret) {
        {
          mutex_lock l(run_state.mu_);
          run_state.status.Update(ret);
        }
        run_state.executors_done.Notify();
      });

  Executor::Args args;
  args.call_frame = &call_frame;
  args.rendezvous = run_state.rendez;
  args.runner = [this, pool](Executor::Args::Closure c) {
    SchedClosure(pool, std::move(c));
  };

  // 3.3.2 lauch all executors.
  for (const auto& item : executors_and_keys->items) {
    item.executor->RunAsync(args, barrier->Get());
  }

  // 3.3.3 wait until all executors finished.
  WaitForNotification(&run_state, 
      &step_cancellation_manager,
      GetTimeoutInMs(run_options));

  // 3.4 fetch outputs.
}
\end{c++}
\end{leftbar}

\subsection{output}

Similarly, for the \code{RetVal} node, the calling sequence is: \code{set\_ret\_val -> get\_ret\_val}. The former is done by \code{RetVal} and the latter by \code{DirectSession}.

\begin{leftbar}
\begin{c++}
struct RetvalOp : OpKernel {
  explicit RetvalOp(OpKernelConstruction* ctx) : OpKernel(ctx) {
    ctx->GetAttr("T", &dtype_);
    ctx->GetAttr("index", &index_);
  }

  void Compute(OpKernelContext* ctx) override {
    // get upstream op's output.
    const Tensor& val = ctx->input(0); 

    auto frame = ctx->call_frame();
    frame->SetRetval(index_, val);
  }

 private:
  int index_;
  DataType dtype_;
};
\end{c++}
\end{leftbar}

After all \code{Executor} has finished running, \code{DirectSession} can take all output values ​​from \code{FunctionCallFrame} and place them in \code{outputs} and return \ascii{Client}.

\begin{leftbar}
\begin{c++}
Status DirectSession::Run(
  const RunOptions& run_options,
  const NamedTensorList& inputs,
  const std::vector<string>& output_names,
  const std::vector<string>& target_nodes,
  std::vector<Tensor>* outputs,
  RunMetadata* run_metadata) {
  
  // 1. prune graph
  // client\_graph = prune(full\_graph, inputs, outputs)
   
  // 2. split graph into partition by devices 
  // executors\_and\_partitions = split(client\_graph, devices)
  executors_and_keys = ... // ignore implements...
  
  // 3. lauch executor per partition
  // def run\_partitions(executors\_and\_partitions, inputs, outputs):
  // \ \ frame = FunctionCallFrame()
  // \ \ frame.set\_args(inputs)
  // \ \ for (executor, partition) in executors\_and\_partitions: 
  // \ \ \ \ exec.run(part)
  // \ \ frame.get\_ret\_vals(outputs)

  // 3.1 construct FunctionCallFrame
  FunctionCallFrame call_frame(
    executors_and_keys->input_types,
    executors_and_keys->output_types);
  
  // 3.2 frame.set\_args(inputs)
  // 3.2.1 construct feeds list
  gtl::InlinedVector<Tensor, 4> feed_args(inputs.size());
  for (const auto& it : inputs) {
    // (first, second) => (tensor\_name, tensor)
    feed_args[executors_and_keys->input_name_to_index[it.first]] = it.second;
  }

  // 3.2.2 frame.set\_args(feeds)
  call_frame.SetArgs(feed_args);
  
  // 3.3 execution competitor
  // barrier = ExecutorBarrier(executors\_and\_partitions.size())
  // for (executor, partition) in executors\_and\_partitions:
  // \ \ executor.run(partition) 
  // barrier.wait()
  RunState run_state(&devices_);
  run_state.rendez = new IntraProcessRendezvous(device_mgr_.get());
  
  // 3.3.1 notify when finished.
  size_t num_executors = executors_and_keys->items.size();
  ExecutorBarrier* barrier = new ExecutorBarrier(
      num_executors, run_state.rendez, [&run_state](const Status& ret) {
        {
          mutex_lock l(run_state.mu_);
          run_state.status.Update(ret);
        }
        run_state.executors_done.Notify();
      });

  Executor::Args args;
  args.call_frame = &call_frame;
  args.rendezvous = run_state.rendez;
  args.runner = [this, pool](Executor::Args::Closure c) {
    SchedClosure(pool, std::move(c));
  };

  // 3.3.2 lauch all executors.
  for (const auto& item : executors_and_keys->items) {
    item.executor->RunAsync(args, barrier->Get());
  }

  // 3.3.3 wait until all executors finished.
  WaitForNotification(&run_state, 
      &step_cancellation_manager,
      GetTimeoutInMs(run_options)); 

  // 3.4 fetch outputs. 
  // 3.4.1 frame.get\_get\_ret\_vals
  std::vector<Tensor> sorted_outputs;
  Status s = call_frame.ConsumeRetvals(&sorted_outputs);

  // 3.4.2 emplace to outputs, and return to client.
  outputs->reserve(sorted_outputs.size());
  for (int i = 0; i < output_names.size(); ++i) {
    const string& output_name = output_names[i];
    outputs->emplace_back(
      std::move(sorted_outputs[
        executors_and_keys->output_name_to_index[output_name]]));
  }
}
\end{c++}
\end{leftbar}

At this point, the entire \code{DirectSession.Run} has been interpreted. However, how does the \code{Send/Recv} between \code{Partition} work in the \code{Partition} how the nodes are scheduled to execute?

Therefore, in the last mile, we must explore three things.

\begin{enum}
  How \eitem{\code{SendOp} works with \code{RecvOp}}
  How \eitem{\code{IntraProcessRendezvous} works} 
  \eitem{\code{Executor} scheduling algorithm}
\end{enum}

\section{Inter-device communication}

\code{SendOp/RecvOp} exchanges data via \code{Rendezvous}; it implements message delivery/acceptance and decouples from specific messaging. For example, in a single process, \code{SendOp/RecvOp} passes data based on \code{IntraProcessRendezvous}; in a multi-process environment, \code{SendOp/RecvOp} can pass data based on \code{GrpcRendezvous}.

First, explore the workings of these two \ascii{OP}; then, explore the working principle of \code{IntraProcessRendezvous} in local mode.

\subsection{SendOp implementation}

As shown in \refig{local-send-recv-ops}, the in-process \code{Send/Recv} exchanges data with a unique identifier \code{ParsedKey}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/local-send-recv-ops.png}
\caption{Data exchange between \code{SendOp} and \code{RecvOp}}
 \label{fig:local-send-recv-ops}
\end{figure}

Referring to the \ascii{Kernel} implementation of \code{SendOp}, it looks very complicated, but it actually does one thing. First, it constructs the keyword \code{ParsedKey} for communication between devices, then calls the \code{Rendezvous.Send} operation to send the upstream \ascii{OP} to the \code{Tensor} of \code{SendOp} to \ In the code{Rendezvous} cache, this operation is non-blocking.

Among them, \code{ParsedKey} includes: the sending device, the receiving device, the device global identifier, and the identifier of the \code{Tensor} to be sent (\code{src:output\_index}).

\begin{leftbar}
\begin{c++}
struct SendOp : OpKernel {
  explicit SendOp(OpKernelConstruction* ctx) : OpKernel(ctx) {
    string send_device;
    ctx->GetAttr("send_device", &send_device);

    string recv_device;
    ctx->GetAttr("recv_device", &recv_device);

    uint64 send_device_incarnation;
    ctx->GetAttr(
        "send_device_incarnation",
        reinterpret_cast<int64*>(&send_device_incarnation));

    string tensor_name;
    ctx->GetAttr("tensor_name", &tensor_name);

    key_prefix_ = GetRendezvousKeyPrefix (
        send_device, recv_device,
        send_device_incarnation, tensor_name);

    GetRendezvousKey(key_prefix_, {0, 0}, &parsed_key_.buf_);
    Rendezvous::ParseKey(parsed_key_.buf_, &parsed_key_);

    if (!ctx->GetAttr("_hostmem_sendrecv", &hostmem_sendrecv_).ok()) {
      hostmem_sendrecv_ = false;
    }
  }

  void Compute(OpKernelContext* ctx) override {
    Rendezvous::Args args;
    args.device_context = ctx->op_device_context();
    args.alloc_attrs = ctx->input_alloc_attr(0);
    
    // get it from upstream op's output, and as this op's input.
    ctx->rendezvous()->Send(
        CreateParsedkey(ctx), args, ctx->input(0),
        ctx->is_input_dead());
  }
 
 private:
  Rendezvous::ParsedKey CreateParsedkey(OpKernelContext* ctx) {
    FrameAndIter frame_iter = GetFrameAndIter (ctx, hostmem_sendrecv_);
    if (frame_iter == FrameAndIter(0, 0)) {
      return parsed_key_;
    } else {
      Rendezvous::ParsedKey in_loop_parsed;
      GetRendezvousKey(key_prefix_, frame_iter, &in_loop_parsed.buf_);
      Rendezvous::ParseKey(in_loop_parsed.buf_, &in_loop_parsed);
      return in_loop_parsed;
    }  
  }

 private:
  string key_prefix_;
  Rendezvous::ParsedKey parsed_key_;
  bool hostmem_sendrecv_;
};
\end{c++}
\end{leftbar}

\subsection{RecvOp implementation}

Similarly, you can guess the implementation of \ascii{Kernel} for \code{Recv}. It first constructs the \code{ParsedKey} of \code{Rendezvous}, then calls the \code{Rendezvous.RecvAsync} operation and fetches the corresponding \code{Tensor} from \code{Rendezvous}.

This is an asynchronous operation. When the data in \code{Rendezvous} is available, it starts executing the callback function \code{done\_cb}, which outputs the resulting \code{Tensor} to the downstream \ascii{OP}.

\begin{leftbar}
\begin{c++}
struct RecvOp : AsyncOpKernel {
  explicit RecvOp(OpKernelConstruction* ctx) : AsyncOpKernel(ctx) {
    string send_device;
    ctx->GetAttr("send_device", &send_device);
  
    string recv_device;
    ctx->GetAttr("recv_device", &recv_device);

    uint64 send_device_incarnation;
    ctx->GetAttr(
        "send_device_incarnation",
        reinterpret_cast<int64*>(&send_device_incarnation));
  
    string tensor_name;
    ctx->GetAttr("tensor_name", &tensor_name);

    key_prefix_ = GetRendezvousKeyPrefix (
        send_device, recv_device,
        send_device_incarnation, tensor_name);
  
    GetRendezvousKey(key_prefix_, {0, 0}, &parsed_key_.buf_);
    Rendezvous::ParseKey(parsed_key_.buf_, &parsed_key_));
    if (!ctx->GetAttr("_hostmem_sendrecv", &hostmem_sendrecv_).ok()) {
      hostmem_sendrecv_ = false;
    }
  }

  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {
    Rendezvous::Args args;
    args.device_context = ctx->op_device_context();
    args.alloc_attrs = ctx->output_alloc_attr(0);

    ctx->rendezvous()->RecvAsync(
      CreateParsedKey(ctx), args, CreateDoneCallback(ctx));
  }

 private:
  Rendezvous::ParsedKey CreateParsedKey(OpKernelContext* ctx) {
    FrameAndIter frame_iter = GetFrameAndIter (ctx, hostmem_sendrecv_);
    if (frame_iter == FrameAndIter(0, 0)) {
      return parsed_key_;
    } else {
      Rendezvous::ParsedKey in_loop_parsed;
      GetRendezvousKey(key_prefix_, frame_iter, &in_loop_parsed.buf_);
      Rendezvous::ParseKey(in_loop_parsed.buf_, &in_loop_parsed);
      return in_loop_parsed;
    }  
  }

  Rendezvous::DoneCallback CreateDoneCallback(OpKernelContext* ctx) {
    using namespace std::placeholders;
    return std::bind([ctx](DoneCallback done, const Status& s, 
        const Rendezvous::Args&, const Rendezvous::Args&, 
        const Tensor& val, bool is_dead) {
          ctx->SetStatus(s);
          if (s.ok()) {
            if (!is_dead) {
              // put it into downstream op's input.
              ctx->set_output(0, val);  
            }
            *ctx->is_output_dead() = is_dead;
          }
          done();
        },
        std::move(done), _1, _2, _3, _4, _5);  
  }

 private:
  string key_prefix_;
  Rendezvous::ParsedKey parsed_key_;
  bool hostmem_sendrecv_;
};
\end{c++}
\end{leftbar}