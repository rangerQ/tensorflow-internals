\begin{savequote}[45mm]
\ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
\qauthor{\ascii{- Martin Flower}}
\end{savequote}

\chapter{linear model} 
\label{ch:linear-model}

\section{logical regression}

\begin{content}

In logistic regression (\ascii{Logistic Regression}), $y$ is a real value, and $y \in \{ 0,1\}$. However, logistic regression solves the problem of two classifications. Generally, if $y \ge 0.5$, it is judged to be a positive class; otherwise, it is judged to be a negative class.

\[c = \left\{ \begin{gathered}
  1,y \ge 0.5 \hfill \\
  0,y < 0.5 \hfill \\ 
\end{gathered} \right.\]

In addition, the above expression is often expressed using \emph{instruction function}.

\[
c = \mathbb I(y \ge 0.5)
\]

Where $\mathbb I(true) = 1$; otherwise $\mathbb I(false) = 0$.

\subsection{symbol definition}

To formalize the problem of logistic regression, some common symbols are defined here. It should be noted that in order to distinguish between the predicted value and the sign of the tag value, they are represented by $y, t$ respectively.

 \begin{itemize}
   \item \ascii{training sample set}: $ S = \{ ({x^{(i)}},{t^{(i)}});i = 1,2,...,m\} $
   \item \ascii{第$i$ training sample}: $ ({x^{(i)}},{t^{(i)}}) $
   \item \ascii{sample input}: $ x = ({x_1},{x_2},...,{x_n})^{T} \in {\mathbb{R}^n} $
   \item \ascii{sample tag}: $ t \in {\mathbb{R}} $
   \item \ascii{predicted value}: $ y \in {\mathbb{R}} $   
   \item \ascii{error value}: $ e = y - t \in {\mathbb{R}} $
   \item \ascii{weight}: $ w \in {\mathbb{R}^{n}} $   
   \item \ascii{offset}: $ b \in {\mathbb{R}} $
   \item \ascii{linear weighted sum}: $ z = w^Tx + b \in {\mathbb{R}} $   
   \item \ascii{activation function}: $ f(z) = \frac{1}{{1 + {e^{ - z}}}} \in {[0, 1]} $
   \item \ascii{regular item}: $ R(w) \in {\mathbb{R}} $
   \item \ascii{L2 regular item}: $ \parallel w\parallel _2^2 \in {\mathbb{R}} $
   \item \ascii{loss function (no regular items)}: $ L(w, b) \in {\mathbb{R}} $
   \item \ascii{loss function (with regular items)}: $ J(w, b) = L(w, b) + \lambda R(w)\in {\mathbb{R}} $
   \item \ascii{gradient function}: $ {\nabla _w}L( {w,b}) \in {\mathbb{R}^n} $
 \end{itemize}

\subsection{model definition}

As shown by \refig{logistic-regression-nn}, logistic regression is equivalent to a network model with only one neuron.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/logistic-regression-nn.png}
\caption{logical regression: neuron model}
 \label{fig:logistic-regression-nn}
\end{figure}

First, the logistic regression completes the linear weighted sum of $z = {w^T}x + b$; then, using the activation function of \ascii{sigmoid}, the nonlinear transformation is done.

\[\begin{aligned}
  y = & {h_{w,b}}(x) \\ 
   = & f({w^T}x + b) \\ 
   = & \frac{1}{{1 + {e^{ - {w^T}x + b}}}} \\ 
\end{aligned} \]

$w$ represents the weight of the model, which is a vector of $n$ dimensions; $b$ represents the bias of the neuron, which is a scalar.

\[\begin{gathered}
  w = \left[ {\begin{array}{*{20}{c}}
  {{w_1}} \\ 
  {{w_2}} \\ 
   \vdots \\ 
  {{w_n}} 
\end{array}} \right] \in {\mathbb{R}^n} \\ 
  b \in \mathbb{R} \\ 
\end{gathered} \]

$x$ represents the input to the model, which is a vector of $n$ dimensions; where $\forall {x_i} \in x,i = 1,2,...,n$, which represents the input vector $x$ An eigenvalue; $y$ represents the output of the model, which is a scalar.

\[\begin{gathered}
  x = \left[ {\begin{array}{*{20}{c}}
  {{x_1}} \\ 
  {{x_2}} \\ 
   \vdots \\ 
  {{x_n}} 
\end{array}} \right] \in {\mathbb{R}^n} \\ 
  {\text{y}} \in {\mathbb{R}} \\ 
\end{gathered}\]

$f(z)$ is the activation function of the neuron, using the \ascii{sigmoid} function here.

\subsection{Sigmoid function}

The \ascii{sigmoid} function compresses the infinite domain space into the range space of $[0, 1]$, so it has a natural probability interpretation.

\[
f(z) = \frac{1}{{1 + {e^{ - z}}}}
\]


\subsubsection{soft saturation}

As shown by \refig{sigmoid}, the \ascii{sigmoid} function has soft saturation, ie

\[\begin{gathered}
  \mathop {\lim }\limits_{z \to + \infty } f(z) = 1 \hfill \\
  \mathop {\lim }\limits_{z \to - \infty } f(z) = 0 \hfill \\ 
\end{gathered} \]

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/sigmoid.png}
\caption{sigmoidfunction}
 \label{fig:sigmoid}
\end{figure}

\subsubsection{derivative}

The derivative of \ascii{sigmoid} has a special form, and its derivative function is a quadratic function of $y$. make

\[
y = \frac{1}{{1 + {e^{ - z}}}}
\]

The derivative of \ascii{sigmoid} can be easily derived from the chain-based derivation rule.

\[\begin{aligned}
  y' = & \frac{d}{{dz}}\left( {\frac{1}{{1 + {e^{ - z}}}}} \right) \\ 
   = & \frac{d}{{dz}}{\left( {1 + {e^{ - z}}} \right)^{ - 1}} \\ 
   = & - {\left( {1 + {e^{ - z}}} \right)^{ - 2}}\frac{d}{{dz}}\left( {{e^{ - z}} } \right) \\ 
   = & - {\left( {1 + {e^{ - z}}} \right)^{ - 2}}{e^{ - z}}\frac{d}{{dz}}\left( { - z} \right) \\ 
   = & - {\left( {1 + {e^{ - z}}} \right)^{ - 2}}{e^{ - z}}\left( { - 1} \right) \\ 
   = & \frac{{{e^{ - z}}}}{{{{\left( {1 + {e^{ - z}}} \right)}^2}}} \\ 
   = & \frac{1}{{1 + {e^{ - z}}}}\left( {1 - \frac{1}{{1 + {e^{ - z}}}}} \right) \\ 
   = & y(1 - y) \\ 
\end{aligned} \]

\subsection{probability explanation}

For any $(x,t) \in S$, according to the functional properties of \ascii{Sigmoid}, assume that $t|x$ obeys the probability distribution of $Bernoulli(y)$.

\[\begin{aligned}
  P(t = 1|x;w,b) = & y \\ 
  P(t = 0|x;w,b) = & 1 - y \\ 
\end{aligned} \]

As we all know, the $Bernoulli(y)$ probability distribution can be described as a more concise expression.

\[P(t|x;w,b) = {y^t}{(1 - y)^{1 - t}}\]

By extension, if there are $m$ independent and identically distributed training samples $\{ ({x^{(i)}}, {t^{(i)}}); i = 1,2,.. .,m\}$, the likelihood function with the parameter $(w,b)$ can be expressed as:

\[\begin{aligned}
  L(w,b) = & P\left( {\vec y|X} \right) \\ 
   = & \prod\limits_{i = 1}^m {P\left( {{t^{(i)}}|{x^{(i)}};w,b} \right)} \\ 
   = & \prod\limits_{i = 1}^m {{{\left( {{y^{(i)}}} \right)}^{{t^{(i)}}}}{{\ Left( {1 - {y^{(i)}}} \right)}^{1 - {t^{(i)}}}}} \\ 
\end{aligned} \]

among them,
${y^{(i)}} = {h_{w,b}}({x^{(i)}}), i=1,2,...,m $. Generally, according to the monotonous increment of the logarithmic function, it is transformed into a log likelihood function, so that the multiplication operation is converted into a continuous addition operation, which simplifies the complexity of the problem.

\[l(w,b) = \log L(w,b) = \sum\limits_{i = 1}^m {{t^{(i)}}\log {y^{(i)}} + \left( {1 - {t^{(i)}}} \right)\log \left( {1 - {y^{(i)}}} \right)} \]

Therefore, the logistic regression problem can be transformed into a parameter estimation problem that maximizes the log likelihood function.

\[\hat w,\hat b = \arg \mathop {\max }\limits_{w,b} \sum\limits_{i = 1}^m {{t^{(i)}}\log {y ^{(i)}} + \left( {1 - {t^{(i)}}} \right)\log \left( {1 - {y^{(i)}}} \right)} \ ]

You can use the gradient-rising optimization method to iteratively find the optimal parameter $(\hat w,\hat b)$.

\subsection{cross entropy loss function}

In the field of machine learning, \emph{loss function} is often used, and the optimization algorithm of \emph{gradient drop} is used to iteratively find the optimal parameter $(\hat w,\hat b)$. According to the derivation of the previous section, given a training sample $(x, t)$, logistic regression can use the loss function defined below, which is a special representation of the cross-entropy loss function under the two-class problem.

\[L(w,b;x,t) = - \left\{ {t\log y + \left( {1 - t} \right)\log \left( {1 - y} \right)} \ Right\}\]

By extension, given training data set $ S = \{ ({x^{(i)}}, {t^{(i)}});i = 1,2,...,m\} $, the loss function of logistic regression can be expressed as:

\[L(w,b; S) = - \sum\limits_{i = 1}^m {{t^{(i)}}\log {y^{(i)}} + \left( {1 - {t^{(i)}}} \right)\log \left( {1 - {y^{(i)}}} \right)} \]

Therefore, the learning problem of logistic regression is transformed into an optimization problem that minimizes the loss function $L(w,b)$.

\[\hat w,\hat b = \arg \mathop {\min }\limits_{w,b} L(w, b)\]

\subsection{regular item}

To control the complexity of the model, you can increase the regular term of $L2$, which is equal to the inner product of the vector $w$.

\[R(w) = \lambda \parallel w\parallel _2^2 = {w^T}w \]

Among them, $\lambda$ is called the regular term factor, which is used to control the degree of influence of the regular term; it is the hyperparameter of the model, and the best value can be obtained through the cross-validation test. After adding the regular term, the model's loss function can be expressed as:

\[J(w,b) = L(w,b) + \lambda \parallel w\parallel _2^2\]

The optimization problem is also expressed as:

\[\hat w,\hat b = \arg \mathop {\min }\limits_{w,b} J(w, b)\]

\subsection{gradient drop}

You can use the gradient descent algorithm to iteratively find the optimal $(\hat w,\hat b)$. Where $\alpha$ represents the learning rate, which represents the size of the parameter update.

\[\begin{aligned}
  w \leftarrow & w - \alpha {\nabla _w}L(w,b) \\ 
  b \leftarrow & b - \alpha {\nabla _b}L(w,b) \\ 
\end{aligned} \]

Among them, the gradients of $w and b$ are defined as:

\[\begin{aligned}
  {\nabla _w}L(w,b) = & \frac{{\partial L}}{{\partial w}} \\ 
  {\nabla _b}L(w,b) = & \frac{{\partial L}}{{\partial b}} \\ 
\end{aligned} \]

As shown by \refig{mnist-gd}, the loss function can be likened to a mountain, and the climber tries to find the best course of action to reach the valley. The climber stood on a hillside and decided to take a small step down the opposite direction of the gradient until a better solution was obtained. When the gradient descent update algorithm is implemented, the initial points are different and the minimum values ​​obtained may be different. Therefore, the gradient drop is only a local minimum.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-gd.jpeg}
\caption{gradient descent algorithm}
 \label{fig:mnist-gd}
\end{figure}

The step size of the gradient drop is very important. If it is too small, the speed of finding the minimum value of the function is very slow; if it is too large, it may exceed the extreme point. Generally, in the early stage of model training, because the target of convergence from the model is still far away, the learning rate is adjusted to be larger; as the number of iterations increases, the learning rate is adjusted to be smaller. Therefore, the learning rate $\alpha$ is adaptive, and there are theoretically many optimization algorithms that learn the rate of $\alpha$ decay with time, such as \ascii{Adagrad}. Therefore, the key to the optimization problem is how the gradient is calculated.

\subsection{calculation gradient}

According to the chain-based derivation rule, given any sample $ (x,t) \in S $, we can infer $ L(y, t) $ relative to $ w_i, i=1,2,...,n The gradient formula of $.

\[\begin{aligned}
  \frac{{\partial L}}{{\partial {w_i}}}
   = & \frac{{\partial L}}{{\partial y}}\frac{{\partial y}}{{\partial z}}\frac{{\partial z}}{{\partial {w_i} }} \\ 
   = & \left( {\frac{{1 - t}}{{1 - y}} - \frac{t}{y}} \right)y(1 - y){x_i} \\ 
   = & (y - t){x_i} \\ 
\end{aligned} \]

Similarly, a gradient formula for $L(W,b)$ relative to $b$ can be derived.

\[\begin{aligned}
  \frac{{\partial L}}{{\partial b}} 
   = & \frac{{\partial L}}{{\partial y}}\frac{{\partial y}}{{\partial z}}\frac{{\partial z}}{{\partial b}} \\ 
   = & \left( {\frac{{1 - t}}{{1 - y}} - \frac{t}{y}} \right)y(1 - y) \\ 
   = & y - t \\ 
\end{aligned} \]

Further, a gradient calculation formula of $w, b$ is obtained by vectorization. Where $\nabla _w}L( {w,b;x,t}) \in {\mathbb{R}^{n}}$,$\nabla _b}L( {w,b;x,t} ) \in {\mathbb{R}}$.

\[\begin{gathered}
  {\nabla _w}L( {w,b;x,t}) = \left({y - t} \right)x \\ 
  {\nabla _b}L( {w,b;x,t) = y - t \\ 
\end{gathered} \]

By extension, given training data set $ S = \{ ({x^{(i)}}, {t^{(i)}});i = 1,2,...,m\} $, get the vectorized gradient formula for $w,b$ as:

\[\begin{gathered}
  {\nabla _w}L(w,b;S) = \left( {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {{y^{(i)} } - {t^{(i)}}} \right)} } \right)x \\ 
  {\nabla _b}L(w,b;S) = \frac{1}{m}\sum\limits_{i = 1}^m {\left( {{y^{(i)}} - {t ^{(i)}}} \right)} \\ 
\end{gathered} \]

\subsection{parameter update}

There are three basic algorithms for parameter updating. For a given training sample $ ({x^{(i)}}, {t^{(i)}}) $, according to the gradient formula of $w, b$, complete the parameter update of this iteration, the algorithm Often referred to as \emph{random gradient descent method} (\ascii{SGD}). Because the parameters are updated once, \ascii{SGD} only needs to read one training sample, and a single sample does not represent universality. There may be a lot of noise, so the convergence of the model is more jittery. However, \ascii{SGD} can achieve rapid convergence of the model and is more efficient.

\[\begin{gathered}
  w \leftarrow w - \alpha \left( {{y^{(i)}} - {t^{(i)}}} \right)x \\ 
  b \leftarrow b - \alpha \left( {{y^{(i)}} - {t^{(i)}}} \right) \\ 
\end{gathered} \]

At the other extreme, given the entire training sample data $ S $, according to the gradient formula of $w, b$, the parameter update of this iteration is completed. This algorithm is often called \emph{batch gradient descent method} (\ascii {BGD}). Because the parameters are updated once, the entire training data set needs to be traversed, so the model convergence is relatively stable. However, due to the large amount of computation, the model convergence speed is slower than \ascii{SGD}.

\[\begin{gathered}
  w \leftarrow w - \alpha \left( {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {{y^{(i)}} - {t^{( i)}}} \right)} } \right)x \\ 
  b \leftarrow b - \alpha \left( {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {{y^{(i)}} - {t^{( i)}}} \right)} } \right) \\ 
\end{gathered} \]

In real-world applications, neither \ascii{BGD} nor \ascii{SGD} will be used, but the \ascii{SGD} algorithm of \ascii{MiniBatch} will be used. What's more, due to the large-scale application of \ascii{MiniBatch}'s \ascii{SGD}, \ascii{SGD}, often referred to as \ascii{MiniBatch}, is the \ascii{SGD} algorithm.

\[\begin{aligned}
  w \leftarrow w - \alpha \left( {\frac{1}{{atch#\zie}}\sum\limits_{i = 1}^{batch\_size} {\left( {{y^{(i) }} - {t^{(i)}}} \right)} } \right)x \\ 
  b \leftarrow b - \alpha \left( {\frac{1}{{atch#\size}}\sum\limits_{i = 1}^{batch\_size} {\left( {{y^{(i) }} - {t^{(i)}}} \right)} } \right) \\ 
\end{aligned} \]

Compared to \ascii{BGD}, \ascii{MiniBatch}'s \ascii{SGD} does not need to traverse the real training data set with new parameters, just traverse a \ascii{batch\_size} training samples; therefore, relative \ascii{BGD}, \ascii{MiniBatch}'s \ascii{SGD} converges faster. And relative to \ascii{SGD}, \ascii{MiniBatch}'s \ascii{SGD} reads new \ascii{batch\_size} training samples with new parameters each time; therefore, relative \ascii {SGD}, \ascii{MiniBatch}'s \ascii{SGD}'s convergence is more stable.

\subsection{calculation chart}

In general, when a neural network is used to represent a learning model, its training process involves two basic steps:

\begin{itemize}
  \item \ascii{forward calculation}: Calculating loss
  \item \ascii{backpropagation}: Calculate the gradient
\end{itemize}

To achieve a positive loss calculation, and its inverse gradient calculation, the network can be translated into an equivalent computational map. In the forward computational graph, the node describes some abstract mathematical operations, such as matrix multiplication, vector inner product, activation function, etc.; the edge represents the output value of the function and serves as the input to the downstream node function.



Positive as logistic regression as shown by \refig{logistic-bp}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/logistic-bp.png}
\caption{logical regression: forward calculation and back propagation}
 \label{fig:logistic-bp}
\end{figure}


\end{content}

\section{Single layer perceptron}

\begin{content}

First, try to build a single layer perceptron for \ascii{10} neurons. As shown by \refig{mnist-slp}, for multi-classification problems such as handwritten digit recognition, the activation function of \ascii{softmax} is often used in theory.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-slp.png}
\caption{Single layer perceptron}
 \label{fig:mnist-slp}
\end{figure}

\subsection{theory basis}

In theory, the \ascii{softmax} regression is a generalized extension of the \ascii{logistic} regression. Among them, \ascii{logistic} regression is to solve the two classification problem, namely $y \in \{ 0,1\}$; and \ascii{softmax} regression is to solve the $k $ classification problem, namely $y \in \{ 1,2,...,k\}$.

\subsubsection{symbol definition}

To formalize the \ascii{softmax} regression problem, some common symbols are defined here.

 \begin{itemize}
   \item \ascii{training sample set}: $ S = \{ ({x^{(i)}}, {y^{(i)}});i = 1,2,...,m\} $
   \item \ascii{第$i$ training sample}: $ ({x^{(i)}}, {y^{(i)}}) $
   \item \ascii{sample input}: $ x = ({x_1},{x_2},...,{x_n})^{T} \in {\mathbb{R}^n} $
   \item \ascii{sample tag (one-hot)}: $ y = ({y_1},{y_2},...,{y_k})^{T} \in {\mathbb{R}^k} $
   \item \ascii{weight}: $ W \in {\mathbb{R}^{n \times k}} $   
   \item \ascii{offset}: $ b \in {\mathbb{R}^k} $   
   \item \ascii{softmax function}: $ 
Softmax {(z_i)} = \tfrac{{{e^{{z_i}}}}}{{\sum\limits_{j = 1}^k {{e^{{z_j}}}} }} \quad i = 1,2,...,k
$
 \end{itemize}

\subsubsection{softmax function}

As shown by \refig{softmax}, the model first obtains the linear weighted sum $z$, then evaluates to $e^z$, and finally implements the normalization operation.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/softmax.png}
\caption{softmax function}
 \label{fig:softmax}
\end{figure}

\subsubsection{weights and offsets}

The weight $W$ is a two-dimensional matrix of $n \times k$.

\[
W = \left( {{W_1},{W_2},...,{W_k}} \right) = \left( {\begin{array}{*{20}{c}}
  {{w_{11}}}& \ldots &{{w_{1k}}} \\ 
   \vdots & \ddots & \vdots \\ 
  {{w_{n1}}}& \cdots &{{w_{nk}}} 
\end{array}} \right) \in {\mathbb{R}^{n \times k}}
\]

Where $W_j$ is a vector of length $n$.

\[
{W_j} = {\left( {{w_{1j}},{w_{2j}},...,{w_{nj}}} \right)^T} \in {\mathbb{R}^n }, j = 1,2,...,k \\
\]

The offset $b$ is a \ascii{\quo{one-hot}} vector of length $k$.

\[
b = {({b_1},{b_2},...,{b_k})^T} \in {\mathbb{R}^k}
\]

\subsubsection{model definition}

The single-layer perceptron model for multi-classification problems, using the \ascii{softmax} activation function, can be defined as such.

\[\begin{aligned}
  y = & {h_{W,b}}(x) = softmax (z) = softmax ({W^T}x + b) \\ 
   = & {\left( {{y_1},{y_2},...,{y_k}} \right)^T} \\ 
   = & \frac{1}{{\sum\limits_{j = 1}^k {{e^{{z_j}}}} }}{\left( {{e^{{z_1}}},{e ^{{z_2}}},...,{e^{{z_k}}}} \right)^T} \\ 
   = & \frac{1}{{\sum\limits_{j = 1}^k {{e^{W_j^Tx + {b_j}}}} }}{\left( {{e^{W_1^Tx + {b_1}}},{e^{W_2^Tx + {b_2}}},...,{e^{W_k^Tx + {b_k}}}} \right)^T} \ 
\end{aligned} \]

Where, for any given sample $ (x, y) \in S $, $ z_i $ represents the linear weighted sum of $W_i^Tx+b_i$, and $y_i(i=1,2,...,k )$ indicates the probability of classifying it as class $i$.

\[\begin{gathered}
  P\left( {y = i|x;W,b} \right) = \frac{{{e^{W_i^Tx + b_i}}}}{{\sum\limits_{j = 1}^k { {e^{W_j^Tx + b_j}}} }} \hfill \\
  i = 1,2,...,k \hfill \\ 
\end{gathered} \]


\subsubsection{cross entropy function}

Based on the sample data set $ S = \{ ({x^{(i)}}, {y^{(i)}});i = 1,2,...,m\} $, cross entropy loss function Can be defined as such.

\[\begin{aligned}
  J(W,b) = & - \frac{1}{m}\sum\limits_{i = 1}^m {{y^{(i)}}\log \left( {{{\widehat y} ^{(i)}}} \right)} \\ 
   = & - \frac{1}{m}\sum\limits_{i = 1}^m {\sum\limits_{j = 1}^k {y_j^{(i)}\log \left( {\widehat Y_j^{(i)}} \right)} } \\
\end{aligned} \]

The \ascii{softmax} multi-classification problem is to find the optimal solution $(W^*,b^*)$, so that

\[W^*,b^* = \mathop {\arg \min }\limits_{W,b} J(W,b)\]

\subsubsection{calculation gradient}

For any sample $(x,y) \in S $, you can derive a gradient formula for $ J(W,b) $ relative to $ W $ and $ b $.

\[\begin{aligned}
  {\nabla _W}J\left( {W,b;x,y} \right) = & \left( {\widehat y - y} \right)x \\ 
  {\nabla _b}J\left( {W,b;{x^{(i)}},{y^{(i)}}} \right) = & \left( {\widehat y - y} \ Right) \\ 
\end{aligned} \]


\subsubsection{parameter update}

For the training sample data $S$, according to the gradient formula of $W, b$, the parameter update of this iteration is completed.

\[\begin{aligned}
  W \leftarrow & W - \alpha \frac{{\sum\limits_{i = 1}^m {{\nabla _W}J\left( {W,b;{x^{(i)}},{y ^{(i)}}} \right)} }}{m} \\ 
  b \leftarrow & b - \alpha \frac{{\sum\limits_{i = 1}^m {{\nabla _b}J\left( {W,b;{x^{(i)}},{y ^{(i)}}} \right)} }}{m} \\ 
\end{aligned} \]

\subsection{define model}

Next, use \tf{} to complete the construction and training of the model. It should be noted that there are subtle differences between the theoretical formula and the concrete implementation of \tf{}. In theory, $x$ in a formula often represents a sample, but \code{x} in \tf{} often represents a sample dataset of \ascii{mini-batch}. Therefore, when using \tf{} to design a network model, you need to pay special attention to whether the changes in the individual tensors are in line with expectations.

\subsubsection{input and tag}

First, use \code{tf.placeholder} to define the input and label of the training sample separately.

\begin{leftbar}
\begin{python}
x = tf.placeholder(tf.float32, [None, 28, 28, 1])
t = tf.placeholder(tf.float32, [None, 10])
\end{python}
\end{leftbar}

\code{tf.placeholder} defines a placeholder \ascii{OP}. \code{None} represents the number of undetermined samples, which represents the size of \code{batch\_size}; when \code{Session.run}, a \ascii is provided via the \code{feed\_dict} dictionary The sample dataset of {mini-batch} automatically derives the size of \code{tf.placeholder}.

In addition, each image is represented by a three-dimensional data of $ 28 \times 28 \times 1 $ (the scale is \ascii{1}). To simplify the problem, the input sample data is flattened here and transformed into a one-dimensional vector of length \ascii{784}. Where \ascii{-1} represents the number of samples of \ascii{mini-batch}, which is automatically deduced by the runtime.

\begin{leftbar}
\begin{python}
x = tf.reshape(x, [-1, 784])
\end{python}
\end{leftbar}

\subsubsection{define variable}

Then, define the model parameters using \code{tf.Variable}. When defining the training parameters, you must specify the initialization value of the parameters; the training parameters will automatically derive the type of the data and its size based on the initial values.

\begin{leftbar}
\begin{python}
w = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
\end{python}
\end{leftbar}

In addition, the variables must be initialized before they are used. Here, \code{init\_op} will initialize all global training parameters.

\begin{leftbar}
\begin{python}
Init_op = tf.global_variables_initializer()
\end{python}
\end{leftbar}

\subsubsection{model definition}

Next, the single-layer perceptron model for multi-classification problems can be easily obtained.

\begin{leftbar}
\begin{python}
y = tf.nn.softmax(tf.matmul(x, w) + b)
\end{python}
\end{leftbar}

As shown in \refig{mnist-linear-sum}, first calculate the matrix multiplication of \code{x} and \code{w}, then let \code{b} broadcast (\ascii{broadcast}) to each of the matrices. Add a line and finally get a linear weighted sum of the training parameters.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-linear-sum.png}
\caption{linear weighted sum}
 \label{fig:mnist-linear-sum}
\end{figure}

As shown by \refig{mnist-softmax}, \ascii{softmax} will implement the operation line by line. Finally, the size of \code{y} is \code{[100, 10]}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-softmax.png}
\caption{Activation function: softmax}
 \label{fig:mnist-softmax}
\end{figure}

\subsubsection{loss function}

For multi-classification problems, the loss function of cross entropy can be used.

\begin{leftbar}
\begin{python}
Cross_entropy = -tf.reduce_sum(t * tf.log(y))
\end{python}
\end{leftbar}

As shown by \refig{mnist-cross-entropy}, \code{t} and \code{y} are both \code{[100, 10]}; in particular, every line of \code{t} Is a \quo{\ascii{one-hot}} vector.

Implementing the \code{tf.log} operation on \code{y} will also result in a matrix of size \code{[100, 10]}. Then, \code{t} is multiplied bit by bit with \code{tf.log(y)} (not multiplied by the matrix), and a matrix of size \code{[100, 10]} will also be obtained. Finally, \code{tf.reduce\_sum} adds all the elements in the matrix to get a scalar (\ascii{scalar}) value.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-cross-entropy.png}
\caption{cross entropy loss function}
 \label{fig:mnist-cross-entropy}
\end{figure}

\subsubsection{accuracy}

\code{tf.argmax(y,1)} will calculate the index of the maximum value by the \ascii{1} dimension. The index value of the maximum value in each row is calculated for each row of $ y_{100 \times 10} $ . Therefore, \code{tf.argmax(y,1)} will get a matrix of size \code{[100, 1]}, or a vector of size \ascii{100}. Similarly, \code{tf.argmax(t,1)} is also a vector of size \ascii{100}.

Then, use \code{tf.equal} to compare them by element (\ascii{element-wise}) for equality, and get a Boolean vector of size \ascii{100}. In order to calculate the precision, the Boolean vector is first converted to a numerical vector, and finally the mean of the numerical vector is obtained.

\begin{leftbar}
\begin{python}
Is_correct = tf.equal(tf.argmax(y,1), tf.argmax(t,1))
Accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
\end{python}
\end{leftbar}

\subsection{Optimization Algorithm}

Next, a gradient descent algorithm is used to minimize the cross entropy loss function. Among them, \code{learning\_rate} indicates the learning rate, describes the speed of the parameter update and the size of the step, which is a typical super parameter.

\begin{leftbar}
\begin{python}
Optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.003)
Train_step = optimizer.minimize(cross_entropy)
\end{python}
\end{leftbar}

\subsection{training model}

Prior to this, \tf{} only constructed the calculation graph and did not initiate the execution of the calculation graph. Next, the client creates a session, establishes a channel with the local or remote computing device set, and initiates the execution of the computation map.

First, the initialization of the training parameters is completed. The initial value is modified in situ to the corresponding training parameter by running the initialization subgraph of the model parameters and executing the initializer of each training parameter concurrently.

\begin{leftbar}
\begin{python}
With tf.Session() as sess:
  Sess.run(init_op)
\end{python}
\end{leftbar}

Then, iteratively executes \code{train\_step} to complete an iterative training of the model. Among them, every \ascii {100} iterations, calculate the accuracy and loss of the current model in the training data set and the test data set.

\begin{leftbar}
\begin{python}
With tf.Session() as sess:
  For step in range(1000):
    Batch_xs, batch_ys = mnist.train.next_batch(100)        
    Sess.run(train_step, feed_dict={x: batch_xs, t: batch_ys})
    
    If step % 100 == 0:
      Acc, loss = sess.run([accuracy, cross_entropy], 
        Feed_dict={x: batch_xs, t: batch_ys})
      Acc, loss = sess.run([accuracy, cross_entropy], 
        Feed_dict={x: mnist.test.images, t: mnist.test.labels}) 
\end{python}
\end{leftbar}

According to statistics, after \ascii{1000} iterations, you get an accuracy of about \percent{92}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-slp-accuracy.png}
\caption{Visualization: Single layer perceptron, running 1000 times step}
 \label{fig:mnist-slp-accuracy}
\end{figure}

\end{content}