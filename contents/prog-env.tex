\begin{savequote}[45mm]
  \ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
  \qauthor{\ascii{- Martin Flower}}
\end{savequote}


\chapter{Programming Environment} 
\label{ch:prog-env}

\begin{content}
In order to achieve a quick introduction to \tf{}, this chapter introduces the programming environment of \tf{}, including code structure, engineering construction, in order to establish a basic perceptual understanding of the \tf{} system architecture.
\end{content}


\section{Code structure}
\begin{content}


\subsection{Clone source}
First, clone the source code for \tf{} from \ascii{Github}.
\begin{leftbar}
\begin{python}
git clone git@github.com:tensorflow/tensorflow.git
\end{python}
\end{leftbar}

Then, switch to the latest stable branch. For example, the \code{r1.4} branch.
\begin{leftbar}
\begin{python}
$ cd tensorflow
$ git checkout r1.4
\end{python}%
\end{leftbar}


\subsection{Source Structure}
Run the following command to print out the organization structure of the \tf{} source:
\begin{leftbar}
\begin{python}[]
$ tree -d -L 1 ./tensorflow
\end{python}
\end{leftbar}

Among them, this book will focus on the \code{core, python} component, and some involve the \code{c, cc, stream\_executor} component.

\begin{leftbar}
\begin{c++}[caption={TensorFlowSource Structure}]
./tensorflow
├── c
├── cc
├── compiler
├── contrib
├── core
├── docs_src
├── examples
├── g3doc
├── go
├── java
├── python
├── stream_executor
├── tools
└── user_ops
\end{c++}
\end{leftbar}

As of the latest release of \ascii{1.4}, the \tf{} codebase has approximately \ascii{100} million code. Among them, including \ascii{53} million line \ascii{C/C++} code, \ascii{37} million lines\ascii{Python} code, and the code size is constantly expanding. Among them, the \ascii{API} provided by \ascii{Python} is the most complete; in contrast, the \ascii{API} of other programming languages ​​is not yet mature, even in its infancy.

\begin{leftbar}
\begin{python}[caption={TensorFlowCode Stats}]
-------------------------------------------------- -----
Language             files    blank    comment    code
-------------------------------------------------- -----
C++                   2238    77610     68275    443099
Python                1881    92018    151807    369399
C/C++ Header          1175    27392     46215     86691
Markdown 218 8859 2 30925
CMake                   50     2183       986     16398
Go                      28     1779     13290     15003
Java                    72     1789      3111      7779
Bourne Shell           103     1487      3105      6074
Protocol Buffers        87     1313      3339      3452
Objective C++            9      227       181      1201
C                        8      157       130       941
make                     4      105       136       612
XML                     25      135       265       315
Groovy                   3       46        74       246
Maven                    5       21         4       245
DOS Batch 9 30 0 143
Dockerfile               7       41        69       133
Perl                     2       29        38       130
Bourne Again Shell       3       24        63       111
JSON                     3        0         0        23
Objective C              1       10        13        21
YAML 1 3 24 15
-------------------------------------------------- -----
SUM:                  5932   215258    291127    982956
-------------------------------------------------- -----
\end{python}
\end{leftbar}


\subsection{Core}
The source code structure of the kernel is as follows, including platform, utility library, base framework, \ascii{Protobuf} definition, local runtime, distributed runtime, graph operation, \ascii{OP} definition, and \ascii{Kernel} implementation and other components, this is one of the key components of this book, will focus on mining the hidden domain model in the basic framework, tracking the life cycle of the entire runtime environment and the detailed process of the graph operation, and reveal the common \ascii{OP} \ascii{Kernel} implementation principle and operating mechanism.

\begin{leftbar}
\begin{c++}[caption={Core source structure}]
./tensorflow/core
├── common_runtime
├── debug
├── distributed_runtime
├── example
├── framework
├── graph
├── grappler
├── kernels
├── lib
├── ops
├── platform
├── profiles
├─ protobuf
├── public
├── user_ops
└── useful
\end{c++}
\end{leftbar}

Among them, \code{core} is mainly implemented in \code{C++}, which has about \ascii{26} million lines of code.

\begin{leftbar}
\begin{python}[caption={CoreCode Stats}]
-------------------------------------------------- -----
Language             files    blank   comment      code
-------------------------------------------------- -----
C++                   1368    44791     38968    259289
C/C++ Header           653    15040     24474     50506
Protocol Buffers        57      736      2371      1806
Markdown 11 327 0 1285
JSON                     2        0         0        18
-------------------------------------------------- -----
SUM:                  2091    60894     65813    312904
-------------------------------------------------- -----
\end{python}
\end{leftbar}


\subsection{Python}
\ascii{Python} defines and implements the programming model of \tf{}, and provides an \ascii{API} for programmers. The source structure is as follows, which is also the focus of this book.

\begin{leftbar}
\begin{c++}[caption={Python source structure}]
./tensorflow/python
├── client
├── debug
├── estimator
├── feature_column
├── framework
├── grappler
├── kernel_tests
├── layers
├── lib
├── ops
├── platform
├── profiles
├── saved_model
├── summary
├── tools
├── training
├── user_ops
└── useful
\end{c++}
\end{leftbar}

Among them, the component is implemented in \code{Python}, which has about \ascii{18} million lines of code.

\begin{leftbar}
\begin{python}[caption={Python Code Statistics}]
-------------------------------------------------- -----
Language            files     blank   comment      code
-------------------------------------------------- -----
Python                714     45769     69407    179565
C++                    20       496       506      3658
C/C++ Header           15       207       387       363
Markdown 4 48 0 200
Protocol Buffers        3        16        10        71
Bourne Shell            1        13        28        68
-------------------------------------------------- -----
SUM:                  757     46549     70338    183925
-------------------------------------------------- -----
\end{python}
\end{leftbar}


\subsection{Contrib}
\code{contrib} is a programming library contributed by third parties. It is also an experimental programming interface before \tf{} standardization, just like the relationship between the \ascii{Boost} community and the \ascii{C++} standard. When the interface of \code{contrib} matures, it will be standardized by \tf{}, and will be moved from \code{contrib} to \code{core, python} and officially released.

\begin{leftbar}
\begin{python}[caption={Contrib source structure}]
./tensorflow/contrib
├── android
├── batching
├── bayesflow
├── benchmark_tools
├── boosted_trees
├── cloud
├── cluster_resolve
├── cmake
├── compiler
├── copy_graph
├── crf
├── cudnn_rnn
├── data
├── decision_trees
├── deprecated
├── distributions
├── eager
├── factorization
├── ffmpeg
├── framework
├── fused_conv
├── gdr
├── graph_editor
├── grid_rnn
├── hooks
├── hvx
├── image
├── imperative
├── input_pipeline
├── integrate
├── hard
├── kernel_methods
├── labeled_tensor
├── layers
├── learn
├── legacy_seq2seq
├── linalg
├── linear_optimizer
├── lookup
├── losses
├── makefile
├── memory_stats
├── meta_graph_transform
├── metrics
├── mpi
├── nccl
├── ndlstm
├── nearest_neighbor
├── nn
├── opt
├── pi_examples
├── predictor
├── quantization
├── reduce_slice_ops
├── remote_fused_graph
├── resampler
├── rnn
├── saved_model
├── seq2seq
├── session_bundle
├── signal
├── slim
├── solvers
├── sparsemax
├── specs
├── staging
├── stat_summarizer
├── stateless
├── tensor_forest
├── tensorboard
├── testing
├── text
├── tfprof
├── timeseries
├── tpu
├── training
├── useful
├── verbs
└── xla_tf_graph
\end{python}
\end{leftbar}

Since the \tf{} community is quite active, the changes to \code{contrib} are quite frequent. As of \ascii{1.4}, there are about \ascii{23} million lines of code, mainly designed and implemented in \ascii{Python}. The interface, partially run by \ascii{C++}.

\begin{leftbar}
\begin{python}[caption={ContribCode Statistics}]
-------------------------------------------------- -----
Language            files     blank   comment      code
-------------------------------------------------- -----
Python               1007     41436     75096    170355
C++                   201      5500      5313     32944
CMake                  48      2172       955     16358
C/C++ Header           99      1875      2867      6583
Markdown 46 1108 0 3485
Bourne Shell           18       232       386      1272
C                       7       151       118       931
Protocol Buffers       20       224       454       680
make                    4       105       136       612
Java                    2        77       209       335
Groovy                  1        10        20        75
Bourne Again Shell      1         6        15        59
Dockerfile              1         2         1        14
XML                     2         3         0         9
-------------------------------------------------- -----
SUM:                 1457     52901     85570    233712
-------------------------------------------------- -----
\end{python}
\end{leftbar}


\subsection{StreamExecutor}
\ascii{StreamExecutor} is another open source component library of \ascii{Google} that provides a host-side (\ascii{host-side}) programming model and runtime environment that implements \ascii{CUDA} and \ascii{Unified packaging for OpenCL}. In the code on the host side, the \ascii{Kernel} function can be deployed seamlessly on the computing device of \code{CUDA} or \code{OpenCL}.
Currently, \ascii{StreamExecutor} is heavily used in the runtime of the \ascii{Google} internal\ascii{GPGPU} application. The \tf{} runtime also contains a snapshot version of \ascii{StreamExecutor} that encapsulates the runtimes of \ascii{CUDA} and \code{OpenCL}. This book will briefly introduce the programming model and threading model of \ascii{CUDA}, and introduce the system architecture and working principle of \ascii{StreamExecutor} in detail, revealing the implementation mode and idiom of \ascii{Kernel} function.

\begin{leftbar}
\begin{c++}[caption={StreamExecutorSource Structure}]
./tensorflow/stream_executor
├── miracles
├── host
├── lib
└── platform
\end{c++}
\end{leftbar}

Among them, a \ascii{StreamExecutor} is implemented in \code{C++}, which has about \ascii{2.5} million lines of code.

\begin{leftbar}
\begin{python}[caption={StreamExecutorCode Statistics}]
-------------------------------------------------- -----
Language            files     blank   comment      code
-------------------------------------------------- -----
C++                    43      2440      1196     16577
C/C++ Header           81      2322      5080      8625
-------------------------------------------------- -----
SUM:                  124      4762      6276     25202
-------------------------------------------------- -----
\end{python}
\end{leftbar}


\subsection{Compiler}
As we all know, flexibility is the most important design goal and core advantage of \tf{}, so the system architecture of \tf{} has good scalability. \tf{} can be used to define arbitrary graph structures and execute efficiently using heterogeneous computing devices. However, the bear's paw and the shark's fin cannot be combined. When the low-level \ascii{OP} is combined to calculate the subgraph and is expected to be effectively executed on \ascii{GPU}, the runtime will start more \ascii{Kernel} Operation.
Therefore, the \tf{} method of decomposing and combining \ascii{OP} is not guaranteed to run in the most efficient way at runtime. At this point, \ascii{XLA} technology was born, it uses \ascii{JIT} compilation technology to analyze the runtime calculation graph, which combines multiple \ascii{OP} and generates a more efficient local machine. Code to improve the execution efficiency of the calculation graph.

\begin{leftbar}
\begin{python}[caption={CompilerSource Structure}]
./tensorflow/compiler
├── aoT
├── jit
├── plugin
├── tests
├── tf2xla
└── xla
\end{python}
\end{leftbar}

\ascii{XLA} technology is currently in the initial stage of research and development, and is a more active research direction in the community. As of now, the code size is approximately \ascii{12.5} million lines, mainly in \ascii{C++}.

\begin{leftbar}
\begin{python}[caption={CompilerCode Stats}]
-------------------------------------------------- -----
Language            files     blank   comment      code
-------------------------------------------------- -----
C++                   455     19010     18334    102537
C/C++ Header          250      5939     10323     15510
Python                 37      1255      1416      6446
Protocol Buffers        5       312       501       781
Markdown 2 0 0 3
-------------------------------------------------- -----
SUM:                  749     26516     30574    125277
-------------------------------------------------- -----
\end{python}
\end{leftbar}

\end{content}


\section{Project Construction}

\begin{content}
Before you start, try the \tf{} source build process, understand the basic build methods of \tf{}, the tools, and their dependent component libraries, third-party toolkits, and have a great understanding of how \tf{} works. help. However, due to limited space, this chapter only uses the \ascii{Mac OS} system as an example to describe the source code compilation, installation, and verification process of \tf{}. For other operating systems, please check the official documentation published by \tf{}.


\subsection{Environment preparation}
The front end of \ascii{TensorFlow} is a programming interface that supports multiple languages. Therefore, before compiling the \ascii{TensorFlow} source code, you need to install the relevant compiler, interpreter, and its runtime environment. For example, to use \ascii{Python} as a programming interface, you need to install the \ascii{Python} interpreter beforehand. Second, before building the system, you also need to install \ascii{GCC} or \ascii{Clang} and other \ascii{C++} compilers to compile the backend system implementation. Since \ascii{TensorFlow} is implemented using the \ascii{C++11} syntax, it is important to ensure that the \ascii{C++} compiler is installed to support \ascii{C++11}. In addition, \ascii{TensorFlow} uses the \ascii{Bazel} build tool, which can be thought of as a more abstract \ascii{Make} tool. Unfortunately, \ascii{Bazel} is implemented using \ascii{Java8}, which relies on \ascii{JDK}. Therefore, before installing \ascii{Bazel}, you need to install \ascii{JDK} in \ascii{1.8} and above.


\subsubsection{Install JDK}
It is recommended to download \ascii{JDK} from the \ascii{Oracle} website and create the relevant environment variables and add them to the \code{~/.bashrc} configuration file.

\begin{leftbar}
\begin{python}
$ echo 'export JAVA_HOME=$(/usr/libexec/java_home)' >> ~/.bashrc
$ echo 'export PATH="$JAVA_HOME/bin:$PATH"' >> ~/.bashrc
\end{python}
\end{leftbar}


\subsubsection{Installing Bazel}
On \ascii{Mac OS}, you can install \ascii{Bazel} using \ascii{brew}.

\begin{leftbar}
\begin{python}
$ brew install bazel
\end{python}
\end{leftbar}

If \ascii{brew} is not installed on the system, you can install \ascii{brew} by executing the following command. Of course, installing \ascii{brew} requires the \ascii{Ruby} interpreter to be installed beforehand, and will not be redundant here.

\begin{leftbar}
\begin{python}
$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
\end{python}
\end{leftbar}


\subsubsection{Install Swig}
\ascii{TensorFlow} uses \ascii{Swig} to build a multi-language programming environment that automatically generates wrappers for related programming languages. Therefore, you need to install the \ascii{Swig} toolkit before you build it.

\begin{leftbar}
\begin{python}
$ brew install swig
\end{python}
\end{leftbar}


\subsubsection{Install Python dependency package}
Use \ascii{pip} to install the \ascii{Python} package that \ascii{TensorFlow} depends on.

\begin{leftbar}
\begin{python}
$ sudo pip install six numpy wheel autograd
\end{python}
\end{leftbar}

If \ascii{pip} is not installed on your system, you can pre-install \ascii{pip} using \ascii{brew}.

\begin{leftbar}
\begin{python}
$ brew install pip
\end{python}
\end{leftbar}


\subsubsection{Install CUDA Toolkit}

When the system installs \ascii{CUDA} to calculate the \ascii{GPU} card with compatibility greater than or equal to \ascii{3.0}, you need to install the \ascii{CUDA} toolkit and its \ascii{cuDNN} to implement\ The \tf{} runtime's \ascii{GPU} acceleration. It is recommended to download \ascii{CUDA Toolkit 8} and above from the \ascii{NVIDIA} website and install it into the system to configure related environment variables.

\begin{leftbar}
\begin{python}
$ echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc
$ echo 'export LD_LIBRARY_PATH=$CUDA_HOME/lib:$LD_LIBRARY_PATH' >> ~/.bashrc
\end{python}
\end{leftbar}

Then, download \ascii{cuDNN 5.1} and above and extract it to the \code{CUDA\_HOME} related system directory.

\begin{leftbar}
\begin{python}
$ sudo tar -xvf cudnn-8.0-macos-x64-v5.1.tgz -C /usr/local
\end{python}
\end{leftbar}


\subsection{Configuration}
At this point, the build environment is ready, execute the \code{./configure} configuration\ascii{TensorFlow} build environment. When the system supports \ascii{GPU}, you need to configure the relevant \ascii{CUDA/cuDNN} build environment.

\begin{leftbar}
\begin{python}
$ ./configure
\end{python}
\end{leftbar}


\subsection{Build}
When the configuration is successful, use \ascii{Bazel} to start the compilation of \ascii{TensorFlow}. Before the compilation starts, it will try to download the source code of the relevant dependencies from the code repository, including \ascii{gRPC, Protobuf, Eigen}, etc., and compile automatically.

\begin{leftbar}
\begin{python}
$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
\end{python}
\end{leftbar}

Add the \code{--config=cuda} compile option when supporting \ascii{GPU} calculations.

\begin{leftbar}
\begin{python}
$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
\end{python}
\end{leftbar}

After compiling successfully, you can build the \ascii{Wheel} package of \ascii{TensorFlow}.

\begin{leftbar}
\begin{python}
$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
\end{python}
\end{leftbar}


\subsection{Installation}
When the \ascii{Wheel} package is successfully built, use \ascii{pip} to install \ascii{TensorFlow} into the system.

\begin{leftbar}
\begin{python}
$ sudo pip install /tmp/tensorflow_pkg/tensorflow-1.4.0-py2-none-any.whl
\end{python}
\end{leftbar}


\subsection{Verification}
Start the \ascii{Python} interpreter and verify that the \ascii{TensorFlow} installation was successful.

\begin{leftbar}
\begin{python}
$ python
>>> import tensorflow as tf
>>> hello = tf.constant('Hello, TensorFlow!')
>>> sess = tf.Session()
>>> print(sess.run(hello))
Hello, TensorFlow!
\end{python}
\end{leftbar}


\subsection{IDE}
Choosing a suitable \ascii{IDE} can improve the quality and speed of code reading before reading the code. It is recommended to use the \ascii{Eclipse CDT} to read the \ascii{C++} code and install the \code{PyDev} plugin to read the \ascii{Python} code. At the same time, it is also recommended to \ascii{Clion}\ascii{C++}, \ascii{PyCharm}\ascii{Python} from \ascii{JetBrains}. However, when reading the \ascii{C++} code, you need to configure the search directory for the \ascii{TensorFlow, CUDA, Eigen3} header files and add the relevant predefined macros so that \code{IDE} correctly parses the symbols in the code. This chapter uses \ascii{Eclipse CDT} as an example to describe the relevant configuration methods.


\subsubsection{Create Eclipse project}
Create a \ascii{Eclipse C++} project, as shown by \refig{setup-eclipse}. Determine the unique project name, manually specify the root directory of the \ascii{TensorFlow} source code, and select the empty project for \ascii{Makefile}. Then, follow the \ascii{Properties > C/C++ General > Paths and Symbols > Includes} configuration directory for the header files.

\begin{table}[!htb]
\resizebox{0.95\textwidth}{!} {
\begin{tabular*}{1.2\textwidth}{@{}ll@{}}
\toprule
\ascii{Configuration Item} & \ascii{Directory} \\
\midrule
\ascii{TensorFlow} & \code{/usr/local/lib/python2.7/site-packages/tensorflow/include} \\
\ascii{CUDA} & \code{/usr/local/cuda/include} \\ 
\bottomrule
\end{tabular*}
}
\caption{header file search directory}
\label{tbl:tf-includes}
\end{table}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.75\textwidth]{figures/setup-eclipse.png}
  \caption{Create Eclipse C++ project}
  \label{fig:setup-eclipse}
\end{figure}


\subsubsection{Configure Eigen}
Unfortunately, \ascii{Eigen}'s publicly available header files lack the suffix of \code{.h}, and \ascii{CDT} cannot resolve related symbols. See \code{\href{http://eigen.tuxfamily.org/index.php?title=IDEs}{http://eigen.tuxfamily.org/index.php?title=IDEs}} for instructions, follow \ascii{Preferences > C/C++ > Coding Style > Organize Includes > Header Substitution} Import the \code{eigen-header-substitution.xml} file as shown by \refig{eclipse-eigen3}.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.75\textwidth]{figures/eclipse-eigen3.png}
  \caption{Replace the header file of \ascii{Eigen}}
  \label{fig:eclipse-eigen3}
\end{figure}

\end{content}


\section{Code generation}
\begin{content}
When building the \ascii{TensorFlow} system, \ascii{Bazel} or \ascii{CMake} will automatically generate some source code. Understanding the output of the code generator can deepen understanding of the system's behavioral patterns.
\end{content}


\section{Technology stack}
\begin{content}
As shown in \refig{tf-stack}, the \tf{} technology stack is presented in terms of the hierarchy of the system, forming the core of the \tf{} ecosystem.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/tf-stack.png}
  \caption{TensorFlow Technology Stack}
  \label{fig:tf-stack}
\end{figure}

\end{content}