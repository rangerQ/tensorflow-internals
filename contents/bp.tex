\begin{savequote}[45mm]
\ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
\qauthor{\ascii{- Martin Flower}}
\end{savequote}

\chapter{BP algorithm} 
\label{ch:bp}

\begin{content}

\end{content}

\section{TensorFlow implementation}

\begin{content}

\tf{} is a software system that implements automatic differentiation. First, it constructs a forward computational graph that implements the forward computation of the computational graph. When calling the \code{Optimizer.minimize} method, use the \code{compute\_gradients} method to implement the construction of the inverse calculation graph; use the \code{apply\_gradients} method to implement the submap construction of the parameter update.

\begin{leftbar}
\begin{python}
Class Optimizer(object):
  Def minimize(self, loss, var_list=None, global_step=None):
    """Add operations to minimize loss by updating var_list.
    """
    Grads_and_vars = self.compute_gradients(
      Loss, var_list=var_list)
    Return self.apply_gradients(
      Grads_and_vars, 
      Global_step=global_step)
\end{python}
\end{leftbar}

\subsection{calculation gradient}

\code{compute\_gradients} will solve the gradient of \code{var\_list=[v1, v2, ..., vn]} according to the value of \code{loss}, and the final result will be: \code{[ (grad\_v1, v1), (grad\_v2, v2), ..., (grad\_vn, vn)]}. Among them, \code{compute\_gradients} will call the \code{gradients} method to construct a submap of backpropagation.

Explain the construction process of the reverse subgraph with a simple example. First, construct a forward calculation graph.

\begin{leftbar}
\begin{python}
X = tf.placeholder("float", name="X")
Y = tf.placeholder("float", name="Y")
w = tf.Variable(0.0, name="w")
b = tf.Variable(0.0, name="b")
Loss = tf.square(Y - X*w - b)
Global_step = tf.Variable(0, trainable=False, collections=[])
\end{python}
\end{leftbar}

Construct a backpropagated subgraph with \code{compute\_gradients}.

\begin{leftbar}
\begin{python}
Sgd = tf.train.GradientDescentOptimizer(0.01)
Grads_and_vars = sgd.compute_gradients(loss)
\end{python}
\end{leftbar}

\subsubsection{structuring algorithm}

The construction algorithm of the reverse subgraph can be formally described as:

\begin{leftbar}
\begin{python}
Def gradients(loss, grad=I):
  Vrg = build_virtual_reversed_graph(loss)
  For op in vrg.topological_sort():
    Grad_fn = ops.get_gradient_function(op)
    Grad = grad_fn(op, grad)
\end{python}
\end{leftbar}

First, construct a virtual reverse subgraph based on the topological map of the forward subgraph. It is called virtual because the real reverse subgraph is much more complicated than it is; more accurately, a node in the virtual reverse subgraph corresponds to a local part of the real reverse subgraph Subgraph.

At the same time, the output of the last node of the forward subgraph has an output gradient of \code{Tensor} of \ascii{1} as the initial gradient value of the reverse subgraph, often denoted as \code{I}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/bp-back-graph-construction.png}
\caption{Construct backpropagation subgraph}
 \label{fig:bp-back-graph-construction}
\end{figure}

Next, construct a real reverse subgraph based on the virtual reverse subgraph. First, performing a topological sorting algorithm according to the reverse virtual subgraph, and obtaining a topological sorting of the virtual reverse subgraph; and then, according to the topological sorting, searching for \ascii{OP} in each forward subgraph Its "gradient function"; finally, the gradient function is called, which will construct the inverse partial subgraph corresponding to \ascii{OP}.

In summary, a positive \ascii{OP} corresponds to a reverse partial local subgraph and is constructed by the gradient function of \ascii{OP}. When the entire topological sorting algorithm is completed, each \ascii{OP} in the forward subgraph can find the corresponding local subgraph in the reverse subgraph.

For example, in the above example, the last \ascii{OP} in the forward graph: the function of finding the square is used as an example to describe how the gradient function works.

\subsubsection{gradient function prototype}

In general, the gradient function satisfies the following prototype:

\begin{leftbar}
\begin{python}
@ops.RegisterGradient("op_name")
Def op_grad_func(op, grad):
\end{python}
\end{leftbar}

Among them, the gradient function is registered by \code{ops.RegisterGradient} and placed in the repository where the gradient function is saved. Later, you can index the corresponding gradient function according to the name of the forward \ascii{OP}.

For a gradient function, the first parameter \code{op} represents the forward calculation of \ascii{OP}, according to which it can get the input and output of \ascii{OP} in the forward calculation; the second parameter \code{ Grad} is the gradient passed by the upstream node in the reverse subgraph. It is a calculated gradient value (the initial gradient value is all 1).

\subsubsection{actual combat: square function}

As a simple example, use only the input to calculate the gradient. \code{y=square(x)}, used to find the square of \code{x}. First, construct a forward calculation graph:

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{figures/bp-square-forward-graph.png}
\caption{Square function: forward propagation subgraph}
 \label{fig:bp-square-forward-graph}
\end{figure}

Then, construct a virtual reverse subgraph in reverse. Construct a true inverse computation subgraph based on the topological ordering of the virtual inverse subgraph. If the current node is \code{Square}, find the corresponding gradient function \code{SquareGrad} from the repository based on its OP name.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth]{figures/bp-square-backward-graph.png}
\caption{Square function: backpropagation subgraph}
 \label{fig:bp-square-backward-graph}
\end{figure}

Because the derivative of \code{y=Square(x)} is \code{y'=2*x}. Therefore, the implementation of its gradient function \code{SquareGrad} is:

\begin{leftbar}
\begin{python}
@ops.RegisterGradient("Square")
Def SquareGrad(op, grad):
  x = op.inputs[0]
  With ops.control_dependencies([grad.op]):
    x = math_ops.conj(x)
    Return grad * (2.0 * x)
\end{python}
\end{leftbar}

After calling the gradient function, you will get \ascii{OP} of the forward \code{Square}, corresponding to the reverse subgraph \code{SquareGrad}. It needs to use the input of \code{Square} to complete the corresponding gradient calculation.

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{figures/bp-square-backward-graph-2.png}
\caption{Square function: backpropagation subgraph}
 \label{fig:bp-square-backward-graph-2}
\end{figure}

In general, a \ascii{OP} in the forward subgraph corresponds to a local subgraph in the reverse subgraph. Because, for the gradient function implementation of \ascii{OP}, multiple \ascii{OP} may be required to complete the corresponding gradient calculation. For example, \ascii{OP} of \code{Square}, corresponding to the gradient function constructed with two \ascii{2} multiplications \ascii{OP}.

\subsubsection{actual warfare: exponential function}

To give a simple example, use only the output to calculate the gradient. \code{y=exp(x)}, exponential function; its derivative is \code{y'=exp(x)}, which is \code{y'=y}. Therefore, its gradient function is implemented as:

\begin{leftbar}
\begin{python}
@ops.RegisterGradient("Exp")
Def _ExpGrad(op, grad):
  """Returns grad * exp(x)."""
  y = op.outputs[0]
  With ops.control_dependencies([grad.op]):
    y = math_ops.conj(y)
    Return grad * y
\end{python}
\end{leftbar}

As shown in the figure below, the output of the \ascii{OP} in the forward subgraph is used for the gradient operation of the corresponding inverse local subgraph. Moreover, the local subgraph contains one node.

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{figures/bp-exp-backward-graph.png}
\caption{Exp function: backpropagation subgraph}
 \label{fig:bp-exp-backward-graph}
\end{figure}

\subsection{Apply gradient}

To make a simple summary, when calling the \code{Optimizer.minimize} method, use the \code{compute\_gradients} method to implement the construction of the inverse computation graph; use the \code{apply\_gradients} method to implement parameter updates. Subgraph construction.

\subsubsection{structuring algorithm}

First, \code{compute\_gradients} will solve the gradient of \code{var\_list=[v1, v2, ..., vn]} at runtime based on the value of \code{loss}, and the final result will be :\code{vars\_and\_grads = [(grad\_v1, v1), (grad\_v2, v2), ..., (grad\_vn, vn)]}.

Then, \code{apply\_gradients} iteration \code{grads\_and\_vars}, for each \code{(grad\_vi, vi)}, construct a submap that updates \code{vi}. Among them, the algorithm can be formally described as:

\begin{leftbar}
\begin{python}
Def apply_gradients(grads_and_vars, learning_rate):
  For (grad, var) in grads_and_vars:
    Apply_gradient_descent(learning_rate, grad, var)
\end{python}
\end{leftbar}

Among them, \code{apply\_gradient\_descent} will construct a computational subgraph that uses the gradient descent algorithm to update the parameters. Use the binary of \code{(grad, var)} and its \code{learning\_rate}\code{Const OP} as input to \code{ApplyGradientDescent}.

\code{ApplyGradientDescent} will apply the \code{var <- var - learning*grad} algorithm to implement an in-place update of \code{var}.

\begin{figure}[!h]
\centering
\includegraphics[width=0.6\textwidth]{figures/bp-update-w.png}
\caption{Parameter update submap}
 \label{fig:bp-update-w}
\end{figure}

\subsubsection{Parameter Update Summary}

If there are multiple trained Variables, a partial subgraph of multiple update parameters is finally generated. They are aggregated together using a control dependency edge via a \code{NoOp} named \code{update}. Because each \code{Variable} is independent of each other, maximum concurrency can be achieved.

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{figures/bp-update-all-params.png}
\caption{Parameter update summary}
 \label{fig:bp-update-all-params}
\end{figure}

\subsubsection{Exploring train\_op}

After a round of \ascii{Step} operation, the parameters are updated according to the gradient, and finally \code{global\_step} is added. The \ascii{OP} that implements \code{global\_step} plus \ascii{1} is \code{AssignAdd} and is marked as \code{train\_op}; it holds the \code{global\_step} variable The reference, then complete the in-place modification and add its value to \ascii{1}.

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{figures/bp-train-op.png}
\caption{train\_op}
 \label{fig:bp-train-op}
\end{figure}

\subsubsection{workflow}

As shown in \refig{bp-train-pipeline}, the entire training process, a \ascii{Step} training process is calculated by forward calculation, reverse gradient calculation, parameter update, and its \code{global\_step} The four basic processes are composed.

Among them, each round \ascii{Step} starts from the start of \code{Session.run}. The output of each \ascii{OP} is obtained by the calculation of the forward subgraph and is used as the input of the downstream \ascii{OP}.

After the calculation is completed on the subgraph, the gradient of each training parameter is inversely calculated by using the initial gradient vector $I$ as input, and finally the gradient list of each training parameter is obtained, and \code{grads\_and\_vars = [( A two-tuple list representation of grad\_v1, v1), ..., (grad\_vn, vn)]}.

Subsequently, the parameter update subgraph takes \code{grads\_and\_vars} as input and performs the gradient descent update algorithm; finally, the \code{global\_step} value is added by \code{train\_op} plus \ascii{1 }, the round of \ascii{Step} is completed.

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{figures/bp-train-pipeline.png}
\caption{Model training workflow}
 \label{fig:bp-train-pipeline}
\end{figure}

\end{content}