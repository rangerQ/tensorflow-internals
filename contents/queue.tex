\begin{savequote}[45mm]
\ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
\qauthor{\ascii{- Martin Flower}}
\end{savequote}

\chapter{Queue} 
\label{ch:queue}

\begin{content}

\ascii{TensorFlow}'s \code{Session} is thread-safe. That is, multiple threads can use the same \code{Session} instance to concurrently execute the same \ascii{OP} of the same graph instance; the \ascii{TensorFlow} execution engine prunes the graph based on input and output. , get a sub-graph with minimal dependence.

Therefore, by multithreading and using the same \code{Session} instance, concurrently executing the different \ascii{OP} of the same graph instance, the final effect is the concurrent execution between the subgraphs.

For typical model training, you can take full advantage of the \code{Session} multi-threaded concurrency and improve the performance of the training. For example, the input submap is run in a separate thread to prepare the sample data; the training submap is run in a separate thread and is taken in a batch according to the size of \code{batch\_size} Sample and start the iterative training process.

This article will explain the infrastructure in the concurrency model above, including queues, multithreaded coordinators, and \ascii{QueueRunner} that controls \code{Enqueue OP} execution.

\end{content}

\section{Queue}

\begin{content}

In the execution engine of \ascii{TensorFlow}, \ascii{Queue} is a powerful tool for controlling asynchronous computing. In particular, \ascii{Queue} is a special \ascii{OP}, similar to \ascii{Variable}, which is a class of stateful \ascii{OP}.

Similarly, \ascii{Variable} has the associated \ascii{Assign} and other \ascii{OP}, \ascii{OP}, and \ascii{OP}, such as \code{Enqueue, Dequeue, EnqueueMany, DequeueMany} and other \ascii{OP}, they can directly modify the state of \ascii{Queue}.

\subsection{FIFOQueue}

Give a simple example. First, a \code{FIFOQueue} queue is constructed; then, a \code{EnqueueMany} is added to the calculation graph, and the \ascii{OP} is used to append \ascii{1} or more elements to the head of the queue. Secondly, add a dequeue of \code{Dequeue}; finally, increase the value of the dequeue element by \ascii{1} and then join the result. The construction of the calculation graph is shown in the figure below before starting the calculation diagram execution.

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{figures/py-queue-example-1.png}
\caption{Figure construction period}
 \label{fig:py-queue-example-1}
\end{figure}

After executing the \code{EnqueueMany} operation, the state of the calculation graph is as shown below.

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{figures/py-queue-example-2.png}
\caption{Figure execution period: Execute EnqueueMany}
 \label{fig:py-queue-example-2}
\end{figure}


After executing the first step \code{Enqueue}, the state of the calculation graph is as shown below.

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{figures/py-queue-example-3.png}
\caption{Figure execution period: Execute Enqueue once}
 \label{fig:py-queue-example-3}
\end{figure}

\subsection{use}

Queues play an important role in model training. Later, we will cover the \ascii{Pipeline} of data loading. The training model often uses \code{RandomShuffleQueue} to prepare sample data for it. To improve the throughput of \ascii{IO}, you can use multithreading to append sample data to the sample queue concurrently; at the same time, the thread of the training model iterates through \code{train\_op} and gets \code once. Batch sample data of {batch\_size} size.

Obviously, the queue plays the role of asynchronous coordination and data exchange in the \ascii{Pipeline} process, which brings a lot of flexibility to the design and implementation of \ascii{Pipeline}.

It's important to note that in order for the queue to maximize its multithreading, two tricky issues need to be addressed:

\begin{enum}
  \eitem{How do I stop all threads at the same time, and how to handle exception reports? }
  \eitem{How to append sample data to the queue concurrently? }
\end{enum}

Therefore, \ascii{TensorFlow} designed two classes, \code{tf.train.Coordinator} and \code{tf.train.QueueRunner}, to solve the above two problems.

These two classes complement each other, \code{Coordinator} coordinates multiple threads to stop running at the same time, and reports an exception to the main program waiting for the stop notification; and \code{QueueRunner} creates a set of threads and collaborates multiple enqueues\ascii Execution of {OP} (eg \code{Enqueue, EnqueueMany}).

\end{content}

\section{Coordinator}

\begin{content}

\code{Coordinator} provides a simple mechanism to stop the execution of a group of threads at the same time. It has \ascii{3} important methods:

\begin{enum}
\eitem{\code{should\_stop}: Determine if the current thread should exit}
\eitem{\code{request\_stop}: Request all threads to stop executing}
\eitem{\code{join}: Wait for all threads to stop executing}
\end{enum}

\subsection{Usage method}

In general, the main program often uses the following pattern to use \code{Coordinator}.

\begin{leftbar}
\ Begin {python}
# Create a coordinator.
coord = tf.train.Coordinator()

# Create 10 threads that run 'MyLoop()'
threads = [threading.Thread(target=MyLoop, args=(coord,)) 
          for i in xrange(10)]

# Start the threads.
for t in threads:
  t.start()
  
# wait for all of them to stop
coord.join(threads)
\end{python}
\end{leftbar}

Any child thread can notify other threads to stop execution by calling \code{coord.request\_stop}. Therefore, in the iterative execution of each thread, check \code{coord.should\_stop()} beforehand. Once \code{coord.request\_stop} is called, \code{coord.request\_stop()} of other threads will immediately return \code{True}.

In general, the iterative execution method of a child thread follows the implementation pattern as follows.

\begin{leftbar}
\ Begin {python}
def MyLoop(coord):
  try
    while not coord.should_stop():
      # ...do something...
  except Exception as e:
    coord.request_stop(e)
\end{python}
\end{leftbar}

\subsection{Exception handling}

When an exception occurs on a thread, the exception can be reported via \code{coord.request\_stop(e)}.

\begin{leftbar}
\ Begin {python}
try:
  while not coord.should_stop():
    # ...do some work...
except Exception as e:
  coord.request_stop(e)
\end{python}
\end{leftbar}

To eliminate duplicate code for exception code handling, you can use the context manager of \code{coord.stop\_on\_exception()}.

\begin{leftbar}
\ Begin {python}
with coord.stop_on_exception():
  while not coord.should_stop():
    # ...do some work...
\end{python}
\end{leftbar}

Among them, the exception will be re-thrown in \code{coord.join}. Therefore, the main program also needs to handle exceptions reasonably.

\begin{leftbar}
\ Begin {python}
try:
  # Create a coordinator.
  coord = tf.train.Coordinator()

  # Create 10 threads that run 'MyLoop()'
  threads = [threading.Thread(target=MyLoop, args=(coord,)) 
            for i in xrange(10)]

  # Start the threads.
  for t in threads:
    t.start()

  # wait for all of them to stop
  coord.join(threads)
except Exception as e:
  # ...exception that was passed to coord.request\_stop(e)
\end{python}
\end{leftbar}

\subsection{Combat: LoopThread}

\end{content}

\section{QueueRunner}

\begin{content}

A \code{QueueRunner} instance holds one or more \code{Enqueue} enqueues\ascii{OP}, which starts a thread for each \code{Enqueue OP}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/tf-queue-runner-model.png}
\caption{TensorFlow System Architecture}
 \label{fig:tf-queue-runner-model}
\end{figure}

\subsection{Register QueueRunner}

You can call \code{tf.train.add\_queue\_runner} to register the \code{QueueRunner} instance into the calculation diagram and add it to the \code{GraphKeys.QUEUE\_RUNNERS} collection.

\begin{leftbar}
\ Begin {python}
def add_queue_runner(qr, collection=ops.GraphKeys.QUEUE_RUNNERS):
  ops.add_to_collection(collection, qr)
\end{python}
\end{leftbar}

\subsection{Execute QueueRunner}

When \code{tf.train.start\_queue\_runners} can be called, it will find all \code{QueueRunner} instances from the calculation graph and fetch all \code{Enqueue OP} from the \code{QueueRunner} instance. Start a thread for each \ascii{OP}.

\begin{leftbar}
\ Begin {python}
def start_queue_runners(sess, coord, daemon=True, start=True,
                        collection=ops.GraphKeys.QUEUE_RUNNERS):
  with sess.graph.as_default():
    threads = []
    for qr in ops.get_collection(collection):
      threads.extend(qr.create_threads(
          sess, coord=coord, daemon=daemon, start=start))
  return threads
\end{python}
\end{leftbar}

In the \code{QueueRunner.create\_threads} method, start a separate thread for each \code{Enqueue} type of \ascii{OP} it contains.

\begin{leftbar}
\ Begin {python}
class QueueRunner(object):
  def create_threads(self, sess, coord, daemon, start):
    """Create threads to run the enqueue ops.
    """
    threads = [threading.Thread(
        target = self._run, args = (sess, op, coord))
        for in self._enqueue_ops]
    if coord:
      threads.append(threading.Thread(
          target=self._close_on_stop, 
          args = (sess, self.cancel_op, coord)))
    for t in threads:
      if coord:
        coord.register_thread(t)
      if daemon:
        t.daemon = daemon
      if start:
        t.start()
    return threads
\end{python}
\end{leftbar}

\subsubsection{Iterative execution Enqueue}

Each \code{Enqueue} child thread will iterate through \code{Enqueue OP}. When a \code{OutOfRangeError} exception occurs, the queue is automatically closed and the child thread is exited; however, if other types of exceptions occur, \code{Coordinator} is actively notified to stop all threads from running and exit the child thread.

\begin{leftbar}
\ Begin {python}
class QueueRunner(object):
  def _run (self, sess, enqueue_op, coord):
    try:
      enqueue_callable = sess.make_callable(enqueue_op)
      while True:
        if coord.should_stop():
          break
        try:
          enqueue_callable()
        except errors.OutOfRangeError:  
          sess.run (self._close_op)
          return
    except Exception as e:
      coord.request_stop(e)
\end{python}
\end{leftbar}

\subsubsection{listening queue off}

In addition, if a \code{Coordinator} instance is given, \code{QueueRunner} will additionally start a thread; when the \code{Coordinator} instance is triggered to call the \code{request\_stop} method, the thread will automatically close. queue.

\begin{leftbar}
\ Begin {python}
class QueueRunner(object):
  def _close_on_stop(self, sess, cancel_op, coord):
    """Close the queue, and cancel pending enqueue ops
       when the Coordinator requests stop.
    """
    coord.wait_for_stop()
    try:
      sess.run (cancel_op)
    except Exception:
      pass
\end{python}
\end{leftbar}

Among them, \code{Cancel OP} and \code{Close OP} of \code{Queue} will close the queue, but \code{Cancel OP} will undo the cached list of \code{Enqueue OP}, but \code{Close OP} keeps a list of cached \code{Enqueue OP}.

\subsection{Close Queue}

When the queue is closed, an error will be generated for any attempt on \code{Enqueue}. However, for any attempt to \code{Dequeue} it is still successful, as long as the elements are left in the queue; otherwise, \code{Dequeue} will fail immediately, throwing a \code{OutOfRangeError} exception without blocking waiting for more elements to be Enter the team.